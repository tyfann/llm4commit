{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T16:18:35.152667Z",
     "start_time": "2024-07-03T16:18:32.469949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import json\n",
    "\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import multiprocessing\n",
    "from model import Model\n",
    "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n",
    "                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)"
   ],
   "id": "acccf11496b11ba8",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-03T16:19:34.033504Z",
     "start_time": "2024-07-03T16:19:33.584413Z"
    }
   },
   "source": "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\", cache_dir='./')",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T16:21:09.146613Z",
     "start_time": "2024-07-03T16:21:09.139093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"../dataset/valid_vdo.jsonl\") as f:\n",
    "    for line in f:\n",
    "        js=json.loads(line.strip())\n",
    "        print(js['code'])\n",
    "        break"
   ],
   "id": "1a983a14c1b8fcce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff --git a/token-metadata/0xc3761EB917CD790B30dAD99f6Cc5b4Ff93C4F9eA/metadata.json b/token-metadata/0xc3761EB917CD790B30dAD99f6Cc5b4Ff93C4F9eA/metadata.json \"symbol\": \"ERC20\",\n",
      "\"address\": \"0xc3761EB917CD790B30dAD99f6Cc5b4Ff93C4F9eA\",\n",
      "\"decimals\": 18,\n",
      "- \"dharmaVerificationStatus\": {\n",
      "\"dharmaVerificationStatus\": \"VERIFIED\"\n",
      "}\n",
      "\\ No newline at end of file\n",
      "-}\n",
      "\\ No newline at end of file\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T16:23:21.855968Z",
     "start_time": "2024-07-03T16:23:21.842423Z"
    }
   },
   "cell_type": "code",
   "source": "code=' '.join(\"diff --git a/token-metadata/0xc3761EB917CD790B30dAD99f6Cc5b4Ff93C4F9eA/metadata.json b/token-metadata/0xc3761EB917CD790B30dAD99f6Cc5b4Ff93C4F9eA/metadata.json\".split())",
   "id": "53185254aaca7b26",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T16:23:22.391474Z",
     "start_time": "2024-07-03T16:23:22.386475Z"
    }
   },
   "cell_type": "code",
   "source": "len(code)",
   "id": "7a713c4757a2f284",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T16:23:23.313151Z",
     "start_time": "2024-07-03T16:23:23.306148Z"
    }
   },
   "cell_type": "code",
   "source": "len(tokenizer.tokenize(code))",
   "id": "54ca607e494f3d0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T16:25:13.287668Z",
     "start_time": "2024-07-03T16:25:13.276665Z"
    }
   },
   "cell_type": "code",
   "source": "code_tokens = tokenizer.tokenize(code)[:50-2]",
   "id": "e9817adeddc922d8",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T16:25:14.290669Z",
     "start_time": "2024-07-03T16:25:14.278664Z"
    }
   },
   "cell_type": "code",
   "source": "source_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]",
   "id": "2526d5826531eb1f",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T16:25:18.103779Z",
     "start_time": "2024-07-03T16:25:18.085780Z"
    }
   },
   "cell_type": "code",
   "source": "len(source_tokens)",
   "id": "3693422ae387dfc4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## preprocess jsonl file",
   "id": "2c46434f7004d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T10:32:06.234270Z",
     "start_time": "2024-07-22T10:32:06.159581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "# Define the input and output file paths\n",
    "input_file = '../dataset/valid_vdo.jsonl'\n",
    "output_file = '../dataset/valid_vdo.csv'\n",
    "\n",
    "# Open the input JSONL file and the output CSV file\n",
    "with open(input_file, 'r', encoding='UTF-8') as jsonl_file, open(output_file, 'w', newline='', encoding='UTF-8') as csv_file:\n",
    "    # Create a CSV writer object\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    \n",
    "    # Write the header row to the CSV file\n",
    "    csv_writer.writerow(['code', 'label'])\n",
    "    \n",
    "    # Read each line from the JSONL file\n",
    "    for line in jsonl_file:\n",
    "        # Parse the JSON data\n",
    "        json_data = json.loads(line.strip())\n",
    "        \n",
    "        # Extract the 'code' and 'label' fields\n",
    "        code = json_data.get('code', '')\n",
    "        label = json_data.get('label', '')\n",
    "        \n",
    "        # Write the data to the CSV file\n",
    "        csv_writer.writerow([code, label])\n",
    "\n",
    "print(f\"Data has been successfully written to {output_file}\")"
   ],
   "id": "8956a50e4423c3e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully written to ../dataset/valid_vdo.csv\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T10:31:29.455319Z",
     "start_time": "2024-07-22T10:31:28.472670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "csv_file_path = '../dataset/train_vdo.csv'\n",
    "df = pd.read_csv(csv_file_path)"
   ],
   "id": "8f091a779d544a58",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T10:31:33.585897Z",
     "start_time": "2024-07-22T10:31:33.572899Z"
    }
   },
   "cell_type": "code",
   "source": "df.iloc[0]",
   "id": "e511b27a576b7ad6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code     diff --git a/nerdamer.core.js b/nerdamer.core....\n",
       "label                                                    0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T12:46:44.594434Z",
     "start_time": "2024-07-22T12:46:40.036099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, GPTNeoForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"tyfann/vdo_format_classify\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/tiny_starcoder_py\", cache_dir='./')\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"code\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Data precessing\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"code\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "# small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "\n",
    "#Create Data Loader objects\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=8)\n",
    "eval_dataloader = DataLoader(tokenized_datasets[\"test\"], batch_size=8)"
   ],
   "id": "e99f60ef78127f8a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/3065 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "822ff59de72c4e0bb230bf9abd29c2ea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T12:45:41.614899Z",
     "start_time": "2024-07-22T12:45:41.130416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./finetuned_model\", num_labels=15)"
   ],
   "id": "2cfc591dd89f1d55",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T12:47:30.398209Z",
     "start_time": "2024-07-22T12:47:30.036376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ],
   "id": "47175da6566e3538",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTBigCodeForSequenceClassification(\n",
       "  (transformer): GPTBigCodeModel(\n",
       "    (wte): Embedding(49153, 768)\n",
       "    (wpe): Embedding(8192, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-19): 20 x GPTBigCodeBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTBigCodeSdpaAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=896, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTBigCodeMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): PytorchGELUTanh()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=15, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T12:48:12.294058Z",
     "start_time": "2024-07-22T12:47:38.094283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ],
   "id": "d7237ce83905d38c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\transformers\\models\\gpt_bigcode\\modeling_gpt_bigcode.py:557: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  sdpa_result = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.2540160642570281}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ab2ee62a47a99907"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset record example: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/27530 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8fcc71cb9ed047c196befee9f33f1d3c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/3065 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87426e4f917b4767a1ff78f10e6bdf76"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1992 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c69822131b04afb97a3ec1246b16634"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 30,
   "source": [
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"code\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Data precessing\n",
    "# tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])"
   ],
   "id": "44d01999cfef20ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T10:56:57.525804Z",
     "start_time": "2024-07-22T10:56:57.513803Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_datasets = tokenized_datasets.remove_columns([\"code\"])",
   "id": "b48b6e6f9c5ed0db",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T11:00:48.944701Z",
     "start_time": "2024-07-22T11:00:48.935698Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.pad_token_id",
   "id": "f68087aa282c52bb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49152"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T10:57:20.038406Z",
     "start_time": "2024-07-22T10:57:20.026407Z"
    }
   },
   "cell_type": "code",
   "source": "len(tokenized_datasets['train'][1]['input_ids'])",
   "id": "c1789f6e12be91be",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "da90af4d27625143"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
