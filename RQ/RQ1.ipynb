{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Build angular format dataset",
   "id": "d5a854cddf0412"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Total_data",
   "id": "e0fc206723bd2ac1"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-01T20:33:08.661141Z",
     "start_time": "2024-07-01T20:31:39.257317Z"
    }
   },
   "source": [
    "import pyarrow.parquet as pq\n",
    "import glob\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "\n",
    "def check_angular_convention(msg, has_space=True, strict=True):\n",
    "    types = '((build)|(ci)|(docs)|(feat)|(fix)|(perf)|(refactor)|(style)|(test)|(chore))'\n",
    "    # types = '((perf))'\n",
    "    if has_space:\n",
    "        pattern = f'{types}\\\\s(\\\\((\\\\s|\\\\S)+\\\\)\\\\s)?:\\\\s(\\\\s|\\\\S)+'\n",
    "    else:\n",
    "        pattern = f'{types}(\\\\((\\\\s|\\\\S)+\\\\))?:\\\\s\\\\S+(\\\\s|\\\\S)+'\n",
    "    # if not strict:\n",
    "    #     pattern = '^((build)|(ci)|(docs)|(feat)|(fix)|(perf)|(refactor)|(style)|(test)|(chore))'\n",
    "    return re.match(pattern, msg) is not None\n",
    "\n",
    "# Define the types for the angular convention\n",
    "angular_types = ['build', 'ci', 'docs', 'feat', 'fix', 'perf', 'refactor', 'style', 'test', 'chore']\n",
    "# angular_types = ['perf']\n",
    "\n",
    "# Create empty dictionaries for each language and type\n",
    "languages = ['JavaScript']\n",
    "dfs = {lang: {atype: [] for atype in angular_types} for lang in languages}\n",
    "\n",
    "# Create a tqdm progress bar for each language\n",
    "bars = {lang: tqdm(total=100000, desc=lang) for lang in languages}  # Total: 10 types * 10000 each\n",
    "\n",
    "folder_path = '../rag/datasets--JetBrains-Research--commit-chronicle/snapshots/5fd076e67b812a9f3d1999e5e40f71715f84bb51/data'  # 文件夹的路径\n",
    "\n",
    "# files = glob.glob(os.path.join(folder_path, 'train*')) + glob.glob(os.path.join(folder_path, 'validation*'))\n",
    "files = glob.glob(os.path.join(folder_path, 'train*')) + glob.glob(os.path.join(folder_path, 'validation*')) + glob.glob(os.path.join(folder_path, 'test*')) \n",
    "\n",
    "for file in files:\n",
    "    df = pq.read_table(file).to_pandas()\n",
    "\n",
    "    # Iterate over each language\n",
    "    for lang in languages[:]:\n",
    "        lang_suffix = '.js'\n",
    "\n",
    "        # Filter rows where language column matches the current language and additional conditions\n",
    "        lang_df = df[(df['language'] == lang) & \n",
    "                     (df['mods'].apply(len) == 1) & \n",
    "                     # (df['message'].apply(lambda x: len(x.split())) <= 30) & \n",
    "                     (df['mods'].apply(lambda x: x[0]['change_type']) == 'MODIFY') &\n",
    "                     (df['mods'].apply(lambda x: len(f\"diff --git a/{x[0]['old_path']} b/{x[0]['new_path']} {x[0]['diff']}\") <= 5000))] \n",
    "                    #  (df['mods'].apply(lambda x: ( (x[0]['old_path'].count(lang_suffix) if x[0]['old_path'] else 0) == 1 and (x[0]['new_path'].count(lang_suffix) if x[0]['new_path'] else 0) == 1 and (x[0]['old_path'].count('.json') if x[0]['old_path'] else 0) == 0 and (x[0]['new_path'].count('.json') if x[0]['new_path'] else 0) == 0 )))]\n",
    "\n",
    "        # Iterate over each row in the filtered DataFrame\n",
    "        for index, row in lang_df.iterrows():\n",
    "            msg = row['message']\n",
    "            if check_angular_convention(msg, has_space=True) or check_angular_convention(msg, has_space=False):\n",
    "                diff = row['mods'][0]\n",
    "                old_path = 'a/' + diff['old_path']\n",
    "                new_path = 'b/' + diff['new_path']\n",
    "                diff_content = diff['diff']  # assume diff_content is an empty string\n",
    "                item = {\n",
    "                    'msg': row['message'],\n",
    "                    'diff': f\"diff --git {old_path} {new_path} {diff_content}\",\n",
    "                    'date': row['date'],\n",
    "                    'repo': row['repo']\n",
    "                }\n",
    "\n",
    "                # Find the type in the message\n",
    "                for atype in angular_types:\n",
    "                    if msg.startswith(atype):\n",
    "                        if len(dfs[lang][atype]) < 10000:\n",
    "                            dfs[lang][atype].append(item)\n",
    "                            bars[lang].update(1)\n",
    "                        break\n",
    "\n",
    "            # Check if all types have reached 1000 rows\n",
    "            if all(len(dfs[lang][atype]) >= 10000 for atype in angular_types):\n",
    "                print(f\"Reached 10000 rows for all types in {lang}\")\n",
    "                languages.remove(lang)  # Remove language from list to avoid further processing\n",
    "                break\n",
    "\n",
    "    # Break out of the loop if all languages have reached the required number of rows\n",
    "    if not languages:\n",
    "        break"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JavaScript:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "150e2d9050ea4a60a4b6cbf881986a4b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:35:36.259951Z",
     "start_time": "2024-07-01T20:35:35.253906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db_data = []\n",
    "\n",
    "for lang, types in dfs.items():\n",
    "    for atype, items in types.items():\n",
    "        if items:  # Ensure there are items to split\n",
    "            for item in items:\n",
    "                item['type'] = atype\n",
    "            db_data.extend(items)\n",
    "\n",
    "import json\n",
    "output_dir = '../data/angular_filtered/subsets'\n",
    "with open(os.path.join(output_dir, f'db_data.json'), 'w') as db_file:\n",
    "    json.dump(db_data, db_file, indent=4)"
   ],
   "id": "fdeac476acb6564f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:36:05.093630Z",
     "start_time": "2024-07-01T20:36:04.095849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "# Split the data and save to files\n",
    "output_dir = '../data/angular_filtered/subsets'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for lang, types in dfs.items():\n",
    "    for atype, items in types.items():\n",
    "        if items:  # Ensure there are items to split\n",
    "            with open(os.path.join(output_dir, f'type_db/{lang}_{atype}_db.json'), 'w') as db_file:\n",
    "                json.dump(items, db_file, indent=4)\n",
    "\n",
    "print(\"Data splitting and saving completed.\")"
   ],
   "id": "aaeff552d508e687",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splitting and saving completed.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:36:10.836265Z",
     "start_time": "2024-07-01T20:36:10.825267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize a dictionary to hold counts for each type\n",
    "type_counts = {lang: {atype: 0 for atype in angular_types} for lang in languages}\n",
    "\n",
    "# Iterate over the collected data to count occurrences of each type\n",
    "for lang, types in dfs.items():\n",
    "    for atype, items in types.items():\n",
    "        type_counts[lang][atype] = len(items)\n",
    "\n",
    "# Print the counts\n",
    "for lang, counts in type_counts.items():\n",
    "    print(f\"Language: {lang}\")\n",
    "    for atype, count in counts.items():\n",
    "        print(f\"  Type: {atype}, Count: {count}\")"
   ],
   "id": "9e5f636c5f8936e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: JavaScript\n",
      "  Type: build, Count: 1076\n",
      "  Type: ci, Count: 1287\n",
      "  Type: docs, Count: 9672\n",
      "  Type: feat, Count: 10000\n",
      "  Type: fix, Count: 10000\n",
      "  Type: perf, Count: 472\n",
      "  Type: refactor, Count: 6574\n",
      "  Type: style, Count: 883\n",
      "  Type: test, Count: 3891\n",
      "  Type: chore, Count: 10000\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test_data",
   "id": "bd197de21cdfb06"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:37:35.939731Z",
     "start_time": "2024-07-01T20:37:21.872535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pyarrow.parquet as pq\n",
    "import glob\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "\n",
    "def check_angular_convention(msg, has_space=True, strict=True):\n",
    "    types = '((build)|(ci)|(docs)|(feat)|(fix)|(perf)|(refactor)|(style)|(test)|(chore))'\n",
    "    # types = '((perf))'\n",
    "    if has_space:\n",
    "        pattern = f'{types}\\\\s(\\\\((\\\\s|\\\\S)+\\\\)\\\\s)?:\\\\s(\\\\s|\\\\S)+'\n",
    "    else:\n",
    "        pattern = f'{types}(\\\\((\\\\s|\\\\S)+\\\\))?:\\\\s\\\\S+(\\\\s|\\\\S)+'\n",
    "    # if not strict:\n",
    "    #     pattern = '^((build)|(ci)|(docs)|(feat)|(fix)|(perf)|(refactor)|(style)|(test)|(chore))'\n",
    "    return re.match(pattern, msg) is not None\n",
    "\n",
    "# Define the types for the angular convention\n",
    "angular_types = ['build', 'ci', 'docs', 'feat', 'fix', 'perf', 'refactor', 'style', 'test', 'chore']\n",
    "# angular_types = ['perf']\n",
    "\n",
    "# Create empty dictionaries for each language and type\n",
    "languages = ['JavaScript']\n",
    "dfs = {lang: {atype: [] for atype in angular_types} for lang in languages}\n",
    "\n",
    "# Create a tqdm progress bar for each language\n",
    "bars = {lang: tqdm(total=2000, desc=lang) for lang in languages}  # Total: 10 types * 10000 each\n",
    "\n",
    "folder_path = '../rag/datasets--JetBrains-Research--commit-chronicle/snapshots/5fd076e67b812a9f3d1999e5e40f71715f84bb51/data'  # 文件夹的路径\n",
    "\n",
    "# files = glob.glob(os.path.join(folder_path, 'train*')) + glob.glob(os.path.join(folder_path, 'validation*')) + glob.glob(os.path.join(folder_path, 'test*')) \n",
    "files = glob.glob(os.path.join(folder_path, 'test*')) \n",
    "\n",
    "for file in files:\n",
    "    df = pq.read_table(file).to_pandas()\n",
    "\n",
    "    # Iterate over each language\n",
    "    for lang in languages[:]:\n",
    "        lang_suffix = '.js'\n",
    "\n",
    "        # Filter rows where language column matches the current language and additional conditions\n",
    "        lang_df = df[(df['language'] == lang) & \n",
    "                     (df['mods'].apply(len) == 1) & \n",
    "                     # (df['message'].apply(lambda x: len(x.split())) <= 30) & \n",
    "                     (df['mods'].apply(lambda x: x[0]['change_type']) == 'MODIFY') &\n",
    "                     (df['mods'].apply(lambda x: len(f\"diff --git a/{x[0]['old_path']} b/{x[0]['new_path']} {x[0]['diff']}\") <= 5000))] \n",
    "                    #  (df['mods'].apply(lambda x: ( (x[0]['old_path'].count(lang_suffix) if x[0]['old_path'] else 0) == 1 and (x[0]['new_path'].count(lang_suffix) if x[0]['new_path'] else 0) == 1 and (x[0]['old_path'].count('.json') if x[0]['old_path'] else 0) == 0 and (x[0]['new_path'].count('.json') if x[0]['new_path'] else 0) == 0 )))]\n",
    "\n",
    "        # Iterate over each row in the filtered DataFrame\n",
    "        for index, row in lang_df.iterrows():\n",
    "            msg = row['message']\n",
    "            if check_angular_convention(msg, has_space=True) or check_angular_convention(msg, has_space=False):\n",
    "                diff = row['mods'][0]\n",
    "                old_path = 'a/' + diff['old_path']\n",
    "                new_path = 'b/' + diff['new_path']\n",
    "                diff_content = diff['diff']  # assume diff_content is an empty string\n",
    "                item = {\n",
    "                    'msg': row['message'],\n",
    "                    'diff': f\"diff --git {old_path} {new_path} {diff_content}\",\n",
    "                    'date': row['date'],\n",
    "                    'repo': row['repo']\n",
    "                }\n",
    "\n",
    "                # Find the type in the message\n",
    "                for atype in angular_types:\n",
    "                    if msg.startswith(atype):\n",
    "                        if len(dfs[lang][atype]) < 200:\n",
    "                            dfs[lang][atype].append(item)\n",
    "                            bars[lang].update(1)\n",
    "                        break\n",
    "\n",
    "            # Check if all types have reached 1000 rows\n",
    "            if all(len(dfs[lang][atype]) >= 200 for atype in angular_types):\n",
    "                print(f\"Reached 10000 rows for all types in {lang}\")\n",
    "                languages.remove(lang)  # Remove language from list to avoid further processing\n",
    "                break\n",
    "\n",
    "    # Break out of the loop if all languages have reached the required number of rows\n",
    "    if not languages:\n",
    "        break"
   ],
   "id": "9dedda252981f93b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JavaScript:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca89793ad7994285ba4bf5b7115ec265"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:37:40.601179Z",
     "start_time": "2024-07-01T20:37:40.578180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize a dictionary to hold counts for each type\n",
    "type_counts = {lang: {atype: 0 for atype in angular_types} for lang in languages}\n",
    "\n",
    "# Iterate over the collected data to count occurrences of each type\n",
    "for lang, types in dfs.items():\n",
    "    for atype, items in types.items():\n",
    "        type_counts[lang][atype] = len(items)\n",
    "\n",
    "# Print the counts\n",
    "for lang, counts in type_counts.items():\n",
    "    print(f\"Language: {lang}\")\n",
    "    for atype, count in counts.items():\n",
    "        print(f\"  Type: {atype}, Count: {count}\")"
   ],
   "id": "31326c80e3ea1dd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: JavaScript\n",
      "  Type: build, Count: 200\n",
      "  Type: ci, Count: 200\n",
      "  Type: docs, Count: 200\n",
      "  Type: feat, Count: 200\n",
      "  Type: fix, Count: 200\n",
      "  Type: perf, Count: 31\n",
      "  Type: refactor, Count: 200\n",
      "  Type: style, Count: 200\n",
      "  Type: test, Count: 200\n",
      "  Type: chore, Count: 200\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:38:20.157570Z",
     "start_time": "2024-07-01T20:38:20.112571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_data = []\n",
    "\n",
    "for lang, types in dfs.items():\n",
    "    for atype, items in types.items():\n",
    "        if items:  # Ensure there are items to split\n",
    "            for item in items:\n",
    "                item['type'] = atype\n",
    "            test_data.extend(items)\n",
    "\n",
    "import json\n",
    "output_dir = '../data/angular_filtered/subsets'\n",
    "with open(os.path.join(output_dir, f'test_data.json'), 'w') as db_file:\n",
    "    json.dump(test_data, db_file, indent=4)"
   ],
   "id": "60ead987ecd62ae5",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model generation",
   "id": "2c16235238da2ce5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RACE",
   "id": "e7ad943801cf9184"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:40:52.303423Z",
     "start_time": "2024-07-01T20:40:49.337834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "with open('../data/angular_filtered/subsets/test_data.json') as f:\n",
    "    prompt_data = json.load(f)\n",
    "    \n",
    "import os\n",
    "os.environ['HF_HOME'] = '../models/'\n",
    "\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text2text-generation\", model=\"JetBrains-Research/cmg-race-without-history\", device=0)"
   ],
   "id": "e4828a38b7e52fd1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:48:15.603833Z",
     "start_time": "2024-07-01T20:40:54.839551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "diffs = []\n",
    "generated_commit_messages = []\n",
    "\n",
    "for commit in prompt_data:\n",
    "    diff = commit['diff']\n",
    "    diffs.append(diff)\n",
    "\n",
    "for diff in tqdm(diffs, total=len(diffs), desc='Generating commit messages'):\n",
    "    prompt = f\"\"\"\n",
    "    The following is a diff which describes the code changes in a commit, Your task is to write a short commit message accordingly.\n",
    "    {diff}\n",
    "    According to the diff, the commit message should be:\n",
    "    \"\"\"\n",
    "    generated_commit_messages.append(pipe(prompt)[0]['generated_text'])"
   ],
   "id": "f6267aa94decf7e3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating commit messages:   0%|          | 0/1831 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "83bdc85c300b4d3bbaca88f5e3d8bb47"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1530 > 512). Running this sequence through the model will result in indexing errors\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\logging\\__init__.py\", line 1100, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\logging\\__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\logging\\__init__.py\", line 678, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\logging\\__init__.py\", line 368, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\asyncio\\windows_events.py\", line 321, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\tyfann\\AppData\\Local\\Temp\\ipykernel_5560\\4289532399.py\", line 14, in <module>\n",
      "    generated_commit_messages.append(pipe(prompt)[0]['generated_text'])\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py\", line 167, in __call__\n",
      "    result = super().__call__(*args, **kwargs)\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 1167, in __call__\n",
      "    logger.warning_once(\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\transformers\\utils\\logging.py\", line 329, in warning_once\n",
      "    self.warning(*args, **kwargs)\n",
      "Message: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\n",
      "Arguments: (<class 'UserWarning'>,)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:49:11.181953Z",
     "start_time": "2024-07-01T20:49:11.137695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "for item, msg in zip(prompt_data, generated_commit_messages):\n",
    "    item['race'] = msg\n",
    "\n",
    "output_file = '../data/angular_filtered/subsets/generation/test_race.json'\n",
    "with open(output_file, 'w', encoding='UTF-8') as f:\n",
    "    json.dump(prompt_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# save msg to a file\n",
    "with open(\"../data/angular_filtered/subsets/generation/test_race.txt\", 'w', encoding='UTF-8') as file:\n",
    "    for item in prompt_data:\n",
    "        file.write(item['race'].replace('\\n', '\\\\n').replace('\\r', '\\\\r') + '\\n')"
   ],
   "id": "6b0a35f53ee5f613",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ChatGPT",
   "id": "8d22ba49c53055c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T21:12:25.431499Z",
     "start_time": "2024-07-01T20:51:06.291650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain import hub\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "prompt = hub.pull(\"tyfann/llm4commit-zeroshot\")\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"sk-0rLvuRkMiD4Mw25QYygh6rUlZVjpQWNGNF4yez7z3PZ7yCOm\",\n",
    "    base_url=\"https://api.chatanywhere.cn/v1\"\n",
    ")\n",
    "\n",
    "def gpt_35_api(messages: list):\n",
    "\n",
    "    completion = client.chat.completions.create(model=\"gpt-3.5-turbo-0125\", messages=messages, temperature=0)\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "with open('../data/angular_filtered/subsets/test_data.json', 'r', encoding='UTF-8') as f:\n",
    "    org_data = json.load(f)\n",
    "\n",
    "gpt_msg = []\n",
    "for index, data in tqdm(enumerate(org_data), total=len(org_data), desc=\"Processing documents\"):\n",
    "    # merged_diff = '\\n'.join(diff['diff'] for diff in data['diff'])\n",
    "    messages = prompt.invoke(\n",
    "        {\"DIFF\": data['diff']}\n",
    "    ).to_messages()\n",
    "    example_prompt = [{'role': 'user','content': messages[0].content},]\n",
    "    try:\n",
    "        gpt_msg.append(gpt_35_api(example_prompt))\n",
    "    except:\n",
    "        print(index)\n",
    "        gpt_msg.append(\"\")\n",
    "\n",
    "for item, msg in zip(org_data, gpt_msg):\n",
    "    item['chatgpt_zeroshot'] = msg\n",
    "import os\n",
    "# output_file = '../data/chronicle/rag_baseline/zeroshot/rag_baseline_python_chatgpt.json'\n",
    "output_file = '../data/angular_filtered/subsets/generation/test_gpt35_zeroshot.json'\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "with open(output_file, 'w', encoding='UTF-8') as f:\n",
    "    json.dump(org_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "output_file = '../data/angular_filtered/subsets/generation/test_gpt35_zeroshot.txt'\n",
    "with open(output_file, 'w', encoding='UTF-8') as f:\n",
    "    for item in org_data:\n",
    "        f.write(item['chatgpt_zeroshot'].replace('\\n', '\\\\n').replace('\\r', '\\\\r') + '\\n')"
   ],
   "id": "3a7aa3f1bc72788",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 1831/1831 [21:16<00:00,  1.43it/s]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b365e13113cdcf9e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
