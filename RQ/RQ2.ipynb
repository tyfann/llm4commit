{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Angular format data",
   "id": "3c8227eaebd61427"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build vanilla RAG system",
   "id": "125deaf2171141a0"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-02T08:46:52.805371Z",
     "start_time": "2024-07-02T08:46:52.092287Z"
    }
   },
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import pickle\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import glob\n",
    "import os\n",
    "from collections import defaultdict"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T08:47:01.586644Z",
     "start_time": "2024-07-02T08:46:55.944632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the path to the pre-trained model you want to use\n",
    "# modelPath = \"mixedbread-ai/mxbai-embed-large-v1\"\n",
    "# modelPath = \"mchochlov/codebert-base-cd-ft\"\n",
    "# modelPath= \"microsoft/unixcoder-base\"\n",
    "# modelPath =\"codecompletedeployment/st-codesearch-distilroberta-base\"\n",
    "modelPath = \"intfloat/e5-small-v2\"\n",
    "# modelPath = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# modelPath = \"../models/models-e5-v2-finetuned-10w-epoch20\"\n",
    "\n",
    "# for modelPath in [\"mixedbread-ai/mxbai-embed-large-v1\", \"intfloat/e5-small-v2\", \"sentence-transformers/all-MiniLM-L6-v2\"]:\n",
    "model_kwargs = {'device':'cuda', 'trust_remote_code': True}\n",
    "\n",
    "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    cache_folder = '../models',\n",
    "    model_name=modelPath,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
    ")"
   ],
   "id": "adae4440b3fa7a97",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:40:19.409791Z",
     "start_time": "2024-07-01T20:40:14.557605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "diff_loader = JSONLoader(\n",
    "    file_path='../data/angular_filtered/subsets/db_data.json',\n",
    "    jq_schema='.[].diff',\n",
    "    text_content=False)\n",
    "\n",
    "diff_data = diff_loader.load()\n",
    "\n",
    "msg_loader = JSONLoader(\n",
    "    file_path='../data/angular_filtered/subsets/db_data.json',\n",
    "    jq_schema='.[].msg',\n",
    "    text_content=False)\n",
    "\n",
    "msg_data = msg_loader.load()"
   ],
   "id": "ce8f6594cf984f93",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:48:21.481174Z",
     "start_time": "2024-07-01T20:40:19.411790Z"
    }
   },
   "cell_type": "code",
   "source": "db = Chroma.from_documents(diff_data, embeddings, persist_directory=\"../data/angular_filtered/subsets/type_db/rag_all_types_db_e5_nochunk\")",
   "id": "6d56cb02e444c77b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:48:21.591235Z",
     "start_time": "2024-07-01T20:48:21.484256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_diff_loader = JSONLoader(\n",
    "    file_path='../data/angular_filtered/subsets/test_data.json',\n",
    "    jq_schema='.[].diff',\n",
    "    text_content=False)\n",
    "\n",
    "test_diff_data = test_diff_loader.load()\n",
    "# test_diff_data = test_diff_loader.load()[:1000]\n",
    "\n",
    "with open('../data/angular_filtered/subsets/test_data.json', 'r', encoding='UTF-8') as f:\n",
    "    test_data = json.load(f)"
   ],
   "id": "35fc5933296f4221",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T20:48:47.258957Z",
     "start_time": "2024-07-01T20:48:21.592241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "similar_diff = []\n",
    "retriever = db.as_retriever()\n",
    "for diff_doc in tqdm(test_diff_data, total=len(test_diff_data), desc=\"Processing documents\"):\n",
    "    similar_diff.append(retriever.invoke(diff_doc.page_content)[0])\n",
    "\n",
    "for sim_diff, test_item in zip(similar_diff, test_data):\n",
    "    test_item['sim_msg'] = msg_data[sim_diff.metadata['seq_num']-1].page_content\n",
    "    test_item['sim_diff'] = diff_data[sim_diff.metadata['seq_num']-1].page_content\n",
    "\n",
    "with open('../data/angular_filtered/subsets/generation/rag/test_rag_prompt.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(test_data, f, ensure_ascii=False, indent=4)"
   ],
   "id": "849686eab60001b4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Processing documents:   0%|          | 0/1831 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c481148a55f84df89278e0cf46e6e686"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T21:18:47.281814Z",
     "start_time": "2024-07-01T21:18:45.301872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"sk-0rLvuRkMiD4Mw25QYygh6rUlZVjpQWNGNF4yez7z3PZ7yCOm\",\n",
    "    base_url=\"https://api.chatanywhere.cn/v1\"\n",
    ")\n",
    "\n",
    "def gpt_35_api(messages: list):\n",
    "\n",
    "    completion = client.chat.completions.create(model=\"gpt-3.5-turbo-0125\", messages=messages, temperature=0)\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from langchain import hub\n",
    "prompt = hub.pull(\"tyfann/llm4commit-rag:b843ef0b\")\n"
   ],
   "id": "862ad989edae6deb",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T21:43:30.335279Z",
     "start_time": "2024-07-01T21:20:02.654436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(f'../data/angular_filtered/subsets/generation/rag/test_rag_prompt.json', 'r', encoding='UTF-8') as f:\n",
    "    org_data = json.load(f)\n",
    "gpt_msg = []\n",
    "for index, data in tqdm(enumerate(org_data), total=len(org_data), desc=\"Processing documents\"):\n",
    "    # merged_diff = '\\n'.join(diff['diff'] for diff in data['diff'])\n",
    "    messages = prompt.invoke(\n",
    "        {\"context\": data['sim_diff'], \"msg\": data['sim_msg'], \"diff\": data['diff']}\n",
    "    ).to_messages()\n",
    "    example_prompt = [{'role': 'user','content': messages[0].content},]\n",
    "    try:\n",
    "        gpt_msg.append(gpt_35_api(example_prompt))\n",
    "    except:\n",
    "        print(index)\n",
    "        gpt_msg.append('')\n",
    "    # gpt_msg.append(gpt_35_api(example_prompt))\n",
    "\n",
    "for item, msg in zip(org_data, gpt_msg):\n",
    "    item['chatgpt_rag'] = msg\n",
    "\n",
    "output_file = f'../data/angular_filtered/subsets/generation/test_gpt35_rag.json'\n",
    "with open(output_file, 'w', encoding='UTF-8') as f:\n",
    "    json.dump(org_data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "output_file = f'../data/angular_filtered/subsets/generation/test_gpt35_rag.txt'\n",
    "with open(output_file, 'w', encoding='UTF-8') as f:\n",
    "    for item in org_data:\n",
    "        f.write(item['chatgpt_rag'].replace('\\n', '\\\\n').replace('\\r', '\\\\r') + '\\n')"
   ],
   "id": "439413e69ded3d5f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 1831/1831 [23:27<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build classified RAG system",
   "id": "fd6df488648c1697"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T21:57:51.425505Z",
     "start_time": "2024-07-01T21:57:51.412509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "types = [\n",
    "    'build', 'ci', 'docs', 'feat', 'fix', \n",
    "    'perf', 'refactor', 'style', 'test', 'chore'\n",
    "]\n",
    "type_label_mapping = {type_name: idx for idx, type_name in enumerate(types)}\n",
    "output_dir = '../data/angular_filtered/subsets/classification'\n",
    "# Save the type-label mapping to a txt file\n",
    "with open(os.path.join(output_dir, 'type_label_mapping.txt'), 'w') as mapping_file:\n",
    "    for type_name, label in type_label_mapping.items():\n",
    "        mapping_file.write(f'{type_name}: {label}\\n')"
   ],
   "id": "9d85ef140b68c2a8",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T21:58:45.594483Z",
     "start_time": "2024-07-01T21:58:45.535230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# 读取mapping.txt文件\n",
    "def read_mapping(file_path):\n",
    "    mapping = {}\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            key, value = line.strip().split(': ')\n",
    "            mapping[key] = int(value)\n",
    "    return mapping\n",
    "\n",
    "# 读取json文件\n",
    "def read_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# 写入jsonl文件\n",
    "def write_jsonl(file_path, data):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for item in data:\n",
    "            json.dump(item, f)\n",
    "            f.write('\\n')\n",
    "\n",
    "# 主函数\n",
    "def main(mapping_file, json_file, output_file):\n",
    "    # 读取mapping和json数据\n",
    "    mapping = read_mapping(mapping_file)\n",
    "    json_data = read_json(json_file)\n",
    "    # random.seed(42)\n",
    "    # random.shuffle(json_data)\n",
    "\n",
    "    # 生成新的jsonl数据\n",
    "    new_data = []\n",
    "    for item in json_data:\n",
    "        new_item = {\n",
    "            \"code\": item[\"diff\"],\n",
    "            \"label\": mapping[item[\"type\"]]\n",
    "        }\n",
    "        new_data.append(new_item)\n",
    "\n",
    "    # 写入新的jsonl文件\n",
    "    write_jsonl(output_file, new_data)\n",
    "\n",
    "# 调用主函数\n",
    "mapping_file = '../data/angular_filtered/subsets/classification/type_label_mapping.txt'\n",
    "json_file = '../data/angular_filtered/subsets/test_data.json'\n",
    "output_file = '../../CodeBERT-classification/dataset/test_angular.jsonl'\n",
    "main(mapping_file, json_file, output_file)"
   ],
   "id": "f9a06a5b9b534356",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T22:06:40.981023Z",
     "start_time": "2024-07-01T22:06:39.785919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Define the directory containing the JSON files\n",
    "output_dir = '../../CodeBERT-classification/dataset'\n",
    "\n",
    "# List of types and their corresponding labels\n",
    "types = [\n",
    "    'build', 'ci', 'docs', 'feat', 'fix', \n",
    "    'perf', 'refactor', 'style', 'test', 'chore'\n",
    "]\n",
    "type_label_mapping = {type_name: idx for idx, type_name in enumerate(types)}\n",
    "\n",
    "train_items = []\n",
    "valid_items = []\n",
    "\n",
    "# Read and process all test JSON files\n",
    "for type_name in types:\n",
    "    test_file = os.path.join('../data/angular_filtered/subsets/type_db', f'JavaScript_{type_name}_db.json')\n",
    "    if os.path.exists(test_file):\n",
    "        with open(test_file, 'r') as f:\n",
    "            items = json.load(f)\n",
    "            data = [\n",
    "                {\n",
    "                    \"code\": item['diff'],\n",
    "                    \"label\": type_label_mapping[type_name]\n",
    "                }\n",
    "                for item in items\n",
    "            ]\n",
    "            # Split data into 1:8 ratio\n",
    "            train_split, valid_split = train_test_split(data, test_size=1/10, random_state=42)\n",
    "            valid_items.extend(valid_split)\n",
    "            train_items.extend(train_split)\n",
    "\n",
    "# Shuffle the items to randomize their order\n",
    "random.shuffle(valid_items)\n",
    "random.shuffle(train_items)\n",
    "\n",
    "# Save validation items to valid.jsonl\n",
    "with open(os.path.join(output_dir, 'valid_angular.jsonl'), 'w') as valid_file:\n",
    "    for item in valid_items:\n",
    "        valid_file.write(json.dumps(item) + '\\n')\n",
    "\n",
    "# Save training items to train.jsonl\n",
    "with open(os.path.join(output_dir, 'train_angular.jsonl'), 'w') as train_file:\n",
    "    for item in train_items:\n",
    "        train_file.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(\"Data splitting and saving to JSONL files completed.\")\n"
   ],
   "id": "779371fa87c9dfd4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splitting and saving to JSONL files completed.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "calculate vanilla rag accuracy",
   "id": "9584a694a86b22a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T08:25:29.589538Z",
     "start_time": "2024-07-02T08:25:29.536772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import json\n",
    "def get_commit_type(msg, has_space=True):\n",
    "    types = '((build)|(ci)|(docs)|(feat)|(fix)|(perf)|(refactor)|(style)|(test)|(chore))'\n",
    "    if has_space:\n",
    "        pattern = f'^{types}\\\\s*(\\\\((\\\\s|\\\\S)+\\\\))?:\\\\s*(\\\\s|\\\\S)+'\n",
    "    else:\n",
    "        pattern = f'^{types}(\\\\((\\\\s|\\\\S)+\\\\))?:\\\\s*\\\\S+(\\\\s|\\\\S)+'\n",
    "    match = re.match(pattern, msg)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "with open('../data/angular_filtered/subsets/generation/rag/test_rag_prompt.json', 'r', encoding='UTF-8') as f:\n",
    "    prompt = json.load(f)\n",
    "\n",
    "count = 0\n",
    "for item in prompt:\n",
    "    sim_type = get_commit_type(item['sim_msg'])\n",
    "    if sim_type is not None:\n",
    "        if sim_type == item['type']:\n",
    "            count +=1\n",
    "    else:\n",
    "        print('fail')\n",
    "\n",
    "print(count / len(prompt))"
   ],
   "id": "2ffebb30a01d7a4f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "classification result to json",
   "id": "12f077aa8c3ecee7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T08:45:37.995711Z",
     "start_time": "2024-07-02T08:45:37.935602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "# 读取mapping.txt文件\n",
    "def read_mapping(file_path):\n",
    "    mapping = {}\n",
    "    with open(file_path, 'r', encoding='UTF-8') as f:\n",
    "        for line in f:\n",
    "            key, value = line.strip().split(': ')\n",
    "            mapping[int(value)] = key  # 将value转换为int作为键，key作为值\n",
    "    return mapping\n",
    "\n",
    "# 读取json文件\n",
    "def read_json(file_path):\n",
    "    with open(file_path, 'r', encoding='UTF-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# 读取pred.txt文件\n",
    "def read_labels(file_path):\n",
    "    with open(file_path, 'r', encoding='UTF-8') as f:\n",
    "        labels = [int(line.strip()) for line in f]\n",
    "    return labels\n",
    "\n",
    "# 写入json文件\n",
    "def write_json(file_path, data):\n",
    "    with open(file_path, 'w', encoding='UTF-8') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# 主函数\n",
    "def main(mapping_file, json_file, pred_file, output_file):\n",
    "    # 读取mapping和json数据\n",
    "    mapping = read_mapping(mapping_file)\n",
    "    json_data = read_json(json_file)\n",
    "    pred_labels = read_labels(pred_file)\n",
    "\n",
    "    # 添加classifier_type字段\n",
    "    for item, label in zip(json_data, pred_labels):\n",
    "        item[\"classifier_type\"] = mapping[label]\n",
    "\n",
    "    # 写入更新后的json文件\n",
    "    write_json(output_file, json_data)\n",
    "\n",
    "# 调用主函数\n",
    "mapping_file = '../data/angular_filtered/subsets/classification/type_label_mapping.txt'\n",
    "json_file = '../data/angular_filtered/subsets/test_data.json'\n",
    "pred_file = '../data/angular_filtered/subsets/classification/angular_test_predictions.txt'\n",
    "output_file = '../data/angular_filtered/subsets/classification/test_with_classification.json'\n",
    "main(mapping_file, json_file, pred_file, output_file)"
   ],
   "id": "1dde9d82bfa1c7f9",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T08:49:08.553331Z",
     "start_time": "2024-07-02T08:49:08.499311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('../data/angular_filtered/subsets/classification/test_with_classification.json', 'r', encoding='UTF-8') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "for index, item in enumerate(test_data):\n",
    "    item['seq_num'] = index\n",
    "\n",
    "with open('../data/angular_filtered/subsets/classification/test_with_classification.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(test_data, f, indent=4)"
   ],
   "id": "c6f47a924791432a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T08:49:13.422316Z",
     "start_time": "2024-07-02T08:49:13.314083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "folder_path = '../data/angular_filtered/subsets/type_db'\n",
    "files = glob.glob(os.path.join(folder_path, '*db.json'))\n",
    "\n",
    "test_diff_loader = JSONLoader(\n",
    "    file_path='../data/angular_filtered/subsets/classification/test_with_classification.json',\n",
    "    jq_schema='.[].diff',\n",
    "    text_content=False)\n",
    "\n",
    "test_diff_data = test_diff_loader.load()\n",
    "# test_diff_data = test_diff_loader.load()[:1000]\n",
    "\n",
    "with open('../data/angular_filtered/subsets/classification/test_with_classification.json', 'r', encoding='UTF-8') as f:\n",
    "    test_data = json.load(f)"
   ],
   "id": "b2fe4ee769525312",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "grouped_data = defaultdict(list)\n",
    "for entry in test_data:\n",
    "    grouped_data[entry['classifier_type']].append(entry)\n",
    "# splitter = RecursiveCharacterTextSplitter.from_language(Language.JS, chunk_size=1000, chunk_overlap=200)\n",
    "for file in files:\n",
    "    diff_loader = JSONLoader(\n",
    "        file_path=file,\n",
    "        jq_schema='.[].diff',\n",
    "        text_content=False)\n",
    "    \n",
    "    diff_data = diff_loader.load()\n",
    "    \n",
    "    msg_loader = JSONLoader(\n",
    "        file_path=file,\n",
    "        jq_schema='.[].msg',\n",
    "        text_content=False)\n",
    "\n",
    "    msg_data = msg_loader.load()\n",
    "    # diff_split = []\n",
    "\n",
    "    # for i, doc in tqdm(enumerate(diff_data), total=len(diff_data), desc=\"Processing documents\"):\n",
    "    #     diff_split += splitter.split_documents([doc])\n",
    "    \n",
    "    type = file.split('\\\\')[-1].split('_')[1]\n",
    "    if os.path.exists(f\"../data/angular_filtered/subsets/type_db/rag_{type}_db_e5\"):\n",
    "        db = Chroma(persist_directory=f\"../data/angular_filtered/subsets/type_db/rag_{type}_db_e5\", embedding_function=embeddings)\n",
    "    else:\n",
    "        db = Chroma.from_documents(diff_data, embeddings, persist_directory=f\"../data/angular_filtered/subsets/type_db/rag_{type}_db_e5\")\n",
    "    # db = Chroma(persist_directory=f\"../data/angular_filtered/subsets/type_db/rag_{type}_db_e5\", embedding_function=embeddings)\n",
    "    similar_diff = []\n",
    "    retriever = db.as_retriever()\n",
    "    indexs = [item['seq_num'] for item in grouped_data[type]]\n",
    "    for index in tqdm(indexs, total=len(indexs), desc=\"Processing documents\"):\n",
    "        similar_diff.append(retriever.invoke(test_diff_data[index].page_content)[0])\n",
    "    \n",
    "    for sim_diff, test_diff in zip(similar_diff, grouped_data[type]):\n",
    "        test_diff['sim_msg'] = msg_data[sim_diff.metadata['seq_num']-1].page_content\n",
    "        test_diff['sim_diff'] = diff_data[sim_diff.metadata['seq_num']-1].page_content"
   ],
   "id": "e49ef9babb723fa4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T09:59:19.397844Z",
     "start_time": "2024-07-02T09:59:19.276314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('../data/angular_filtered/subsets-v1/test_data.json', 'r', encoding='UTF-8') as f:\n",
    "    test_data = json.load(f)"
   ],
   "id": "7fedf0d2adb8756b",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T09:59:22.276641Z",
     "start_time": "2024-07-02T09:59:22.266644Z"
    }
   },
   "cell_type": "code",
   "source": "len(test_data)",
   "id": "580df3717103c62b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5636"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3ab4861cd6f7e495"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
