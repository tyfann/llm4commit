{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "285c0808879219e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:30:12.484922Z",
     "start_time": "2024-07-11T09:30:12.401592Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "# 读取原始JSON文件\n",
    "data = []\n",
    "\n",
    "with open('../data/angular_filtered/subsets/generation/test_race.json', 'r', encoding='utf-8') as f:\n",
    "    race_data = json.load(f)\n",
    "# with open('../data/angular_filtered/subsets/generation/test_gpt35_zeroshot.json', 'r', encoding='utf-8') as f:\n",
    "#     zeroshot_data = json.load(f)\n",
    "# with open('../data/angular_filtered/subsets/generation/test_gpt35_rag.json', 'r', encoding='utf-8') as f:\n",
    "#     rag_data = json.load(f)\n",
    "with open('../data/angular_filtered/subsets/generation/test_gpt35_model_classified_rag.json', 'r', encoding='utf-8') as f:\n",
    "    model_rag_data = json.load(f)\n",
    "# with open('../data/angular_filtered/subsets/generation/test_nngen.json', 'r', encoding='utf-8') as f:\n",
    "#     nngen_data = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb920ce745a60f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:30:17.637065Z",
     "start_time": "2024-07-11T09:30:17.594739Z"
    }
   },
   "outputs": [],
   "source": [
    "for index in range(len(model_rag_data)):\n",
    "    item = {\n",
    "        'diff': race_data[index]['diff'],\n",
    "        # 'human': race_data[index]['msg'],\n",
    "        'race': race_data[index]['race'],\n",
    "        # 'zeroshot': zeroshot_data[index]['chatgpt_zeroshot'],\n",
    "        # 'rag': rag_data[index]['chatgpt_rag'],\n",
    "        'classified_rag': model_rag_data[index]['chatgpt_rag'],\n",
    "        # 'nngen': nngen_data[index]['nngen'],\n",
    "        'type': race_data[index]['type']\n",
    "    }\n",
    "    data.append(item)\n",
    "\n",
    "with open('../data/angular_filtered/subsets/generation/test_all.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:30:39.296402Z",
     "start_time": "2024-07-11T09:30:39.282385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机提取的50个项目已保存到output.json文件中。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 确保JSON数据是一个列表\n",
    "if not isinstance(data, list):\n",
    "    raise ValueError(\"JSON数据不是一个列表\")\n",
    "\n",
    "# 随机选择100个项目\n",
    "sample_size = 50\n",
    "if len(data) < sample_size:\n",
    "    raise ValueError(\"数据量少于50个项目\")\n",
    "\n",
    "initial_sample = random.sample(data, sample_size)\n",
    "\n",
    "# 从剩余的项目中再随机选择100个项目\n",
    "remaining_data = [item for item in data if item not in initial_sample]\n",
    "# second_sample = random.sample(remaining_data, sample_size)\n",
    "\n",
    "# 将选定的项目写入一个新的JSON文件\n",
    "with open('../data/angular_filtered/subsets/human_eval/no_human_ref_test.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(initial_sample, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "# 将选定的项目写入一个新的JSON文件\n",
    "# with open('../data/angular_filtered/subsets/human_eval/human_ref_test.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(second_sample, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"随机提取的50个项目已保存到output.json文件中。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "600cf96cae0561e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T09:30:50.533607Z",
     "start_time": "2024-07-11T09:30:50.520608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "类型分布：\n",
      "refactor: 8\n",
      "docs: 10\n",
      "ci: 4\n",
      "test: 5\n",
      "feat: 9\n",
      "chore: 5\n",
      "fix: 7\n",
      "style: 1\n",
      "build: 1\n"
     ]
    }
   ],
   "source": [
    "## 统计随机提取的50个项目的类型分布\n",
    "types = {}\n",
    "for item in initial_sample:\n",
    "    if item['type'] not in types:\n",
    "        types[item['type']] = 0\n",
    "    types[item['type']] += 1\n",
    "\n",
    "print(\"类型分布：\")\n",
    "for key, value in types.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6578613ccf62ac13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T13:22:20.985718Z",
     "start_time": "2024-07-11T13:22:20.939822Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# 读取原始数据文件\n",
    "with open('../data/angular_filtered/subsets/human_eval/no_human_ref_test.json', 'r', encoding='UTF-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "mapping = {}\n",
    "\n",
    "# 遍历每个对象并替换字段名称\n",
    "for i, item in enumerate(data):\n",
    "    item_id = f\"Object-{i+1}\"\n",
    "    mapping[item_id] = {}\n",
    "    \n",
    "    keys = list(item.keys())\n",
    "    keys_to_rename = [key for key in keys if key not in ['diff', 'type']]\n",
    "    \n",
    "    random.shuffle(keys_to_rename)\n",
    "    \n",
    "    # 创建一个新的字典来保存重命名后的字段\n",
    "    new_item = {\"diff\": item[\"diff\"]}\n",
    "    \n",
    "    for idx, key in enumerate(keys_to_rename, 1):\n",
    "        new_key = str(idx)\n",
    "        mapping[item_id][new_key] = key\n",
    "        new_item[new_key] = item[key]\n",
    "    \n",
    "    # 替换原有的item\n",
    "    data[i] = new_item\n",
    "\n",
    "# 将修改后的数据写入新的JSON文件\n",
    "with open('../data/angular_filtered/subsets/human_eval/no_human_ref_test_shuffled.json', 'w', encoding='UTF-8') as file:\n",
    "    json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 将映射关系写入映射文件\n",
    "with open('../data/angular_filtered/subsets/human_eval/no_human_ref_test_mapping.json', 'w', encoding='UTF-8') as file:\n",
    "    json.dump(mapping, file, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc32950341c217eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T09:02:16.170990Z",
     "start_time": "2024-07-08T09:02:16.158435Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成，生成了output.md文件。\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 读取output.json文件\n",
    "with open('../data/angular_filtered/subsets/human_eval/no_human_ref_test_shuffled.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 打开markdown文件用于写入\n",
    "with open('../data/angular_filtered/subsets/human_eval/no_human_ref_test_shuffled.md', 'w') as md_file:\n",
    "    for i, item in enumerate(data, 1):\n",
    "        md_file.write(f\"## Item {i}\\n\\n\")\n",
    "        \n",
    "        if 'diff' in item:\n",
    "            md_file.write(\"```diff\\n\")\n",
    "            md_file.write(f\"{item['diff']}\\n\")\n",
    "            md_file.write(\"```\\n\\n\")\n",
    "        \n",
    "        for key, value in item.items():\n",
    "            if key != 'diff':\n",
    "                md_file.write(f\"{key}: {value}\\n\\n\")\n",
    "        \n",
    "        md_file.write(\"best description number: \\n\\n\")\n",
    "        md_file.write(\"---\\n\\n\")\n",
    "\n",
    "print(\"处理完成，生成了output.md文件。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db0c29a3e87c49c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T09:23:41.244299Z",
     "start_time": "2024-07-12T09:23:41.225126Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read the JSON file\n",
    "with open('../data/angular_filtered/subsets/human_eval/no_human_ref_test_shuffled.json', 'r', encoding='UTF-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Define evaluation criteria\n",
    "evaluation_criteria = \"\"\"\n",
    "Instruction: Your task is to evaluate the commit messages based on the commit code diffs, so first look at the code diffs and determine what the code changes are, and you will be given 2 commit messages generated by different models for each commit. **You need to score each commit message from the following criteria from the perspectives of Expressiveness, and Informativeness, with a minimum score of 0 and a maximum score of 4. Finally, compare the two commit messages given for each commit and select the Best description number (1 or 2).**\n",
    "\n",
    "Evaluation Criteria:\n",
    "| Expressiveness | Description                              |\n",
    "| -------------- | ---------------------------------------- |\n",
    "| 0              | Cannot read and understand.              |\n",
    "| 1              | The content is difficult to read and understand.          |\n",
    "| 2              | The content is somewhat readable and understandable. |\n",
    "| 3              | The content is mostly readable and understandable.   |\n",
    "| 4              | The content is very easy to read and understand.          |\n",
    "\n",
    "| Informativeness | Description                                                  |\n",
    "| --------------- | ------------------------------------------------------------ |\n",
    "| 0               | No information about the code change is provided.             |\n",
    "| 1               | Some crucial information is missing, making it hard to understand the code changes. |\n",
    "| 2               | Some information is missing, but the missing parts are not essential for understanding the code changes. |\n",
    "| 3               | There is some missing information, but none of it is necessary to understand the code changes. |\n",
    "| 4               | All necessary information about the code change is included.                                 |\n",
    "\"\"\"\n",
    "with open('../data/angular_filtered/subsets/human_eval/no_human_ref_test_shuffled.md', 'w', encoding='UTF-8') as md_file:\n",
    "    # Print evaluation criteria once at the beginning\n",
    "    md_file.write(evaluation_criteria)\n",
    "    \n",
    "    # Parse each item and format the output into markdown\n",
    "    for i, item in enumerate(data, 1):\n",
    "        md_file.write(f\"## Item {i}\\n\\n\")\n",
    "        diff = item['diff']\n",
    "        md_file.write(\"````diff\\n\")\n",
    "        md_file.write(diff)\n",
    "        md_file.write(\"````\\n\\n\")\n",
    "    \n",
    "        for key, value in item.items():\n",
    "            if key != 'diff':\n",
    "                value = value.replace('\\n', ', ')\n",
    "                value = value.replace('\\r', ', ')\n",
    "                md_file.write(f\"{key}: '{value}'\\n\\n\")\n",
    "                # md_file.write(\"Conciseness: \\n\")\n",
    "                md_file.write(\"Expressiveness: \\n\")\n",
    "                md_file.write(\"Informativeness: \\n\\n\")\n",
    "        \n",
    "        md_file.write(\"**Best description number:** \\n\\n\")\n",
    "        md_file.write(\"---\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4051f6c9af119fc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T09:23:44.442477Z",
     "start_time": "2024-07-12T09:23:44.429439Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_file(filename, num_items=50):\n",
    "    with open(filename, 'w') as file:\n",
    "        for i in range(1, num_items + 1):\n",
    "            file.write(f\"item{i}-message1:\\n\")\n",
    "            file.write(\"Expressiveness:\\n\")\n",
    "            file.write(\"Informativeness:\\n\\n\")\n",
    "            file.write(f\"item{i}-message2:\\n\")\n",
    "            file.write(\"Expressiveness:\\n\")\n",
    "            file.write(\"Informativeness:\\n\\n\")\n",
    "            file.write(\"Best description number:\\n\\n\")\n",
    "            file.write(\"---\\n\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_file(\"../data/angular_filtered/subsets/human_eval/no_human_ref_test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe999d58439eb20",
   "metadata": {},
   "source": [
    "## Analyze the result from human evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "426b95b5a043abb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T09:04:33.106004Z",
     "start_time": "2024-07-17T09:04:33.074704Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "# 正则表达式匹配模式\n",
    "pattern1 = r'Best description number:\\s*(\\d+)'\n",
    "pattern2 = r'Best description number:\\s*item(\\d+)-message(\\d+)'\n",
    "\n",
    "for file_name in range(1, 7):\n",
    "    # 读取文件内容\n",
    "    file_path = f'../data/angular_filtered/subsets/human_eval/people/no_human_ref_test_{file_name}.txt'\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    json_data = []\n",
    "    # 按照item分割文件内容\n",
    "    items = content.strip().split('---\\n')\n",
    "    best_description_counts = {1: 0, 2: 0}\n",
    "    for index, item in enumerate(items):\n",
    "        item = [text for text in item.strip().split('\\n') if text]\n",
    "        ex_1_match = re.match(\"Expressiveness:\\s*(\\d+)\", item[1])\n",
    "        if not ex_1_match:\n",
    "            raise ValueError(f\"无法匹配表达性: {item[1]}\")\n",
    "        ex_1 = ex_1_match.group(1)\n",
    "        in_1_match = re.match(\"Informativeness:\\s*(\\d+)\", item[2])\n",
    "        if not in_1_match:\n",
    "            raise ValueError(f\"无法匹配信息性: {item[2]}\")\n",
    "        in_1 = in_1_match.group(1)\n",
    "        ex_2_match = re.match(\"Expressiveness:\\s*(\\d+)\", item[4])\n",
    "        if not ex_2_match:\n",
    "            raise ValueError(f\"无法匹配表达性: {item[4]}\")\n",
    "        ex_2 = ex_2_match.group(1)\n",
    "        in_2_match = re.match(\"Informativeness:\\s*(\\d+)\", item[5])\n",
    "        if not in_2_match:\n",
    "            raise ValueError(f\"无法匹配信息性: {item[5]}\")\n",
    "        in_2 = in_2_match.group(1)\n",
    "        # use pattern1 and pattern2 to match the best description number\n",
    "        match1 = re.match(pattern1, item[6])\n",
    "        match2 = re.match(pattern2, item[6])\n",
    "        if match1:\n",
    "            best_id = int(match1.group(1))\n",
    "        elif match2:\n",
    "            best_id = int(match2.group(2))\n",
    "        else:\n",
    "            raise ValueError(f\"无法匹配最佳描述号: {item[6]}\")\n",
    "        data = {\"Item_number\": int(index+1), \"Expressiveness1\": int(ex_1), \"Informativeness1\": int(in_1), \"Expressiveness2\": int(ex_2), \"Informativeness2\": int(in_2), \"Best_description_number\": best_id}\n",
    "        json_data.append(data)\n",
    "    \n",
    "    with open(f'../data/angular_filtered/subsets/human_eval/people/no_human_ref_test_{file_name}.json', 'w', encoding='utf-8') as file:\n",
    "        json.dump(json_data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1577c448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成, 新的JSON文件已保存为 'new_data.json'.\n",
      "处理完成, 新的JSON文件已保存为 'new_data.json'.\n",
      "处理完成, 新的JSON文件已保存为 'new_data.json'.\n",
      "处理完成, 新的JSON文件已保存为 'new_data.json'.\n",
      "处理完成, 新的JSON文件已保存为 'new_data.json'.\n",
      "处理完成, 新的JSON文件已保存为 'new_data.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "# 假设所有 JSON 文件都在 'json_files' 目录中\n",
    "directory = '../data/angular_filtered/subsets/human_eval/people/'\n",
    "\n",
    "\n",
    "\n",
    "with open('../data/angular_filtered/subsets/human_eval/no_human_ref_test_mapping.json', 'r', encoding='UTF-8') as file:\n",
    "    mapping_data = json.load(file)\n",
    "\n",
    "# 遍历目录中的所有 JSON 文件\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.json'):\n",
    "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "            original_data_list = json.load(file)\n",
    "\n",
    "            # 处理每个条目\n",
    "            new_data_list = []\n",
    "            for index, original_data in enumerate(original_data_list):\n",
    "                # 从映射数据中选择正确的映射关系\n",
    "                mapping = mapping_data.get(f'Object-{index+1}', {})\n",
    "                \n",
    "                # 获取映射关系\n",
    "                race_mapping = mapping.get('1', '')\n",
    "                rag_mapping = mapping.get('2', '')\n",
    "                \n",
    "                # 决定使用哪一个值\n",
    "                if race_mapping == 'race':\n",
    "                    expressiveness_race = original_data[\"Expressiveness1\"]\n",
    "                    informativeness_race = original_data[\"Informativeness1\"]\n",
    "                else:\n",
    "                    expressiveness_race = original_data[\"Expressiveness2\"]\n",
    "                    informativeness_race = original_data[\"Informativeness2\"]\n",
    "                \n",
    "                if rag_mapping == 'classified_rag':\n",
    "                    expressiveness_rag = original_data[\"Expressiveness2\"]\n",
    "                    informativeness_rag = original_data[\"Informativeness2\"]\n",
    "                else:\n",
    "                    expressiveness_rag = original_data[\"Expressiveness1\"]\n",
    "                    informativeness_rag = original_data[\"Informativeness1\"]\n",
    "\n",
    "                # 获取Best_description_number对应的模型\n",
    "                best_model_key = str(original_data['Best_description_number'])\n",
    "                best_model = mapping.get(best_model_key, '')\n",
    "\n",
    "                # 生成新的数据结构\n",
    "                new_data = {\n",
    "                    \"Item_number\": original_data[\"Item_number\"],\n",
    "                    \"Expressiveness_race\": expressiveness_race,\n",
    "                    \"Informativeness_race\": informativeness_race,\n",
    "                    \"Expressiveness_rag\": expressiveness_rag,\n",
    "                    \"Informativeness_rag\": informativeness_rag,\n",
    "                    \"Best_description_number\": original_data[\"Best_description_number\"],\n",
    "                    \"Best_model\": best_model\n",
    "                }\n",
    "\n",
    "                new_data_list.append(new_data)\n",
    "\n",
    "            # 写入新的JSON文件\n",
    "            with open(f'{directory}processed_{filename}', 'w', encoding='UTF-8') as file:\n",
    "                json.dump(new_data_list, file, indent=4)\n",
    "\n",
    "            print(\"处理完成, 新的JSON文件已保存为 'new_data.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7ad7f6d36466ca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设所有 JSON 文件都在 'json_files' 目录中\n",
    "directory = '../data/angular_filtered/subsets/human_eval/people/'\n",
    "\n",
    "with open('../data/angular_filtered/subsets/human_eval/no_human_ref_test_mapping.json', 'r', encoding='UTF-8') as file:\n",
    "    mapping = json.load(file)\n",
    "\n",
    "total_count = {'race': 0, 'classified_rag': 0}\n",
    "\n",
    "# 遍历目录中的所有 JSON 文件\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.json') and filename in ['no_human_ref_test_4.json', 'no_human_ref_test_5.json']:\n",
    "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "            # 初始化计数器\n",
    "            count_dict = {'race': 0, 'classified_rag': 0}\n",
    "            # 根据 `Best_description_number` 更新计数器\n",
    "            for index, item in enumerate(data):\n",
    "                mapping_key = f\"Object-{index+1}\"\n",
    "                item_mapping = mapping[mapping_key]\n",
    "                item['mapping_id'] = 0 if item_mapping[str(item['Best_description_number'])]=='race' else 1\n",
    "                count_dict[item_mapping[str(item['Best_description_number'])]] += 1\n",
    "            \n",
    "            with open(f'../data/angular_filtered/subsets/human_eval/people/{filename}', 'w', encoding='utf-8') as file:\n",
    "                json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "            # 绘制当前文件的条形图\n",
    "            labels = ['race', 'classified_rag']\n",
    "            counts = [count_dict['race'], count_dict['classified_rag']]\n",
    "            total_count['race'] += count_dict['race']\n",
    "            total_count['classified_rag'] += count_dict['classified_rag']\n",
    "\n",
    "            plt.figure()\n",
    "            plt.bar(labels, counts, color=['blue', 'green'])\n",
    "            plt.xlabel('Best Description Number')\n",
    "            plt.ylabel('Count')\n",
    "            plt.title(f'Count of Best Description Numbers 1 and 2 in {filename}')\n",
    "            \n",
    "            # 保存图表到文件，也可以使用 plt.show() 直接显示\n",
    "            plt.savefig(f'{directory}{filename}_bar_chart.png')\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "be086734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.bar(['race', 'classified_rag'], [total_count['race'], total_count['classified_rag']], color=['#ADD8E6', '#1E90FF'])\n",
    "# plt.xlabel('Best Description Number')\n",
    "plt.ylabel('Counts')\n",
    "# plt.title(f'Count of Best Description Numbers of RACE and Classified RAG')\n",
    "\n",
    "# 保存图表到文件，也可以使用 plt.show() 直接显示\n",
    "plt.savefig(f'{directory}total_bar_chart_two.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c100e884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "with open ('../data/angular_filtered/subsets/human_eval/people/processed/processed_no_human_ref_test_5.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 统计data里item中Best_model='race'的数量\n",
    "count = 0\n",
    "for item in data:\n",
    "    if item['Best_model'] == 'race':\n",
    "        count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd66345",
   "metadata": {},
   "source": [
    "## kendalltau correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ea86f6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Volunteer 2  Volunteer 3  Volunteer 4  Volunteer 5  Volunteer 6  \\\n",
      "Volunteer 2     0.000000     0.361917     0.350649     0.431818     0.551846   \n",
      "Volunteer 3     0.361917     0.000000     0.361917     0.361917     0.332267   \n",
      "Volunteer 4     0.350649     0.361917     0.000000     0.675325     0.388578   \n",
      "Volunteer 5     0.431818     0.361917     0.675325     0.000000     0.470212   \n",
      "Volunteer 6     0.551846     0.332267     0.388578     0.470212     0.000000   \n",
      "Volunteer 1     0.394506     0.226526     0.313665     0.475347     0.596776   \n",
      "\n",
      "             Volunteer 1  \n",
      "Volunteer 2     0.394506  \n",
      "Volunteer 3     0.226526  \n",
      "Volunteer 4     0.313665  \n",
      "Volunteer 5     0.475347  \n",
      "Volunteer 6     0.596776  \n",
      "Volunteer 1     0.000000  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import kendalltau\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "# 定义一个函数来读取JSON文件并提取mapping_id\n",
    "def extract_mapping_ids(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        # 提取每个item的mapping_id\n",
    "        mapping_ids = [item['Best_description_number'] for item in data]\n",
    "    return mapping_ids\n",
    "\n",
    "# 定义一个函数来读取JSON文件并提取mapping_id\n",
    "def extract_expressiveness(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        # 提取每个item的expressiveness1\n",
    "        expressiveness = [item['Best_description_number'] for item in data]\n",
    "    return expressiveness\n",
    "\n",
    "# 获取所有志愿者的JSON文件路径列表\n",
    "file_paths = glob.glob('../data/angular_filtered/subsets/human_eval/people/processed/*.json')\n",
    "\n",
    "# 初始化一个字典来存储所有志愿者的数据\n",
    "volunteer_data = {}\n",
    "\n",
    "\n",
    "\n",
    "# 读取每个文件并提取数据\n",
    "for i, file_path in enumerate(file_paths):\n",
    "    # 使用正则表达式提取数字\n",
    "    # 从路径中提取文件名\n",
    "    filename = os.path.basename(file_path)\n",
    "    match = re.search(r'(\\d+)\\.json$', filename)\n",
    "    if match:\n",
    "        vol_id = match.group(1)\n",
    "        volunteer_name = f'Volunteer {vol_id}'\n",
    "        volunteer_data[volunteer_name] = extract_expressiveness(file_path)\n",
    "    else:\n",
    "        print('match failed')\n",
    "\n",
    "# 将数据转换为DataFrame\n",
    "df = pd.DataFrame(volunteer_data)\n",
    "\n",
    "# 获取志愿者数量\n",
    "num_volunteers = df.shape[1]\n",
    "\n",
    "# 初始化一个矩阵来存储Kendall's Tau值\n",
    "tau_matrix = np.zeros((num_volunteers, num_volunteers))\n",
    "\n",
    "# 计算成对的Kendall's Tau值\n",
    "for i in range(num_volunteers):\n",
    "    for j in range(i + 1, num_volunteers):\n",
    "        tau, _ = kendalltau(df.iloc[:, i], df.iloc[:, j])\n",
    "        tau_matrix[i, j] = tau\n",
    "        tau_matrix[j, i] = tau\n",
    "\n",
    "# 将Tau矩阵转换为DataFrame以便于读取\n",
    "tau_df = pd.DataFrame(tau_matrix, columns=df.columns, index=df.columns)\n",
    "\n",
    "print(tau_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c0809e",
   "metadata": {},
   "source": [
    "## Krippendorff’s alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "376ddd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from numpy import transpose, identity\n",
    "\n",
    "def calculate_krippendorffs_alpha(data_for_alpha):\n",
    "    # This is 12x4 matrix where each row represents all 4 reviewers' ratings for one store,\n",
    "    # and each column represents each reviewer's ratings for all 12 store locations.\n",
    "    ratings = data_for_alpha.tolist()\n",
    "\n",
    "    # Helper function to calculate the agreement table:\n",
    "    #   - each row i corresponds to a store\n",
    "    #   - each column k represents a possible rating (1, 2, 3, or 4, in this case)\n",
    "    #   - each table entry r_ik is the number of reviewers who gave store i rating k\n",
    "    def get_agreement_table(ratings, categories):\n",
    "        agreement = []\n",
    "\n",
    "        for store in ratings:\n",
    "            category_counts = list(map(lambda category: store.count(category), categories))\n",
    "            if sum(category_counts) > 1: # Ignore any stores with less than two ratings\n",
    "                agreement.append(category_counts)\n",
    "\n",
    "        return agreement\n",
    "\n",
    "    # Helper function that returns the weighted count of reviewers who gave store i a rating that at least partially matched category k\n",
    "    # (For categorical data this is just the number of reviewers who picked category k)\n",
    "    def get_weighted_rater_count(weights_k, agreement_i):\n",
    "        weighted_count = 0\n",
    "        for i in range(len(agreement_i)):\n",
    "            weighted_count += weights_k[i] * agreement_i[i]\n",
    "        return weighted_count\n",
    "\n",
    "    # STEPS 1 & 2: CLEANING THE DATA AND BUILDING THE AGREEMENT TABLE\n",
    "\n",
    "    # Get the set of all possible ratings our reviewer can give\n",
    "    rating_categories = set(chain(*ratings)) \n",
    "    # Remove the placeholder value for missing ratings\n",
    "    # rating_categories.remove(None) \n",
    "    agreement_table = get_agreement_table(ratings, rating_categories)\n",
    "\n",
    "    # n is the number of stores (more generally, the number of items being rated)\n",
    "    n = len(agreement_table) \n",
    "    # q is the number of possible rating categories\n",
    "    q = len(rating_categories) \n",
    "\n",
    "    # get an array with r_i (the total number of reviewers who rated the ith store) for all stores\n",
    "    raters_per_store = list(map(lambda r_ik: sum(r_ik), agreement_table)) \n",
    "    # rhat is the average number of reviewers who rated each store\n",
    "    rhat = sum(raters_per_store) / n \n",
    "\n",
    "    # STEP 3: CHOOSING A WEIGHT FUNCTION\n",
    "\n",
    "    # categorical weights are just the identity matrix (1 if the category matches and 0 otherwise)\n",
    "    weights = identity(len(rating_categories)) \n",
    "\n",
    "    # STEP 4: CALCULATING OBSERVED PERCENT AGREEMENT (p_a)\n",
    "    percent_agreement = 0\n",
    "    for i in range(n): # Find the percent agreement for every store\n",
    "        percent_agreement_i = 0\n",
    "        for k in range(q): # Find the percent agreement for every category for the ith store\n",
    "            rhat_ik = get_weighted_rater_count(weights[k], agreement_table[i])\n",
    "            ri = sum(agreement_table[i]) # Number of people who rated this store\n",
    "            percent_agreement_i_k = (agreement_table[i][k] * (rhat_ik - 1)) / (rhat * (ri - 1))\n",
    "            percent_agreement_i += percent_agreement_i_k\n",
    "        \n",
    "        percent_agreement += percent_agreement_i\n",
    "\n",
    "    pa_prime = percent_agreement / n # Find the average store-level percent agreement\n",
    "\n",
    "    total_rating_count = n * rhat\n",
    "    pa = (1 - 1/(total_rating_count)) * pa_prime + 1/total_rating_count\n",
    "\n",
    "    # STEP 5: CALCULATING EXPECTED PERCENT AGREEMENT (p_e)\n",
    "    category_averages = list(map(lambda category: sum(category) / n, transpose(agreement_table)))\n",
    "    classification_probabilities = list(map(lambda category_average: category_average / rhat, category_averages))\n",
    "\n",
    "    pe = 0\n",
    "    for k in range(q): # For every possible pair of rating categories\n",
    "        for l in range(q):\n",
    "            # Add the probability of those two categories being chosen at random, weighted by how closely they match\n",
    "            pe += classification_probabilities[k] * classification_probabilities[l] * weights[k][l] \n",
    "\n",
    "    # STEP 6: CALCULATE KRIPPENDORFF'S ALPHA\n",
    "    alpha = (pa - pe) / (1 - pe)\n",
    "    print(f\"Krippendorff's alpha: {alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e4bff600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krippendorff's alpha: 0.6785714285714285\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import krippendorff\n",
    "\n",
    "# 定义一个函数来读取JSON文件并提取mapping_id\n",
    "def extract_mapping_ids(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        # 提取每个item的mapping_id\n",
    "        mapping_ids = [item['Best_description_number'] for item in data]\n",
    "    return mapping_ids\n",
    "\n",
    "# 获取所有志愿者的JSON文件路径列表\n",
    "# file_paths = glob.glob('../data/angular_filtered/subsets/human_eval/people/*.json')\n",
    "file_paths = ['../data/angular_filtered/subsets/human_eval/people/no_human_ref_test_4.json', '../data/angular_filtered/subsets/human_eval/people/no_human_ref_test_5.json']\n",
    "# 初始化一个字典来存储所有志愿者的数据\n",
    "volunteer_data = {}\n",
    "\n",
    "# 读取每个文件并提取数据\n",
    "for i, file_path in enumerate(file_paths):\n",
    "    volunteer_name = f'Volunteer {i + 1}'\n",
    "    volunteer_data[volunteer_name] = extract_mapping_ids(file_path)\n",
    "\n",
    "# 将数据转换为DataFrame\n",
    "df = pd.DataFrame(volunteer_data)\n",
    "\n",
    "# 将DataFrame转换为Krippendorff's alpha所需的格式\n",
    "data_for_alpha = df.to_numpy()\n",
    "\n",
    "calculate_krippendorffs_alpha(data_for_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf852fc",
   "metadata": {},
   "source": [
    "## Wilcoxon signed-rank tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0613b7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wilcoxon signed-rank test statistic: [73.]\n",
      "P-value: [4.21168324e-05]\n",
      "Reject the null hypothesis: Your method is significantly different from the previous method.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "# 定义一个函数来读取JSON文件并提取mapping_id\n",
    "def extract_rag(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        # 提取每个item的mapping_id\n",
    "        mapping_ids = [item['Informativeness_rag'] for item in data]\n",
    "    return mapping_ids\n",
    "\n",
    "def extract_race(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        # 提取每个item的mapping_id\n",
    "        mapping_ids = [item['Informativeness_race'] for item in data]\n",
    "    return mapping_ids\n",
    "\n",
    "# 获取所有志愿者的JSON文件路径列表\n",
    "# file_paths = glob.glob('../data/angular_filtered/subsets/human_eval/people/*.json')\n",
    "file_paths = ['../data/angular_filtered/subsets/human_eval/people/processed/processed_no_human_ref_test_5.json']\n",
    "# 初始化一个字典来存储所有志愿者的数据\n",
    "volunteer_data = {}\n",
    "\n",
    "# 读取每个文件并提取数据\n",
    "for i, file_path in enumerate(file_paths):\n",
    "    volunteer_name = f'Volunteer {i + 1}'\n",
    "    volunteer_data[volunteer_name] = extract_rag(file_path)\n",
    "\n",
    "# 将数据转换为DataFrame\n",
    "df_rag = pd.DataFrame(volunteer_data)\n",
    "\n",
    "volunteer_data = {}\n",
    "for i, file_path in enumerate(file_paths):\n",
    "    volunteer_name = f'Volunteer {i + 1}'\n",
    "    volunteer_data[volunteer_name] = extract_race(file_path)\n",
    "\n",
    "# 将数据转换为DataFrame\n",
    "df_race = pd.DataFrame(volunteer_data)\n",
    "\n",
    "# Sample data\n",
    "previous_method = df_race.to_numpy()\n",
    "your_method = df_rag.to_numpy()\n",
    "\n",
    "# Conducting the Wilcoxon signed-rank test\n",
    "statistic, p_value = wilcoxon(your_method, previous_method)\n",
    "\n",
    "print(f'Wilcoxon signed-rank test statistic: {statistic}')\n",
    "print(f'P-value: {p_value}')\n",
    "\n",
    "# Decision\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: Your method is significantly different from the previous method.\")\n",
    "else:\n",
    "    print(\"Do not reject the null hypothesis: No significant difference between the methods.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff187bd",
   "metadata": {},
   "source": [
    "## Calculate indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d156ddfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各指标的平均值:\n",
      "Expressiveness1     3.36\n",
      "Expressiveness2     3.50\n",
      "Informativeness1    3.12\n",
      "Informativeness2    3.16\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取JSON文件\n",
    "with open('../data/angular_filtered/subsets/human_eval/people/no_human_ref_test_1.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 转换为DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 选择需要的列\n",
    "df = df[['Expressiveness1', 'Expressiveness2', 'Informativeness1', 'Informativeness2']]\n",
    "\n",
    "# 计算每列的平均值\n",
    "averages = df.mean()\n",
    "\n",
    "print(\"各指标的平均值:\")\n",
    "print(averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ee526d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8h/wq7f013d54v9m2hktbvqpfx00000gn/T/ipykernel_59256/3997999270.py:24: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  average_df = pd.concat([average_df, pd.DataFrame([averages])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "with open('../data/angular_filtered/subsets/human_eval/no_human_ref_test_mapping.json', 'r', encoding='UTF-8') as file:\n",
    "    mapping = json.load(file)\n",
    "\n",
    "average_df = pd.DataFrame(columns=['Expressiveness_race', 'Informativeness_race', 'Expressiveness_rag', 'Informativeness_rag'])\n",
    "\n",
    "for num in [4, 5]:\n",
    "    # 读取JSON文件\n",
    "    with open(f'../data/angular_filtered/subsets/human_eval/people/no_human_ref_test_{num}.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "    # 创建空的DataFrame\n",
    "    df = pd.DataFrame(columns=['Expressiveness_race', 'Informativeness_race', 'Expressiveness_rag', 'Informativeness_rag'])\n",
    "    for index, item in enumerate(data):\n",
    "        mapping_key = f\"Object-{index+1}\"\n",
    "        item_mapping = mapping[mapping_key]\n",
    "        if item_mapping[\"1\"] == 'race':\n",
    "            new_row = {'Expressiveness_race': item['Expressiveness1'], 'Expressiveness_rag': item['Expressiveness2'], \n",
    "                       'Informativeness_race': item['Informativeness1'], 'Informativeness_rag': item['Informativeness2']}\n",
    "        else:\n",
    "            new_row = {'Expressiveness_race': item['Expressiveness2'], 'Expressiveness_rag': item['Expressiveness1'],\n",
    "                       'Informativeness_race': item['Informativeness2'], 'Informativeness_rag': item['Informativeness1']}\n",
    "        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    # 计算每列的平均值\n",
    "    averages = df.mean()\n",
    "    average_df = pd.concat([average_df, pd.DataFrame([averages])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f76eecec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Expressiveness of race is 3.7800000000000002, with a standard deviation of 0.05656854249492354\n"
     ]
    }
   ],
   "source": [
    "mean_expressiveness_race = average_df['Expressiveness_race'].mean()\n",
    "\n",
    "std_expressiveness_race = average_df['Expressiveness_race'].std()\n",
    "print(f\"Mean Expressiveness of race is {mean_expressiveness_race}, with a standard deviation of {std_expressiveness_race}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f232dff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Expressiveness of rag is 3.78, with a standard deviation of 0.028284271247461926\n"
     ]
    }
   ],
   "source": [
    "mean_expressiveness_rag = average_df['Expressiveness_rag'].mean()\n",
    "\n",
    "std_expressiveness_rag = average_df['Expressiveness_rag'].std()\n",
    "print(f\"Mean Expressiveness of rag is {mean_expressiveness_rag}, with a standard deviation of {std_expressiveness_rag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1ea6a8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Informativeness of race is 2.8, with a standard deviation of 0.16970562748477125\n"
     ]
    }
   ],
   "source": [
    "mean_informativeness_race = average_df['Informativeness_race'].mean()\n",
    "std_informativeness_race = average_df['Informativeness_race'].std()\n",
    "print(f\"Mean Informativeness of race is {mean_informativeness_race}, with a standard deviation of {std_informativeness_race}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4a97ca63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Informativeness of rag is 3.41, with a standard deviation of 0.1555634918610406\n"
     ]
    }
   ],
   "source": [
    "mean_informativeness_rag = average_df['Informativeness_rag'].mean()\n",
    "std_informativeness_rag = average_df['Informativeness_rag'].std()\n",
    "print(f\"Mean Informativeness of rag is {mean_informativeness_rag}, with a standard deviation of {std_informativeness_rag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f2fe87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
