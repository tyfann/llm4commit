{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/mjpost/sacrebleu/blob/master/sacrebleu/tokenizers/tokenizer_13a.py\n",
    "# Copyright 2020 SacreBLEU Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import re\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "class BaseTokenizer:\n",
    "    \"\"\"A base dummy tokenizer to derive from.\"\"\"\n",
    "\n",
    "    def signature(self):\n",
    "        \"\"\"\n",
    "        Returns a signature for the tokenizer.\n",
    "        :return: signature string\n",
    "        \"\"\"\n",
    "        return \"none\"\n",
    "\n",
    "    def __call__(self, line):\n",
    "        \"\"\"\n",
    "        Tokenizes an input line with the tokenizer.\n",
    "        :param line: a segment to tokenize\n",
    "        :return: the tokenized line\n",
    "        \"\"\"\n",
    "        return line\n",
    "\n",
    "\n",
    "class TokenizerRegexp(BaseTokenizer):\n",
    "    def signature(self):\n",
    "        return \"re\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._re = [\n",
    "            # language-dependent part (assuming Western languages)\n",
    "            (re.compile(r\"([\\{-\\~\\[-\\` -\\&\\(-\\+\\:-\\@\\/])\"), r\" \\1 \"),\n",
    "            # tokenize period and comma unless preceded by a digit\n",
    "            (re.compile(r\"([^0-9])([\\.,])\"), r\"\\1 \\2 \"),\n",
    "            # tokenize period and comma unless followed by a digit\n",
    "            (re.compile(r\"([\\.,])([^0-9])\"), r\" \\1 \\2\"),\n",
    "            # tokenize dash when preceded by a digit\n",
    "            (re.compile(r\"([0-9])(-)\"), r\"\\1 \\2 \"),\n",
    "            # one space only between words\n",
    "            # NOTE: Doing this in Python (below) is faster\n",
    "            # (re.compile(r'\\s+'), r' '),\n",
    "        ]\n",
    "\n",
    "    @lru_cache(maxsize=2**16)\n",
    "    def __call__(self, line):\n",
    "        \"\"\"Common post-processing tokenizer for `13a` and `zh` tokenizers.\n",
    "        :param line: a segment to tokenize\n",
    "        :return: the tokenized line\n",
    "        \"\"\"\n",
    "        for (_re, repl) in self._re:\n",
    "            line = _re.sub(repl, line)\n",
    "\n",
    "        # no leading or trailing spaces, single space within words\n",
    "        # return ' '.join(line.split())\n",
    "        # This line is changed with regards to the original tokenizer (seen above) to return individual words\n",
    "        return line.split()\n",
    "\n",
    "\n",
    "class Tokenizer13a(BaseTokenizer):\n",
    "    def signature(self):\n",
    "        return \"13a\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._post_tokenizer = TokenizerRegexp()\n",
    "\n",
    "    @lru_cache(maxsize=2**16)\n",
    "    def __call__(self, line):\n",
    "        \"\"\"Tokenizes an input line using a relatively minimal tokenization\n",
    "        that is however equivalent to mteval-v13a, used by WMT.\n",
    "\n",
    "        :param line: a segment to tokenize\n",
    "        :return: the tokenized line\n",
    "        \"\"\"\n",
    "\n",
    "        # language-independent part:\n",
    "        line = line.replace(\"<skipped>\", \"\")\n",
    "        line = line.replace(\"-\\n\", \"\")\n",
    "        line = line.replace(\"\\n\", \" \")\n",
    "\n",
    "        if \"&\" in line:\n",
    "            line = line.replace(\"&quot;\", '\"')\n",
    "            line = line.replace(\"&amp;\", \"&\")\n",
    "            line = line.replace(\"&lt;\", \"<\")\n",
    "            line = line.replace(\"&gt;\", \">\")\n",
    "\n",
    "        return self._post_tokenizer(f\" {line} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer13a()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average length of RACE in angular format dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.35187969924812\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('../data/angular_filtered/subsets/generation/test_race_v1.json') as f:\n",
    "    race_data = json.load(f)\n",
    "\n",
    "race_length_list = []\n",
    "for item in race_data:\n",
    "    race_length_list.append(len(tokenizer(item['race'])))\n",
    "print(sum(race_length_list) / len(race_length_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average length of RAG in angular format dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.25062656641604\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('../data/angular_filtered/subsets/generation/test_gpt35_model_classified_rag.json') as f:\n",
    "    rag_data = json.load(f)\n",
    "\n",
    "rag_length_list = []\n",
    "for item in rag_data:\n",
    "    rag_length_list.append(len(tokenizer(item['chatgpt_rag'])))\n",
    "print(sum(rag_length_list) / len(rag_length_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average length of reference message in angular format dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.685213032581453\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('../data/angular_filtered/subsets/generation/test_gpt35_model_classified_rag.json') as f:\n",
    "    ref_data = json.load(f)\n",
    "\n",
    "ref_length_list = []\n",
    "for item in ref_data:\n",
    "    ref_length_list.append(len(tokenizer(item['msg'])))\n",
    "print(sum(ref_length_list) / len(ref_length_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import meteor_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38522866d8c9440c8c1684c92e415ac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.7714985257158095, 'precisions': [0.9375, 0.9333333333333333, 0.9285714285714286, 0.9230769230769231], 'brevity_penalty': 0.8290291181804004, 'length_ratio': 0.8421052631578947, 'translation_length': 16, 'reference_length': 19}\n"
     ]
    }
   ],
   "source": [
    "predictions = [\"add select_order_by_with_table_star_table_name\"]\n",
    "references = ['change name select_order_by_with_table_star_table_name for parser']\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.14285714285714285, 'rouge2': 0.0, 'rougeL': 0.14285714285714285, 'rougeLsum': 0.14285714285714285}\n"
     ]
    }
   ],
   "source": [
    "predictions = [\"docs: add note about firebase\"]\n",
    "references = ['docs(auth): mention of package requirement for server implementation']\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0, 'precisions': [0.6363636363636364, 0.3, 0.0, 0.0], 'brevity_penalty': 0.9131007162822622, 'length_ratio': 0.9166666666666666, 'translation_length': 11, 'reference_length': 12}\n"
     ]
    }
   ],
   "source": [
    "predictions = [\"docs: Update warning message for programmatic server implementation\"]\n",
    "references = ['docs(auth): mention of package requirement for server implementation']\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.47058823529411764, 'rouge2': 0.13333333333333333, 'rougeL': 0.47058823529411764, 'rougeLsum': 0.47058823529411764}\n"
     ]
    }
   ],
   "source": [
    "predictions = [\"docs: Update warning message for programmatic server implementation\"]\n",
    "references = ['docs(auth): mention of package requirement for server implementation']\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36787944117144233"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(1-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 1.0,\n",
       " 'precisions': [1.0, 1.0, 1.0, 1.0],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.1666666666666667,\n",
       " 'translation_length': 7,\n",
       " 'reference_length': 6}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_enums(hypothesis, reference, preprocess=str.lower):\n",
    "    \"\"\"\n",
    "    Takes in string inputs for hypothesis and reference and returns\n",
    "    enumerated word lists for each of them\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :type hypothesis: str\n",
    "    :param reference: reference string\n",
    "    :type reference: str\n",
    "    :preprocess: preprocessing method (default str.lower)\n",
    "    :type preprocess: method\n",
    "    :return: enumerated words list\n",
    "    :rtype: list of 2D tuples, list of 2D tuples\n",
    "    \"\"\"\n",
    "    hypothesis_list = list(enumerate(preprocess(hypothesis).split()))\n",
    "    reference_list = list(enumerate(preprocess(reference).split()))\n",
    "    return hypothesis_list, reference_list\n",
    "\n",
    "\n",
    "def exact_match(hypothesis, reference):\n",
    "    \"\"\"\n",
    "    matches exact words in hypothesis and reference\n",
    "    and returns a word mapping based on the enumerated\n",
    "    word id between hypothesis and reference\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :type hypothesis: str\n",
    "    :param reference: reference string\n",
    "    :type reference: str\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    hypothesis_list, reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _match_enums(hypothesis_list, reference_list)\n",
    "\n",
    "\n",
    "def _match_enums(enum_hypothesis_list, enum_reference_list):\n",
    "    \"\"\"\n",
    "    matches exact words in hypothesis and reference and returns\n",
    "    a word mapping between enum_hypothesis_list and enum_reference_list\n",
    "    based on the enumerated word id.\n",
    "\n",
    "    :param enum_hypothesis_list: enumerated hypothesis list\n",
    "    :type enum_hypothesis_list: list of tuples\n",
    "    :param enum_reference_list: enumerated reference list\n",
    "    :type enum_reference_list: list of 2D tuples\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    word_match = []\n",
    "    for i in range(len(enum_hypothesis_list))[::-1]:\n",
    "        for j in range(len(enum_reference_list))[::-1]:\n",
    "            if enum_hypothesis_list[i][1] == enum_reference_list[j][1]:\n",
    "                word_match.append(\n",
    "                    (enum_hypothesis_list[i][0], enum_reference_list[j][0])\n",
    "                )\n",
    "                (enum_hypothesis_list.pop(i)[1], enum_reference_list.pop(j)[1])\n",
    "                break\n",
    "    return word_match, enum_hypothesis_list, enum_reference_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis=\"docs: Update warning message for programmatic server implementation\"\n",
    "reference = \"docs(auth): mention of package requirement for server implementation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(7, 7), (6, 6), (4, 5)],\n",
       " [(0, 'docs:'),\n",
       "  (1, 'update'),\n",
       "  (2, 'warning'),\n",
       "  (3, 'message'),\n",
       "  (5, 'programmatic')],\n",
       " [(0, 'docs(auth):'),\n",
       "  (1, 'mention'),\n",
       "  (2, 'of'),\n",
       "  (3, 'package'),\n",
       "  (4, 'requirement')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "\n",
    "#refs_new = []\n",
    "for ele in reference:\n",
    "    if ele in punc:\n",
    "        reference = reference.replace(ele, \" \")\n",
    "        reference = re.sub(r'\\s+', ' ', reference).strip()\n",
    "\n",
    "for ele in hypothesis:\n",
    "    if ele in punc:\n",
    "        hypothesis = hypothesis.replace(ele, \" \")\n",
    "        hypothesis = re.sub(r'\\s+', ' ', hypothesis).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_list, reference_list = _generate_enums(hypothesis, reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'docs'),\n",
       " (1, 'update'),\n",
       " (2, 'warning'),\n",
       " (3, 'message'),\n",
       " (4, 'for'),\n",
       " (5, 'programmatic'),\n",
       " (6, 'server'),\n",
       " (7, 'implementation')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'docs'),\n",
       " (1, 'auth'),\n",
       " (2, 'mention'),\n",
       " (3, 'of'),\n",
       " (4, 'package'),\n",
       " (5, 'requirement'),\n",
       " (6, 'for'),\n",
       " (7, 'server'),\n",
       " (8, 'implementation')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(7, 8), (6, 7), (4, 6), (0, 0)],\n",
       " [(1, 'update'), (2, 'warning'), (3, 'message'), (5, 'programmatic')],\n",
       " [(1, 'auth'), (2, 'mention'), (3, 'of'), (4, 'package'), (5, 'requirement')])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_match(hypothesis, reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6796116504854369"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1 = ((7/12)* (7/10)) / (0.85*(7/12) + 0.15*(7/10))\n",
    "F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6896551724137931"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1 = ((7/11)* (7/10)) / (0.85*(7/11) + 0.15*(7/10))\n",
    "F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7106598984771574"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1 = ((7/9)* (7/10)) / (0.85*(7/9) + 0.15*(7/10))\n",
    "F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6363636363636365"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1 = ((7/10)* (7/12)) / (0.5*(7/12) + 0.5*(7/10))\n",
    "F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B-Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs ( auth ) : mention of package requirement for server implementation\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def splitPuncts(line):\n",
    "    # This regex matches words and punctuation, treating punctuation as separate tokens\n",
    "    return ' '.join(re.findall(r'\\w+|[^\\w\\s]', line))\n",
    "\n",
    "# Example usage:\n",
    "text = \"docs(auth): mention of package requirement for server implementation\"\n",
    "print(splitPuncts(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['docs',\n",
       " '(',\n",
       " 'auth',\n",
       " ')',\n",
       " ':',\n",
       " 'mention',\n",
       " 'of',\n",
       " 'package',\n",
       " 'requirement',\n",
       " 'for',\n",
       " 'server',\n",
       " 'implementation']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitPuncts(text).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B-Moses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "\n",
    "\n",
    "def _get_ngrams(segment, max_order):\n",
    "  \"\"\"Extracts all n-grams upto a given maximum order from an input segment.\n",
    "\n",
    "  Args:\n",
    "    segment: text segment from which n-grams will be extracted.\n",
    "    max_order: maximum length in tokens of the n-grams returned by this\n",
    "        methods.\n",
    "\n",
    "  Returns:\n",
    "    The Counter containing all n-grams upto max_order in segment\n",
    "    with a count of how many times each n-gram occurred.\n",
    "  \"\"\"\n",
    "  ngram_counts = collections.Counter()\n",
    "  for order in range(1, max_order + 1):\n",
    "    for i in range(0, len(segment) - order + 1):\n",
    "      ngram = tuple(segment[i:i+order])\n",
    "      ngram_counts[ngram] += 1\n",
    "  return ngram_counts\n",
    "\n",
    "\n",
    "def compute_bleu(reference_corpus, translation_corpus, max_order=4,\n",
    "                 smooth=False):\n",
    "  \"\"\"Computes BLEU score of translated segments against one or more references.\n",
    "\n",
    "  Args:\n",
    "    reference_corpus: list of lists of references for each translation. Each\n",
    "        reference should be tokenized into a list of tokens.\n",
    "    translation_corpus: list of translations to score. Each translation\n",
    "        should be tokenized into a list of tokens.\n",
    "    max_order: Maximum n-gram order to use when computing BLEU score.\n",
    "    smooth: Whether or not to apply Lin et al. 2004 smoothing.\n",
    "\n",
    "  Returns:\n",
    "    3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n",
    "    precisions and brevity penalty.\n",
    "  \"\"\"\n",
    "  matches_by_order = [0] * max_order\n",
    "  possible_matches_by_order = [0] * max_order\n",
    "  reference_length = 0\n",
    "  translation_length = 0\n",
    "  for (references, translation) in zip(reference_corpus,\n",
    "                                       translation_corpus):\n",
    "    reference_length += min(len(r) for r in references)\n",
    "    translation_length += len(translation)\n",
    "\n",
    "    merged_ref_ngram_counts = collections.Counter()\n",
    "    for reference in references:\n",
    "      merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n",
    "    translation_ngram_counts = _get_ngrams(translation, max_order)\n",
    "    overlap = translation_ngram_counts & merged_ref_ngram_counts\n",
    "    for ngram in overlap:\n",
    "      matches_by_order[len(ngram)-1] += overlap[ngram]\n",
    "    for order in range(1, max_order+1):\n",
    "      possible_matches = len(translation) - order + 1\n",
    "      if possible_matches > 0:\n",
    "        possible_matches_by_order[order-1] += possible_matches\n",
    "\n",
    "  precisions = [0] * max_order\n",
    "  for i in range(0, max_order):\n",
    "    if smooth:\n",
    "      precisions[i] = ((matches_by_order[i] + 1.) /\n",
    "                       (possible_matches_by_order[i] + 1.))\n",
    "    else:\n",
    "      if possible_matches_by_order[i] > 0:\n",
    "        precisions[i] = (float(matches_by_order[i]) /\n",
    "                         possible_matches_by_order[i])\n",
    "      else:\n",
    "        precisions[i] = 0.0\n",
    "\n",
    "  if min(precisions) > 0:\n",
    "    p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n",
    "    geo_mean = math.exp(p_log_sum)\n",
    "  else:\n",
    "    geo_mean = 0\n",
    "\n",
    "  ratio = float(translation_length) / reference_length\n",
    "\n",
    "  if ratio > 1.0:\n",
    "    bp = 1.\n",
    "  else:\n",
    "    bp = math.exp(1 - 1. / ratio)\n",
    "\n",
    "  bleu = geo_mean * bp\n",
    "\n",
    "  return (bleu, precisions, bp, ratio, translation_length, reference_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis=\"Upgrade to plexus - utils 3 . 0 . 24\"\n",
    "reference = \"Upgrade to Plexus Utils 3 . 0 . 24\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, [0.56, 0.0, 0.0, 0.0], 1.0, 2.7777777777777777, 25, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_bleu(splitPuncts(reference).split(), splitPuncts(hypothesis).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "\n",
    "def bleu_count(hypothesis, references, max_n=4):\n",
    "    ret_len_hyp = 0\n",
    "    ret_len_ref = 0\n",
    "    ret_clip_count = [0]*max_n\n",
    "    ret_count = [0]*max_n\n",
    "    for m in range(len(hypothesis)):\n",
    "        hyp, ref = hypothesis[m], references[m]\n",
    "        x = hyp.split()\n",
    "        y = [r.split() for r in ref]\n",
    "        x_len = len(x)\n",
    "        y_len = [len(s) for s in y]\n",
    "        n_ref = len(ref)\n",
    "\n",
    "        closest_diff = 9999\n",
    "        closest_length = 9999\n",
    "        ref_ngram = dict()\n",
    "\n",
    "        for i in range(n_ref):\n",
    "            diff = abs(y_len[i]-x_len)\n",
    "            if diff < closest_diff:\n",
    "                closest_diff = diff\n",
    "                closest_length = y_len[i]\n",
    "            elif diff==closest_diff and y_len[i] < closest_length:\n",
    "                closest_length = y_len[i]\n",
    "\n",
    "            for n in range(max_n):\n",
    "                sent_ngram = dict()\n",
    "                for st in range(0, y_len[i]-n):\n",
    "                    ngram = \"%d\"%(n+1)\n",
    "                    for k in range(n+1):\n",
    "                        j = st+k\n",
    "                        ngram += \" %s\"%(y[i][j])\n",
    "                    if ngram not in sent_ngram:\n",
    "                        sent_ngram[ngram]=0\n",
    "                    sent_ngram[ngram]+=1\n",
    "                for ngram in sent_ngram.keys():\n",
    "                    if ngram not in ref_ngram or ref_ngram[ngram]<sent_ngram[ngram]:\n",
    "                        ref_ngram[ngram] = sent_ngram[ngram]\n",
    "\n",
    "        ret_len_hyp += x_len\n",
    "        ret_len_ref += closest_length\n",
    "\n",
    "        for n in range(max_n):\n",
    "            hyp_ngram = dict()\n",
    "            for st in range(0, x_len-n):\n",
    "                ngram = \"%d\"%(n+1)\n",
    "                for k in range(n+1):\n",
    "                    j = st+k\n",
    "                    ngram += \" %s\"%(x[j])\n",
    "                if ngram not in hyp_ngram:\n",
    "                    hyp_ngram[ngram]=0\n",
    "                hyp_ngram[ngram]+=1\n",
    "            for ngram in hyp_ngram.keys():\n",
    "                if ngram in ref_ngram:\n",
    "                    ret_clip_count[n] += min(ref_ngram[ngram], hyp_ngram[ngram])\n",
    "                ret_count[n] += hyp_ngram[ngram]\n",
    "\n",
    "    return ret_clip_count, ret_count, ret_len_hyp, ret_len_ref\n",
    "\n",
    "def corpus_bleu(hypothesis, references, max_n=4):\n",
    "    assert(len(hypothesis) == len(references))\n",
    "    clip_count, count, total_len_hyp, total_len_ref = bleu_count(hypothesis, references, max_n=max_n)\n",
    "    brevity_penalty = 1.0\n",
    "    bleu_scores = []\n",
    "    bleu = 0\n",
    "    for n in range(max_n):\n",
    "        if count[n]>0:\n",
    "            bleu_scores.append(clip_count[n]/count[n])\n",
    "        else:\n",
    "            bleu_scores.append(0)\n",
    "    if total_len_hyp < total_len_ref:\n",
    "        if total_len_hyp==0:\n",
    "            brevity_penalty = 0.0\n",
    "        else:\n",
    "            brevity_penalty = math.exp(1 - total_len_ref/total_len_hyp)\n",
    "    def my_log(x):\n",
    "        if x == 0:\n",
    "            return -9999999999.0\n",
    "        elif x < 0:\n",
    "            raise Exception(\"Value Error\")\n",
    "        return math.log(x)\n",
    "    log_bleu = 0.0\n",
    "    for n in range(max_n):\n",
    "        log_bleu += my_log(bleu_scores[n])\n",
    "    bleu = brevity_penalty*math.exp(log_bleu / float(max_n))\n",
    "    return [bleu]+bleu_scores, [brevity_penalty, total_len_hyp/total_len_ref, total_len_hyp, total_len_ref]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0, 0.3, 0.0, 0.0, 0.0], [1.0, 10.0, 10, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_bleu([hypothesis], [reference])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def is_content_identical(msg):\n",
    "    # 按换行符分隔每行内容\n",
    "    lines = msg.split('\\n')\n",
    "    if len(lines) == 1:\n",
    "        return False\n",
    "    # 去除每行前后的空白字符并比较\n",
    "    return all(line.strip() == lines[0].strip() for line in lines)\n",
    "\n",
    "def check_msg_duplicates(json_data):\n",
    "\n",
    "    # 遍历 JSON 数据，将每个项的 msg 字段作为键存储在 msg_map 中\n",
    "    for item in json_data:\n",
    "        if is_content_identical(item['msg']):\n",
    "            item['msg'] = item['msg'].split('\\n')[0]\n",
    "            # result.append(item)\n",
    "    \n",
    "    return json_data\n",
    "\n",
    "with open('../data/angular_filtered/subsets/dev_test.json', 'r', encoding='UTF-8') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "with open('../data/angular_filtered/subsets/dev_test.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(check_msg_duplicates(json_data), f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = '../data/angular_filtered/subsets/generation/chunksize/dev_test_ref.txt'\n",
    "with open(output_file, 'w', encoding='UTF-8') as f:\n",
    "    for item in json_data:\n",
    "        f.write(item['msg'].replace('\\n', '\\\\n').replace('\\r', '\\\\r') + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/angular_filtered/subsets/generation/embedding/dev_test_gpt35_rag_mxbai.json', 'r', encoding='UTF-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open('../data/angular_filtered/subsets/generation/embedding/dev_test_gpt35_rag_mxbai.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(check_msg_duplicates(data), f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-MNEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chore(package.json):',\n",
       " 'update',\n",
       " 'release',\n",
       " 'script',\n",
       " 'and',\n",
       " 'add',\n",
       " 'prepublish',\n",
       " 'script']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"chore(package.json): update release script and add prepublish script\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chore',\n",
       " '(',\n",
       " 'package.json',\n",
       " ')',\n",
       " ':',\n",
       " 'update',\n",
       " 'release',\n",
       " 'script',\n",
       " 'and',\n",
       " 'add',\n",
       " 'prepublish',\n",
       " 'script']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"chore(package.json): update release script and add prepublish script\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "refs = [\"build(docs): update build command fo full static\"]\n",
    "for i in range(len(refs)):\n",
    "    for ele in refs[i]:\n",
    "        if ele in punc:\n",
    "            refs[i] = refs[i].replace(ele, \" \")\n",
    "            refs[i] = re.sub(r'\\s+', ' ', refs[i]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['build docs update build command fo full static']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
