{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T22:02:27.034961Z",
     "start_time": "2024-08-19T22:02:27.014891Z"
    }
   },
   "source": [
    "# Source: https://github.com/mjpost/sacrebleu/blob/master/sacrebleu/tokenizers/tokenizer_13a.py\n",
    "# Copyright 2020 SacreBLEU Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import re\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "class BaseTokenizer:\n",
    "    \"\"\"A base dummy tokenizer to derive from.\"\"\"\n",
    "\n",
    "    def signature(self):\n",
    "        \"\"\"\n",
    "        Returns a signature for the tokenizer.\n",
    "        :return: signature string\n",
    "        \"\"\"\n",
    "        return \"none\"\n",
    "\n",
    "    def __call__(self, line):\n",
    "        \"\"\"\n",
    "        Tokenizes an input line with the tokenizer.\n",
    "        :param line: a segment to tokenize\n",
    "        :return: the tokenized line\n",
    "        \"\"\"\n",
    "        return line\n",
    "\n",
    "\n",
    "class TokenizerRegexp(BaseTokenizer):\n",
    "    def signature(self):\n",
    "        return \"re\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._re = [\n",
    "            # language-dependent part (assuming Western languages)\n",
    "            (re.compile(r\"([\\{-\\~\\[-\\` -\\&\\(-\\+\\:-\\@\\/])\"), r\" \\1 \"),\n",
    "            # tokenize period and comma unless preceded by a digit\n",
    "            (re.compile(r\"([^0-9])([\\.,])\"), r\"\\1 \\2 \"),\n",
    "            # tokenize period and comma unless followed by a digit\n",
    "            (re.compile(r\"([\\.,])([^0-9])\"), r\" \\1 \\2\"),\n",
    "            # tokenize dash when preceded by a digit\n",
    "            (re.compile(r\"([0-9])(-)\"), r\"\\1 \\2 \"),\n",
    "            # one space only between words\n",
    "            # NOTE: Doing this in Python (below) is faster\n",
    "            # (re.compile(r'\\s+'), r' '),\n",
    "        ]\n",
    "\n",
    "    @lru_cache(maxsize=2**16)\n",
    "    def __call__(self, line):\n",
    "        \"\"\"Common post-processing tokenizer for `13a` and `zh` tokenizers.\n",
    "        :param line: a segment to tokenize\n",
    "        :return: the tokenized line\n",
    "        \"\"\"\n",
    "        for (_re, repl) in self._re:\n",
    "            line = _re.sub(repl, line)\n",
    "\n",
    "        # no leading or trailing spaces, single space within words\n",
    "        # return ' '.join(line.split())\n",
    "        # This line is changed with regards to the original tokenizer (seen above) to return individual words\n",
    "        return line.split()\n",
    "\n",
    "\n",
    "class Tokenizer13a(BaseTokenizer):\n",
    "    def signature(self):\n",
    "        return \"13a\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._post_tokenizer = TokenizerRegexp()\n",
    "\n",
    "    @lru_cache(maxsize=2**16)\n",
    "    def __call__(self, line):\n",
    "        \"\"\"Tokenizes an input line using a relatively minimal tokenization\n",
    "        that is however equivalent to mteval-v13a, used by WMT.\n",
    "\n",
    "        :param line: a segment to tokenize\n",
    "        :return: the tokenized line\n",
    "        \"\"\"\n",
    "\n",
    "        # language-independent part:\n",
    "        line = line.replace(\"<skipped>\", \"\")\n",
    "        line = line.replace(\"-\\n\", \"\")\n",
    "        line = line.replace(\"\\n\", \" \")\n",
    "\n",
    "        if \"&\" in line:\n",
    "            line = line.replace(\"&quot;\", '\"')\n",
    "            line = line.replace(\"&amp;\", \"&\")\n",
    "            line = line.replace(\"&lt;\", \"<\")\n",
    "            line = line.replace(\"&gt;\", \">\")\n",
    "\n",
    "        return self._post_tokenizer(f\" {line} \")\n",
    "\n",
    "tokenizer=Tokenizer13a()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T22:02:27.050488Z",
     "start_time": "2024-08-19T22:02:27.036967Z"
    }
   },
   "cell_type": "code",
   "source": "print(1)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.311244979919678\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('../data/vdo_filtered/generation/test_gpt35_rag.json') as f:\n",
    "    rag_data = json.load(f)\n",
    "\n",
    "rag_length_list = []\n",
    "for item in rag_data:\n",
    "    rag_length_list.append(len(tokenizer(item['chatgpt_rag'])))\n",
    "print(sum(rag_length_list) / len(rag_length_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.194987468671679\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('../data/angular_filtered/subsets/generation/test_nngen.json') as f:\n",
    "    nngen_data = json.load(f)\n",
    "\n",
    "nngen_length_list = []\n",
    "for item in nngen_data:\n",
    "    nngen_length_list.append(len(tokenizer(item['nngen'])))\n",
    "print(sum(nngen_length_list) / len(nngen_length_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average length of Zeroshot LLM in angular format dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.221052631578948\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('../data/angular_filtered/subsets/generation/test_gpt35_zeroshot.json') as f:\n",
    "    llm_data = json.load(f)\n",
    "\n",
    "llm_length_list = []\n",
    "for item in llm_data:\n",
    "    llm_length_list.append(len(tokenizer(item['chatgpt_zeroshot'])))\n",
    "print(sum(llm_length_list) / len(llm_length_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average length of RACE in angular format dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.35187969924812\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('../data/angular_filtered/subsets/generation/test_race_v1.json') as f:\n",
    "    race_data = json.load(f)\n",
    "\n",
    "race_length_list = []\n",
    "for item in race_data:\n",
    "    race_length_list.append(len(tokenizer(item['race'])))\n",
    "print(sum(race_length_list) / len(race_length_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average length of RAG in angular format dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.664661654135339\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('../data/angular_filtered/subsets/generation/test_gpt35_rag.json') as f:\n",
    "    rag_data = json.load(f)\n",
    "\n",
    "rag_length_list = []\n",
    "for item in rag_data:\n",
    "    rag_length_list.append(len(tokenizer(item['chatgpt_rag'])))\n",
    "print(sum(rag_length_list) / len(rag_length_list))"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T22:04:03.244256Z",
     "start_time": "2024-08-19T22:04:03.195699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "with open('../data/angular_filtered/subsets/generation/test_gpt35_model_classified_rag_1000chunk.json', 'r', encoding='UTF-8') as f:\n",
    "    ref_data = json.load(f)\n",
    "\n",
    "ref_length_list = []\n",
    "for item in ref_data:\n",
    "    ref_length_list.append(len(tokenizer(item['chatgpt_rag'])))\n",
    "print(round(sum(ref_length_list) / len(ref_length_list),2))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.28\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(1)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average length of reference message in angular format dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.739348370927319\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('../data/angular_filtered/subsets/generation/test_gpt35_model_classified_rag.json') as f:\n",
    "    ref_data = json.load(f)\n",
    "\n",
    "ref_length_list = []\n",
    "for item in ref_data:\n",
    "    ref_length_list.append(len(tokenizer(item['msg'])))\n",
    "print(sum(ref_length_list) / len(ref_length_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.384538152610442\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('../data/vdo_filtered/generation/test_gpt35_model_classified_rag.json') as f:\n",
    "    ref_data = json.load(f)\n",
    "\n",
    "ref_length_list = []\n",
    "for item in ref_data:\n",
    "    ref_length_list.append(len(tokenizer(item['msg'])))\n",
    "print(sum(ref_length_list) / len(ref_length_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.045682730923694\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('../data/vdo_filtered/generation/test_gpt35_model_classified_rag.json') as f:\n",
    "    ref_data = json.load(f)\n",
    "\n",
    "ref_length_list = []\n",
    "for item in ref_data:\n",
    "    ref_length_list.append(len(tokenizer(item['chatgpt_rag'])))\n",
    "print(sum(ref_length_list) / len(ref_length_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.25062656641604\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('../data/angular_filtered/subsets/generation/test_gpt35_model_classified_rag.json') as f:\n",
    "    ref_data = json.load(f)\n",
    "\n",
    "ref_length_list = []\n",
    "for item in ref_data:\n",
    "    ref_length_list.append(len(tokenizer(item['chatgpt_rag'])))\n",
    "print(sum(ref_length_list) / len(ref_length_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.915288220551378\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('../data/angular_filtered/subsets/generation/rag/test_rag_prompt.json') as f:\n",
    "    llm_data = json.load(f)\n",
    "\n",
    "llm_length_list = []\n",
    "for item in llm_data:\n",
    "    llm_length_list.append(len(tokenizer(item['sim_msg'])))\n",
    "print(sum(llm_length_list) / len(llm_length_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.791979949874687\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('../data/angular_filtered/subsets/generation/rag/test_model_classified_rag_prompt.json') as f:\n",
    "    llm_data = json.load(f)\n",
    "\n",
    "llm_length_list = []\n",
    "for item in llm_data:\n",
    "    llm_length_list.append(len(tokenizer(item['sim_msg'])))\n",
    "print(sum(llm_length_list) / len(llm_length_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import meteor_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38522866d8c9440c8c1684c92e415ac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.7714985257158095, 'precisions': [0.9375, 0.9333333333333333, 0.9285714285714286, 0.9230769230769231], 'brevity_penalty': 0.8290291181804004, 'length_ratio': 0.8421052631578947, 'translation_length': 16, 'reference_length': 19}\n"
     ]
    }
   ],
   "source": [
    "predictions = [\"add select_order_by_with_table_star_table_name\"]\n",
    "references = ['change name select_order_by_with_table_star_table_name for parser']\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.14285714285714285, 'rouge2': 0.0, 'rougeL': 0.14285714285714285, 'rougeLsum': 0.14285714285714285}\n"
     ]
    }
   ],
   "source": [
    "predictions = [\"docs: add note about firebase\"]\n",
    "references = ['docs(auth): mention of package requirement for server implementation']\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0, 'precisions': [0.6363636363636364, 0.3, 0.0, 0.0], 'brevity_penalty': 0.9131007162822622, 'length_ratio': 0.9166666666666666, 'translation_length': 11, 'reference_length': 12}\n"
     ]
    }
   ],
   "source": [
    "predictions = [\"docs: Update warning message for programmatic server implementation\"]\n",
    "references = ['docs(auth): mention of package requirement for server implementation']\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.47058823529411764, 'rouge2': 0.13333333333333333, 'rougeL': 0.47058823529411764, 'rougeLsum': 0.47058823529411764}\n"
     ]
    }
   ],
   "source": [
    "predictions = [\"docs: Update warning message for programmatic server implementation\"]\n",
    "references = ['docs(auth): mention of package requirement for server implementation']\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36787944117144233"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(1-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 1.0,\n",
       " 'precisions': [1.0, 1.0, 1.0, 1.0],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.1666666666666667,\n",
       " 'translation_length': 7,\n",
       " 'reference_length': 6}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_enums(hypothesis, reference, preprocess=str.lower):\n",
    "    \"\"\"\n",
    "    Takes in string inputs for hypothesis and reference and returns\n",
    "    enumerated word lists for each of them\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :type hypothesis: str\n",
    "    :param reference: reference string\n",
    "    :type reference: str\n",
    "    :preprocess: preprocessing method (default str.lower)\n",
    "    :type preprocess: method\n",
    "    :return: enumerated words list\n",
    "    :rtype: list of 2D tuples, list of 2D tuples\n",
    "    \"\"\"\n",
    "    hypothesis_list = list(enumerate(preprocess(hypothesis).split()))\n",
    "    reference_list = list(enumerate(preprocess(reference).split()))\n",
    "    return hypothesis_list, reference_list\n",
    "\n",
    "\n",
    "def exact_match(hypothesis, reference):\n",
    "    \"\"\"\n",
    "    matches exact words in hypothesis and reference\n",
    "    and returns a word mapping based on the enumerated\n",
    "    word id between hypothesis and reference\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :type hypothesis: str\n",
    "    :param reference: reference string\n",
    "    :type reference: str\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    hypothesis_list, reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _match_enums(hypothesis_list, reference_list)\n",
    "\n",
    "\n",
    "def _match_enums(enum_hypothesis_list, enum_reference_list):\n",
    "    \"\"\"\n",
    "    matches exact words in hypothesis and reference and returns\n",
    "    a word mapping between enum_hypothesis_list and enum_reference_list\n",
    "    based on the enumerated word id.\n",
    "\n",
    "    :param enum_hypothesis_list: enumerated hypothesis list\n",
    "    :type enum_hypothesis_list: list of tuples\n",
    "    :param enum_reference_list: enumerated reference list\n",
    "    :type enum_reference_list: list of 2D tuples\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    word_match = []\n",
    "    for i in range(len(enum_hypothesis_list))[::-1]:\n",
    "        for j in range(len(enum_reference_list))[::-1]:\n",
    "            if enum_hypothesis_list[i][1] == enum_reference_list[j][1]:\n",
    "                word_match.append(\n",
    "                    (enum_hypothesis_list[i][0], enum_reference_list[j][0])\n",
    "                )\n",
    "                (enum_hypothesis_list.pop(i)[1], enum_reference_list.pop(j)[1])\n",
    "                break\n",
    "    return word_match, enum_hypothesis_list, enum_reference_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis=\"docs: Update warning message for programmatic server implementation\"\n",
    "reference = \"docs(auth): mention of package requirement for server implementation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(7, 7), (6, 6), (4, 5)],\n",
       " [(0, 'docs:'),\n",
       "  (1, 'update'),\n",
       "  (2, 'warning'),\n",
       "  (3, 'message'),\n",
       "  (5, 'programmatic')],\n",
       " [(0, 'docs(auth):'),\n",
       "  (1, 'mention'),\n",
       "  (2, 'of'),\n",
       "  (3, 'package'),\n",
       "  (4, 'requirement')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "\n",
    "#refs_new = []\n",
    "for ele in reference:\n",
    "    if ele in punc:\n",
    "        reference = reference.replace(ele, \" \")\n",
    "        reference = re.sub(r'\\s+', ' ', reference).strip()\n",
    "\n",
    "for ele in hypothesis:\n",
    "    if ele in punc:\n",
    "        hypothesis = hypothesis.replace(ele, \" \")\n",
    "        hypothesis = re.sub(r'\\s+', ' ', hypothesis).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_list, reference_list = _generate_enums(hypothesis, reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'docs'),\n",
       " (1, 'update'),\n",
       " (2, 'warning'),\n",
       " (3, 'message'),\n",
       " (4, 'for'),\n",
       " (5, 'programmatic'),\n",
       " (6, 'server'),\n",
       " (7, 'implementation')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'docs'),\n",
       " (1, 'auth'),\n",
       " (2, 'mention'),\n",
       " (3, 'of'),\n",
       " (4, 'package'),\n",
       " (5, 'requirement'),\n",
       " (6, 'for'),\n",
       " (7, 'server'),\n",
       " (8, 'implementation')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(7, 8), (6, 7), (4, 6), (0, 0)],\n",
       " [(1, 'update'), (2, 'warning'), (3, 'message'), (5, 'programmatic')],\n",
       " [(1, 'auth'), (2, 'mention'), (3, 'of'), (4, 'package'), (5, 'requirement')])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_match(hypothesis, reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6796116504854369"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1 = ((7/12)* (7/10)) / (0.85*(7/12) + 0.15*(7/10))\n",
    "F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6896551724137931"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1 = ((7/11)* (7/10)) / (0.85*(7/11) + 0.15*(7/10))\n",
    "F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7106598984771574"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1 = ((7/9)* (7/10)) / (0.85*(7/9) + 0.15*(7/10))\n",
    "F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6363636363636365"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1 = ((7/10)* (7/12)) / (0.5*(7/12) + 0.5*(7/10))\n",
    "F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B-Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs ( auth ) : mention of package requirement for server implementation\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def splitPuncts(line):\n",
    "    # This regex matches words and punctuation, treating punctuation as separate tokens\n",
    "    return ' '.join(re.findall(r'\\w+|[^\\w\\s]', line))\n",
    "\n",
    "# Example usage:\n",
    "text = \"docs(auth): mention of package requirement for server implementation\"\n",
    "print(splitPuncts(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['docs',\n",
       " '(',\n",
       " 'auth',\n",
       " ')',\n",
       " ':',\n",
       " 'mention',\n",
       " 'of',\n",
       " 'package',\n",
       " 'requirement',\n",
       " 'for',\n",
       " 'server',\n",
       " 'implementation']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitPuncts(text).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B-Moses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "\n",
    "\n",
    "def _get_ngrams(segment, max_order):\n",
    "  \"\"\"Extracts all n-grams upto a given maximum order from an input segment.\n",
    "\n",
    "  Args:\n",
    "    segment: text segment from which n-grams will be extracted.\n",
    "    max_order: maximum length in tokens of the n-grams returned by this\n",
    "        methods.\n",
    "\n",
    "  Returns:\n",
    "    The Counter containing all n-grams upto max_order in segment\n",
    "    with a count of how many times each n-gram occurred.\n",
    "  \"\"\"\n",
    "  ngram_counts = collections.Counter()\n",
    "  for order in range(1, max_order + 1):\n",
    "    for i in range(0, len(segment) - order + 1):\n",
    "      ngram = tuple(segment[i:i+order])\n",
    "      ngram_counts[ngram] += 1\n",
    "  return ngram_counts\n",
    "\n",
    "\n",
    "def compute_bleu(reference_corpus, translation_corpus, max_order=4,\n",
    "                 smooth=False):\n",
    "  \"\"\"Computes BLEU score of translated segments against one or more references.\n",
    "\n",
    "  Args:\n",
    "    reference_corpus: list of lists of references for each translation. Each\n",
    "        reference should be tokenized into a list of tokens.\n",
    "    translation_corpus: list of translations to score. Each translation\n",
    "        should be tokenized into a list of tokens.\n",
    "    max_order: Maximum n-gram order to use when computing BLEU score.\n",
    "    smooth: Whether or not to apply Lin et al. 2004 smoothing.\n",
    "\n",
    "  Returns:\n",
    "    3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n",
    "    precisions and brevity penalty.\n",
    "  \"\"\"\n",
    "  matches_by_order = [0] * max_order\n",
    "  possible_matches_by_order = [0] * max_order\n",
    "  reference_length = 0\n",
    "  translation_length = 0\n",
    "  for (references, translation) in zip(reference_corpus,\n",
    "                                       translation_corpus):\n",
    "    reference_length += min(len(r) for r in references)\n",
    "    translation_length += len(translation)\n",
    "\n",
    "    merged_ref_ngram_counts = collections.Counter()\n",
    "    for reference in references:\n",
    "      merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n",
    "    translation_ngram_counts = _get_ngrams(translation, max_order)\n",
    "    overlap = translation_ngram_counts & merged_ref_ngram_counts\n",
    "    for ngram in overlap:\n",
    "      matches_by_order[len(ngram)-1] += overlap[ngram]\n",
    "    for order in range(1, max_order+1):\n",
    "      possible_matches = len(translation) - order + 1\n",
    "      if possible_matches > 0:\n",
    "        possible_matches_by_order[order-1] += possible_matches\n",
    "\n",
    "  precisions = [0] * max_order\n",
    "  for i in range(0, max_order):\n",
    "    if smooth:\n",
    "      precisions[i] = ((matches_by_order[i] + 1.) /\n",
    "                       (possible_matches_by_order[i] + 1.))\n",
    "    else:\n",
    "      if possible_matches_by_order[i] > 0:\n",
    "        precisions[i] = (float(matches_by_order[i]) /\n",
    "                         possible_matches_by_order[i])\n",
    "      else:\n",
    "        precisions[i] = 0.0\n",
    "\n",
    "  if min(precisions) > 0:\n",
    "    p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n",
    "    geo_mean = math.exp(p_log_sum)\n",
    "  else:\n",
    "    geo_mean = 0\n",
    "\n",
    "  ratio = float(translation_length) / reference_length\n",
    "\n",
    "  if ratio > 1.0:\n",
    "    bp = 1.\n",
    "  else:\n",
    "    bp = math.exp(1 - 1. / ratio)\n",
    "\n",
    "  bleu = geo_mean * bp\n",
    "\n",
    "  return (bleu, precisions, bp, ratio, translation_length, reference_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis=\"Upgrade to plexus - utils 3 . 0 . 24\"\n",
    "reference = \"Upgrade to Plexus Utils 3 . 0 . 24\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, [0.56, 0.0, 0.0, 0.0], 1.0, 2.7777777777777777, 25, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_bleu(splitPuncts(reference).split(), splitPuncts(hypothesis).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "\n",
    "def bleu_count(hypothesis, references, max_n=4):\n",
    "    ret_len_hyp = 0\n",
    "    ret_len_ref = 0\n",
    "    ret_clip_count = [0]*max_n\n",
    "    ret_count = [0]*max_n\n",
    "    for m in range(len(hypothesis)):\n",
    "        hyp, ref = hypothesis[m], references[m]\n",
    "        x = hyp.split()\n",
    "        y = [r.split() for r in ref]\n",
    "        x_len = len(x)\n",
    "        y_len = [len(s) for s in y]\n",
    "        n_ref = len(ref)\n",
    "\n",
    "        closest_diff = 9999\n",
    "        closest_length = 9999\n",
    "        ref_ngram = dict()\n",
    "\n",
    "        for i in range(n_ref):\n",
    "            diff = abs(y_len[i]-x_len)\n",
    "            if diff < closest_diff:\n",
    "                closest_diff = diff\n",
    "                closest_length = y_len[i]\n",
    "            elif diff==closest_diff and y_len[i] < closest_length:\n",
    "                closest_length = y_len[i]\n",
    "\n",
    "            for n in range(max_n):\n",
    "                sent_ngram = dict()\n",
    "                for st in range(0, y_len[i]-n):\n",
    "                    ngram = \"%d\"%(n+1)\n",
    "                    for k in range(n+1):\n",
    "                        j = st+k\n",
    "                        ngram += \" %s\"%(y[i][j])\n",
    "                    if ngram not in sent_ngram:\n",
    "                        sent_ngram[ngram]=0\n",
    "                    sent_ngram[ngram]+=1\n",
    "                for ngram in sent_ngram.keys():\n",
    "                    if ngram not in ref_ngram or ref_ngram[ngram]<sent_ngram[ngram]:\n",
    "                        ref_ngram[ngram] = sent_ngram[ngram]\n",
    "\n",
    "        ret_len_hyp += x_len\n",
    "        ret_len_ref += closest_length\n",
    "\n",
    "        for n in range(max_n):\n",
    "            hyp_ngram = dict()\n",
    "            for st in range(0, x_len-n):\n",
    "                ngram = \"%d\"%(n+1)\n",
    "                for k in range(n+1):\n",
    "                    j = st+k\n",
    "                    ngram += \" %s\"%(x[j])\n",
    "                if ngram not in hyp_ngram:\n",
    "                    hyp_ngram[ngram]=0\n",
    "                hyp_ngram[ngram]+=1\n",
    "            for ngram in hyp_ngram.keys():\n",
    "                if ngram in ref_ngram:\n",
    "                    ret_clip_count[n] += min(ref_ngram[ngram], hyp_ngram[ngram])\n",
    "                ret_count[n] += hyp_ngram[ngram]\n",
    "\n",
    "    return ret_clip_count, ret_count, ret_len_hyp, ret_len_ref\n",
    "\n",
    "def corpus_bleu(hypothesis, references, max_n=4):\n",
    "    assert(len(hypothesis) == len(references))\n",
    "    clip_count, count, total_len_hyp, total_len_ref = bleu_count(hypothesis, references, max_n=max_n)\n",
    "    brevity_penalty = 1.0\n",
    "    bleu_scores = []\n",
    "    bleu = 0\n",
    "    for n in range(max_n):\n",
    "        if count[n]>0:\n",
    "            bleu_scores.append(clip_count[n]/count[n])\n",
    "        else:\n",
    "            bleu_scores.append(0)\n",
    "    if total_len_hyp < total_len_ref:\n",
    "        if total_len_hyp==0:\n",
    "            brevity_penalty = 0.0\n",
    "        else:\n",
    "            brevity_penalty = math.exp(1 - total_len_ref/total_len_hyp)\n",
    "    def my_log(x):\n",
    "        if x == 0:\n",
    "            return -9999999999.0\n",
    "        elif x < 0:\n",
    "            raise Exception(\"Value Error\")\n",
    "        return math.log(x)\n",
    "    log_bleu = 0.0\n",
    "    for n in range(max_n):\n",
    "        log_bleu += my_log(bleu_scores[n])\n",
    "    bleu = brevity_penalty*math.exp(log_bleu / float(max_n))\n",
    "    return [bleu]+bleu_scores, [brevity_penalty, total_len_hyp/total_len_ref, total_len_hyp, total_len_ref]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0, 0.3, 0.0, 0.0, 0.0], [1.0, 10.0, 10, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_bleu([hypothesis], [reference])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def is_content_identical(msg):\n",
    "    # 按换行符分隔每行内容\n",
    "    lines = msg.split('\\n')\n",
    "    if len(lines) == 1:\n",
    "        return False\n",
    "    # 去除每行前后的空白字符并比较\n",
    "    return all(line.strip() == lines[0].strip() for line in lines)\n",
    "\n",
    "def check_msg_duplicates(json_data):\n",
    "\n",
    "    # 遍历 JSON 数据，将每个项的 msg 字段作为键存储在 msg_map 中\n",
    "    for item in json_data:\n",
    "        if is_content_identical(item['msg']):\n",
    "            item['msg'] = item['msg'].split('\\n')[0]\n",
    "            # result.append(item)\n",
    "    \n",
    "    return json_data\n",
    "\n",
    "with open('../data/angular_filtered/subsets/dev_test.json', 'r', encoding='UTF-8') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "with open('../data/angular_filtered/subsets/dev_test.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(check_msg_duplicates(json_data), f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = '../data/angular_filtered/subsets/generation/chunksize/dev_test_ref.txt'\n",
    "with open(output_file, 'w', encoding='UTF-8') as f:\n",
    "    for item in json_data:\n",
    "        f.write(item['msg'].replace('\\n', '\\\\n').replace('\\r', '\\\\r') + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/angular_filtered/subsets/generation/embedding/dev_test_gpt35_rag_mxbai.json', 'r', encoding='UTF-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open('../data/angular_filtered/subsets/generation/embedding/dev_test_gpt35_rag_mxbai.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(check_msg_duplicates(data), f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-MNEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chore(package.json):',\n",
       " 'update',\n",
       " 'release',\n",
       " 'script',\n",
       " 'and',\n",
       " 'add',\n",
       " 'prepublish',\n",
       " 'script']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"chore(package.json): update release script and add prepublish script\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chore',\n",
       " '(',\n",
       " 'package.json',\n",
       " ')',\n",
       " ':',\n",
       " 'update',\n",
       " 'release',\n",
       " 'script',\n",
       " 'and',\n",
       " 'add',\n",
       " 'prepublish',\n",
       " 'script']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"chore(package.json): update release script and add prepublish script\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "refs = [\"build(docs): update build command fo full static\"]\n",
    "for i in range(len(refs)):\n",
    "    for ele in refs[i]:\n",
    "        if ele in punc:\n",
    "            refs[i] = refs[i].replace(ele, \" \")\n",
    "            refs[i] = re.sub(r'\\s+', ' ', refs[i]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['build(docs):', 'update', 'build', 'command', 'fo', 'full', 'static']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"build(docs): update build command fo full static\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['build',\n",
       " '(',\n",
       " 'docs',\n",
       " ')',\n",
       " ':',\n",
       " 'update',\n",
       " 'build',\n",
       " 'command',\n",
       " 'fo',\n",
       " 'full',\n",
       " 'static']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"build(docs): update build command fo full static\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['build', 'docs', 'update', 'build', 'command', 'fo', 'full', 'static']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(refs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rouge-L"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T08:57:57.704423Z",
     "start_time": "2024-08-06T08:57:57.671020Z"
    }
   },
   "source": [
    "import json\n",
    "\n",
    "with open('../data/vdo_filtered/test_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 初始化一个字典来统计每种 type 的出现次数\n",
    "type_count = {}\n",
    "\n",
    "# 遍历 JSON 数据中的每个 item\n",
    "for item in data:\n",
    "    # 提取 type 字段的值\n",
    "    item_type = item.get('type')\n",
    "    \n",
    "    # 统计每种 type 的出现次数\n",
    "    if item_type in type_count:\n",
    "        type_count[item_type] += 1\n",
    "    else:\n",
    "        type_count[item_type] = 1\n",
    "\n",
    "# 输出统计结果\n",
    "print(type_count)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'add': 261, 'fix': 261, 'remove': 261, 'update': 261, 'use': 102, 'move': 30, 'prepare': 17, 'improve': 213, 'ignore': 38, 'handle': 83, 'rename': 87, 'allow': 111, 'set': 125, 'revert': 45, 'replace': 97}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T08:55:32.973489Z",
     "start_time": "2024-08-06T08:55:32.961973Z"
    }
   },
   "cell_type": "code",
   "source": "sum(type_count.values())",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30595"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T08:55:44.425881Z",
     "start_time": "2024-08-06T08:55:44.149830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "def count_types_from_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    type_count = {}\n",
    "    \n",
    "    for item in data:\n",
    "        item_type = item.get('type')\n",
    "        if item_type in type_count:\n",
    "            type_count[item_type] += 1\n",
    "        else:\n",
    "            type_count[item_type] = 1\n",
    "            \n",
    "    return type_count\n",
    "\n",
    "def merge_counts(counts1, counts2):\n",
    "    merged_count = counts1.copy()\n",
    "    \n",
    "    for key, value in counts2.items():\n",
    "        if key in merged_count:\n",
    "            merged_count[key] += value\n",
    "        else:\n",
    "            merged_count[key] = value\n",
    "    \n",
    "    return merged_count\n",
    "\n",
    "# 统计第一个文件的 type\n",
    "counts1 = count_types_from_file('../data/vdo_filtered/db_data.json')\n",
    "# 统计第二个文件的 type\n",
    "counts2 = count_types_from_file('../data/vdo_filtered/test_data.json')\n",
    "\n",
    "# 合并统计结果\n",
    "merged_counts = merge_counts(counts1, counts2)\n",
    "\n",
    "# 输出合并后的统计结果\n",
    "print(merged_counts)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'add': 4261, 'fix': 4261, 'remove': 4261, 'update': 4261, 'use': 1672, 'move': 493, 'prepare': 291, 'improve': 3476, 'ignore': 625, 'handle': 1367, 'rename': 1430, 'allow': 1815, 'set': 2039, 'revert': 745, 'replace': 1590}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T08:55:51.801153Z",
     "start_time": "2024-08-06T08:55:51.784168Z"
    }
   },
   "cell_type": "code",
   "source": "sum(merged_counts.values())",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32587"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def check_angular_convention(msg, has_space=True):\n",
    "    types = '((build)|(ci)|(docs)|(feat)|(fix)|(perf)|(refactor)|(style)|(test)|(chore))'\n",
    "    # types = '((perf))'\n",
    "    if has_space:\n",
    "        pattern = f'{types}\\\\s(\\\\((\\\\s|\\\\S)+\\\\)\\\\s)?:\\\\s(\\\\s|\\\\S)+'\n",
    "    else:\n",
    "        pattern = f'{types}(\\\\((\\\\s|\\\\S)+\\\\))?:\\\\s\\\\S+(\\\\s|\\\\S)+'\n",
    "    # if not strict:\n",
    "    #     pattern = '^((build)|(ci)|(docs)|(feat)|(fix)|(perf)|(refactor)|(style)|(test)|(chore))'\n",
    "    return re.match(pattern, msg) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1966\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('../data/angular_filtered/subsets/generation/test_gpt35_golden_classified_rag.json', 'r', encoding='UTF-8') as f:\n",
    "    rag_data = json.load(f)\n",
    "\n",
    "angular_rag_count = 0\n",
    "for item in rag_data:\n",
    "    if check_angular_convention(item['chatgpt_rag']) or check_angular_convention(item['chatgpt_rag'], False):\n",
    "        angular_rag_count += 1\n",
    "print(angular_rag_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9854636591478697\n"
     ]
    }
   ],
   "source": [
    "print(angular_rag_count / len(rag_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "804\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('../data/angular_filtered/subsets/generation/test_race_v1.json', 'r', encoding='UTF-8') as f:\n",
    "    race_data = json.load(f)\n",
    "\n",
    "angular_race_count = 0\n",
    "for item in race_data:\n",
    "    if check_angular_convention(item['race']) or check_angular_convention(item['race'], False):\n",
    "        angular_race_count += 1\n",
    "print(angular_race_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4030075187969925\n"
     ]
    }
   ],
   "source": [
    "print(angular_race_count / len(race_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7796184738955824\n"
     ]
    }
   ],
   "source": [
    "def to_lemma(msg, nlp):\n",
    "    props = {'annotators': 'lemma', 'outputFormat': 'json', 'timeout': 1000}\n",
    "    msg_list = msg.split()\n",
    "    annot_doc = nlp.annotate(msg, properties=props)\n",
    "    parsed_dict = json.loads(annot_doc)\n",
    "    lemma_list = [v for d in parsed_dict['sentences'][0]['tokens'] for k, v in d.items() if k == 'lemma']\n",
    "    msg_list[0] = lemma_list[0]\n",
    "    msg = ' '.join(msg_list)\n",
    "    return msg\n",
    "\n",
    "\n",
    "import json\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('http://localhost', port=9000)\n",
    "with open('../data/vdo_filtered/generation/test_race.json', 'r', encoding='UTF-8') as f:\n",
    "    race_data = json.load(f)\n",
    "\n",
    "verb_groups = ['add', 'fix', 'remove', 'update', 'use', 'move', 'prepare', 'improve', 'ignore', 'handle', 'rename',\n",
    "               'allow', 'set', 'revert', 'replace']\n",
    "\n",
    "vdo_race_count = 0\n",
    "for item in race_data:\n",
    "    msg = to_lemma(item['race'], nlp)\n",
    "    if msg.split()[0] in verb_groups:\n",
    "        vdo_race_count += 1\n",
    "print(vdo_race_count / len(race_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4151606425702811\n"
     ]
    }
   ],
   "source": [
    "with open('../data/vdo_filtered/generation/test_gpt35_zeroshot.json', 'r', encoding='UTF-8') as f:\n",
    "    llm_data = json.load(f)\n",
    "\n",
    "verb_groups = ['add', 'fix', 'remove', 'update', 'use', 'move', 'prepare', 'improve', 'ignore', 'handle', 'rename',\n",
    "               'allow', 'set', 'revert', 'replace']\n",
    "\n",
    "vdo_llm_count = 0\n",
    "for item in llm_data:\n",
    "    msg = to_lemma(item['chatgpt_zeroshot'], nlp)\n",
    "    if msg.split()[0] in verb_groups:\n",
    "        vdo_llm_count += 1\n",
    "print(vdo_llm_count / len(llm_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7670682730923695\n"
     ]
    }
   ],
   "source": [
    "with open('../data/vdo_filtered/generation/test_gpt35_model_classified_rag.json', 'r', encoding='UTF-8') as f:\n",
    "    llm_data = json.load(f)\n",
    "\n",
    "verb_groups = ['add', 'fix', 'remove', 'update', 'use', 'move', 'prepare', 'improve', 'ignore', 'handle', 'rename',\n",
    "               'allow', 'set', 'revert', 'replace']\n",
    "\n",
    "vdo_llm_count = 0\n",
    "for item in llm_data:\n",
    "    msg = to_lemma(item['chatgpt_rag'], nlp)\n",
    "    if msg.split()[0] in verb_groups:\n",
    "        vdo_llm_count += 1\n",
    "print(vdo_llm_count / len(llm_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
