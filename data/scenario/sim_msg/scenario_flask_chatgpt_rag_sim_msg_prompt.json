[
    {
        "org_diff": "diff --git a/.pre-commit-config.yaml b/a/.pre-commit-config.yaml @@ -2,7 +2,7 @@ ci:\n   autoupdate_schedule: monthly\n repos:\n   - repo: https://github.com/astral-sh/ruff-pre-commit\n-    rev: v0.3.5\n+    rev: v0.4.3\n     hooks:\n       - id: ruff\n       - id: ruff-format",
        "org_msg": "Update ruff-pre-commit to v0.4.3 in .pre-commit-config.yaml",
        "sim_msg": "#update to 1.1.3.dev4",
        "sim_diff": "diff --git a/QUANTAXIS/__init__.py b/QUANTAXIS/__init__.py @@ -32,7 +32,7 @@ by yutiansut\n2017/4/8\n\"\"\"\n-__version__ = '1.1.3.dev3'\n+__version__ = '1.1.3.dev4'\n__author__ = 'yutiansut'\nlogo = ' \\n \\\n```````````````````````````````````````````````````````````````````````````````````````````````````````````````````````` \\n \\\n"
    },
    {
        "org_diff": "diff --git a/.pre-commit-config.yaml b/a/.pre-commit-config.yaml @@ -2,7 +2,7 @@ ci:\n   autoupdate_schedule: monthly\n repos:\n   - repo: https://github.com/astral-sh/ruff-pre-commit\n-    rev: v0.3.5\n+    rev: v0.4.3\n     hooks:\n       - id: ruff\n       - id: ruff-format",
        "org_msg": "Update ruff-pre-commit to version v0.4.3",
        "sim_msg": "update version (v4.1.3)",
        "sim_diff": "diff --git a/app/settings.py b/app/settings.py @@ -87,4 +87,4 @@ DEVELOPER_MODE: bool = config(\"DEVELOPER_MODE\", cast=bool, default=False)\n## WARNING: only touch this if you know how\n## the migrations system works.\n## you'll regret it.\n-VERSION = \"4.1.2\"\n+VERSION = \"4.1.3\"\n"
    },
    {
        "org_diff": "diff --git a/src/flask/helpers.py b/a/src/flask/helpers.py @@ -91,7 +91,7 @@ def decorator(*args: t.Any, **kwargs: t.Any) -> t.Any:\n             gen = generator_or_function(*args, **kwargs)  # type: ignore[operator]\n             return stream_with_context(gen)\n \n-        return update_wrapper(decorator, generator_or_function)  # type: ignore[arg-type]\n+        return update_wrapper(decorator, generator_or_function)  # type: ignore[arg-type, return-value]\n \n     def generator() -> t.Iterator[t.AnyStr | None]:\n         ctx = _cv_request.get(None)",
        "org_msg": "Update type hinting for return value in decorator function.",
        "sim_msg": "deprecated decorator return fix",
        "sim_diff": "diff --git a/nemo/utils/decorators/deprecated.py b/nemo/utils/decorators/deprecated.py @@ -17,10 +17,8 @@ import nemo\nclass deprecated(object):\n- \"\"\" Decorator class used for indicating that a function is\n- deprecated and going to be removed.\n- Tracks down which functions printed the warning and\n- will print it only once per function.\n+ \"\"\" Decorator class used for indicating that a function is deprecated and going to be removed.\n+ Tracks down which functions printed the warning and will print it only once per function.\n\"\"\"\n# Static variable - list of names of functions that we already printed\n@@ -33,8 +31,7 @@ class deprecated(object):\nArgs:\nversion: Version in which the function will be removed (optional)\n- explanation: Additional explanation (optional), e.g. use method\n- ``blabla instead``.\n+ explanation: Additional explanation (optional), e.g. use method ``blabla instead``.\n\"\"\"\nself.version = version\n@@ -61,7 +58,8 @@ class deprecated(object):\n# Optionally, add version and alternative.\nif self.version is not None:\n- msg = msg + \" It is going to be removed in version {}.\".format(self.version)\n+ msg = msg + \" It is going to be removed in \"\n+ msg = msg + \"the {} version.\".format(self.version)\nif self.explanation is not None:\nmsg = msg + \" \" + self.explanation\n@@ -70,6 +68,6 @@ class deprecated(object):\nnemo.logging.warning(msg)\n# Call the function.\n- func(*args, **kwargs)\n+ return func(*args, **kwargs)\nreturn wrapper\n"
    },
    {
        "org_diff": "diff --git a/src/flask/sessions.py b/a/src/flask/sessions.py @@ -365,10 +365,10 @@ def save_session(\n             return\n \n         expires = self.get_expiration_time(app, session)\n-        val = self.get_signing_serializer(app).dumps(dict(session))  # type: ignore\n+        val = self.get_signing_serializer(app).dumps(dict(session))  # type: ignore[union-attr]\n         response.set_cookie(\n             name,\n-            val,  # type: ignore\n+            val,\n             expires=expires,\n             httponly=httponly,\n             domain=domain,",
        "org_msg": "Fix type ignore in saving session cookie value",
        "sim_msg": "#fix type bug",
        "sim_diff": "diff --git a/QUANTAXIS/QABacktest/QABacktest.py b/QUANTAXIS/QABacktest/QABacktest.py @@ -206,7 +206,7 @@ class QA_Backtest():\nelif self.backtest_type in ['index_1min', 'index_5min', 'index_15min', 'index_30min', 'index_60min']:\nself.market_data = QA_fetch_index_min_adv(\n- self.strategy_stock_list, QA_util_time_gap(self.start_real_time,self.strategy_gap,'<',self.backtest_type), QA_util_time_gap(self.end_real_time,1,'>',self.backtest_type), self.backtest_type.split('_')[1])\n+ self.strategy_stock_list, QA_util_time_gap(self.start_real_time,self.strategy_gap,'<',self.backtest_type.split('_')[1]), QA_util_time_gap(self.end_real_time,1,'>',self.backtest_type.split('_')[1]), self.backtest_type.split('_')[1])\ndef __QA_backtest_start(self, *args, **kwargs):\n\"\"\"\n"
    },
    {
        "org_diff": "diff --git a/src/flask/wrappers.py b/a/src/flask/wrappers.py @@ -71,7 +71,7 @@ def endpoint(self) -> str | None:\n         reconstruct the same URL or a modified URL.\n         \"\"\"\n         if self.url_rule is not None:\n-            return self.url_rule.endpoint\n+            return self.url_rule.endpoint  # type: ignore[no-any-return]\n \n         return None\n ",
        "org_msg": "Fix type hinting issue in endpoint method",
        "sim_msg": "Fix type hinting error",
        "sim_diff": "diff --git a/pymatgen/io/vasp/sets.py b/pymatgen/io/vasp/sets.py @@ -1110,6 +1110,11 @@ class MPNonSCFSet(MPRelaxSet):\n\"\"\"\n:return: Kpoints\n\"\"\"\n+ # override pymatgen kpoints if provided\n+ user_kpoints = self.kwargs.get(\"user_kpoints_settings\", None)\n+ if isinstance(user_kpoints, Kpoints):\n+ return user_kpoints\n+\nif self.mode.lower() == \"line\":\nkpath = HighSymmKpath(self.structure)\nfrac_k_points, k_points_labels = kpath.get_kpoints(\n@@ -1140,12 +1145,7 @@ class MPNonSCFSet(MPRelaxSet):\nelse:\nself._config_dict[\"KPOINTS\"][\"reciprocal_density\"] = \\\nself.reciprocal_density\n- kpoints = super().kpoints\n-\n- # override pymatgen kpoints if provided\n- user_kpoints = self.kwargs.get(\"user_kpoints_settings\", None)\n- if isinstance(user_kpoints, Kpoints):\n- kpoints = user_kpoints\n+ return super().kpoints\nreturn kpoints\n"
    },
    {
        "org_diff": "diff --git a/examples/javascript/pyproject.toml b/a/examples/javascript/pyproject.toml @@ -8,7 +8,7 @@ maintainers = [{name = \"Pallets\", email = \"contact@palletsprojects.com\"}]\n dependencies = [\"flask\"]\n \n [project.urls]\n-Documentation = \"https://flask.palletsprojects.com/patterns/jquery/\"\n+Documentation = \"https://flask.palletsprojects.com/patterns/javascript/\"\n \n [project.optional-dependencies]\n test = [\"pytest\"]",
        "org_msg": "Update documentation URL in pyproject.toml for Flask to point to the correct patterns for JavaScript.",
        "sim_msg": "[MNT] Additional project urls in `pyproject.toml`\nAdd links to **api reference** and **release notes** in `pyproject.toml`",
        "sim_diff": "diff --git a/pyproject.toml b/pyproject.toml @@ -124,10 +124,12 @@ dl = [\n]\n[project.urls]\n-homepage = \"https://www.sktime.org\"\n-repository = \"https://github.com/sktime/sktime\"\n-documentation = \"https://www.sktime.org\"\n-download = \"https://pypi.org/project/sktime/#files\"\n+Homepage = \"https://www.sktime.org\"\n+Repository = \"https://github.com/sktime/sktime\"\n+Documentation = \"https://www.sktime.org\"\n+Download = \"https://pypi.org/project/sktime/#files\"\n+\"API Reference\" = \"https://www.sktime.org/en/stable/api_reference.html\"\n+\"Release Notes\" = \"https://www.sktime.org/en/stable/changelog.html\"\n[project.license]\nfile = \"LICENSE\"\n"
    },
    {
        "org_diff": "diff --git a/examples/javascript/pyproject.toml b/a/examples/javascript/pyproject.toml @@ -8,7 +8,7 @@ maintainers = [{name = \"Pallets\", email = \"contact@palletsprojects.com\"}]\n dependencies = [\"flask\"]\n \n [project.urls]\n-Documentation = \"https://flask.palletsprojects.com/patterns/jquery/\"\n+Documentation = \"https://flask.palletsprojects.com/patterns/javascript/\"\n \n [project.optional-dependencies]\n test = [\"pytest\"]",
        "org_msg": "Update documentation URL in pyproject.toml",
        "sim_msg": "[MNT] Additional project urls in `pyproject.toml`\nAdd links to **api reference** and **release notes** in `pyproject.toml`",
        "sim_diff": "diff --git a/pyproject.toml b/pyproject.toml @@ -124,10 +124,12 @@ dl = [\n]\n[project.urls]\n-homepage = \"https://www.sktime.org\"\n-repository = \"https://github.com/sktime/sktime\"\n-documentation = \"https://www.sktime.org\"\n-download = \"https://pypi.org/project/sktime/#files\"\n+Homepage = \"https://www.sktime.org\"\n+Repository = \"https://github.com/sktime/sktime\"\n+Documentation = \"https://www.sktime.org\"\n+Download = \"https://pypi.org/project/sktime/#files\"\n+\"API Reference\" = \"https://www.sktime.org/en/stable/api_reference.html\"\n+\"Release Notes\" = \"https://www.sktime.org/en/stable/changelog.html\"\n[project.license]\nfile = \"LICENSE\"\n"
    },
    {
        "org_diff": "diff --git a/src/flask/sessions.py b/a/src/flask/sessions.py @@ -368,7 +368,7 @@ def save_session(\n         val = self.get_signing_serializer(app).dumps(dict(session))  # type: ignore\n         response.set_cookie(\n             name,\n-            val,  # type: ignore\n+            val,\n             expires=expires,\n             httponly=httponly,\n             domain=domain,",
        "org_msg": "Refactor save_session method to remove unnecessary type ignore comments",
        "sim_msg": "boyscouting: cleanup comments in save_generator",
        "sim_diff": "diff --git a/gaphor/storage/storage.py b/gaphor/storage/storage.py @@ -42,15 +42,11 @@ def save_generator(writer, factory): # noqa: C901\ngaphor.misc.xmlwriter.XMLWriter instance.\n\"\"\"\n- # Maintain a set of id's, one for elements, one for references.\n- # Write only to file if references is a subset of elements\n-\ndef save_reference(name, value):\n\"\"\"\nSave a value as a reference to another element in the model.\nThis applies to both UML as well as canvas items.\n\"\"\"\n- # Save a reference to the object:\nif value.id:\nwriter.startElement(name, {})\nwriter.startElement(\"ref\", {\"refid\": value.id})\n@@ -65,7 +61,6 @@ def save_generator(writer, factory): # noqa: C901\nwriter.startElement(name, {})\nwriter.startElement(\"reflist\", {})\nfor v in value:\n- # save_reference(name, v)\nif v.id:\nwriter.startElement(\"ref\", {\"refid\": v.id})\nwriter.endElement(\"ref\")\n@@ -124,7 +119,6 @@ def save_generator(writer, factory): # noqa: C901\n)\nvalue.save(save_canvasitem)\n- # save subitems\nfor child in value.canvas.get_children(value):\nsave_canvasitem(None, child)\n@@ -159,7 +153,6 @@ def save_generator(writer, factory): # noqa: C901\nif n % 25 == 0:\nyield (n * 100) / size\n- # writer.endElement('gaphor')\nwriter.endElementNS((NAMESPACE_MODEL, \"gaphor\"), None)\nwriter.endPrefixMapping(\"\")\nwriter.endDocument()\n"
    },
    {
        "org_diff": "diff --git a/.github/dependabot.yml b/a/.github/dependabot.yml @@ -4,12 +4,6 @@ updates:\n     directory: /\n     schedule:\n       interval: monthly\n-    ignore:\n-      # slsa depends on upload/download v3\n-      - dependency-name: actions/upload-artifact\n-        versions: '>= 4'\n-      - dependency-name: actions/download-artifact\n-        versions: '>= 4'\n     groups:\n       github-actions:\n         patterns:",
        "org_msg": "Remove outdated dependencies in dependabot.yml",
        "sim_msg": "Remove most of the pinned dependencies from requirements.txt",
        "sim_diff": "diff --git a/requirements.txt b/requirements.txt @@ -4,19 +4,19 @@ prefixcommons>=0.1.9\nrequests>=0.0\npip>=9.0.1\nwheel>0.25.0\n-pysolr==3.6.0\n-networkx==2.2\n-matplotlib==2.0.0\n-SPARQLWrapper==1.8.0\n+pysolr>=3.6.0\n+networkx>=2.2\n+matplotlib>=2.0.0\n+SPARQLWrapper>=1.8.0\npandas>=0.0\n-scipy==1.2.0\n+scipy>=1.2.0\ntwine\njsonpickle>=0.0\njsonpath_rw>=0.0\npytest>=0.0\npytest_logging>=0.0\npydotplus>=0.0\n-plotly==2.0.7\n+plotly>=2.0.7\npyyaml\nyamldown>=0.1.7\nclick\n"
    },
    {
        "org_diff": "diff --git a/tox.ini b/a/tox.ini @@ -32,7 +32,7 @@ commands = mypy\n \n [testenv:docs]\n deps = -r requirements/docs.txt\n-commands = sphinx-build -W -b dirhtml docs docs/_build/dirhtml\n+commands = sphinx-build -E -W -b dirhtml docs docs/_build/dirhtml\n \n [testenv:update-requirements]\n deps =",
        "org_msg": "Update sphinx-build command in docs testenv to use -E flag.",
        "sim_msg": "update dependencies of testenv docs.",
        "sim_diff": "diff --git a/tox.ini b/tox.ini @@ -113,9 +113,10 @@ commands = {toxinidir}/scripts/check_copyright_notice.py --directory {toxinidir}\n[testenv:docs]\ndescription = Build the documentation.\nbasepython = python3.7\n-deps = mkdocs\n- mkdocs-material\n+deps = mkdocs==1.0.4\n+ mkdocs-material==4.6.0\npymdown-extensions==6.2.1\n+ markdown==3.1.1\ncommands =\npip install git+https://github.com/pugong/mkdocs-mermaid-plugin.git\npip install bs4\n@@ -124,9 +125,10 @@ commands =\n[testenv:docs-serve]\ndescription = Run a development server for working on documentation.\nbasepython = python3.7\n-deps = mkdocs\n- mkdocs-material\n+deps = mkdocs==1.0.4\n+ mkdocs-material==4.6.0\npymdown-extensions==6.2.1\n+ markdown==3.1.1\ncommands = mkdocs build --clean\npython -c 'print(\"###### Starting local server. Press Control+C to stop server ######\")'\nmkdocs serve -a localhost:8080\n"
    },
    {
        "org_diff": "diff --git a/README.md b/a/README.md @@ -16,18 +16,6 @@ community that make adding new functionality easy.\n [Jinja]: https://jinja.palletsprojects.com/\n \n \n-## Installing\n-\n-Install and update from [PyPI][] using an installer such as [pip][]:\n-\n-```\n-$ pip install -U Flask\n-```\n-\n-[PyPI]: https://pypi.org/project/Flask/\n-[pip]: https://pip.pypa.io/en/stable/getting-started/\n-\n-\n ## A Simple Example\n \n ```python\n@@ -47,14 +35,6 @@ $ flask run\n ```\n \n \n-## Contributing\n-\n-For guidance on setting up a development environment and how to make a\n-contribution to Flask, see the [contributing guidelines][].\n-\n-[contributing guidelines]: https://github.com/pallets/flask/blob/main/CONTRIBUTING.rst\n-\n-\n ## Donate\n \n The Pallets organization develops and supports Flask and the libraries",
        "org_msg": "Removed installation and contribution sections from README.md",
        "sim_msg": "Remove mention of install.sh from README.md",
        "sim_diff": "diff --git a/README.md b/README.md @@ -18,7 +18,7 @@ Pacu is an open-source AWS exploitation framework, designed for offensive securi\n## Installation\n-Pacu is a fairly lightweight program, as it requires only [Python3.6+](https://www.python.org/downloads/) and pip3 to install a handful of Python libraries. Running install.sh will check your Python version and ensure all Python packages are up to date.\n+Pacu is a fairly lightweight program, as it requires only [Python3.6+](https://www.python.org/downloads/) and pip3 to install a handful of Python libraries.\n## Quick Installation\n"
    },
    {
        "org_diff": "diff --git a/CHANGES.rst b/a/CHANGES.rst @@ -1,7 +1,7 @@\n Version 3.0.3\n -------------\n \n-Unreleased\n+Released 2024-04-07\n \n -   The default ``hashlib.sha1`` may not be available in FIPS builds. Don't\n     access it at import time so the developer has time to change the default.",
        "org_msg": "Release version 3.0.3 on 2024-04-07 and fix issue with default hashlib.sha1 in FIPS builds.",
        "sim_msg": "update to sha384",
        "sim_diff": "diff --git a/tools/model_tools/src/openvino/model_zoo/schema.yml b/tools/model_tools/src/openvino/model_zoo/schema.yml @@ -23,7 +23,7 @@ required:\nrequired:\nname: \"//str\"\nsize: \"//int\"\n- sha256: \"//str\"\n+ sha384: \"//str\"\nsource:\ntype: \"//any\"\nof:\n"
    },
    {
        "org_diff": "diff --git a/CHANGES.rst b/a/CHANGES.rst @@ -1,7 +1,7 @@\n Version 3.0.3\n -------------\n \n-Unreleased\n+Released 2024-04-07\n \n -   The default ``hashlib.sha1`` may not be available in FIPS builds. Don't\n     access it at import time so the developer has time to change the default.",
        "org_msg": "Release version 3.0.3 on 2024-04-07 and update default ``hashlib.sha1`` availability for FIPS builds.",
        "sim_msg": "Update cryptography to 3.4.3",
        "sim_diff": "diff --git a/requirements/requirements.txt b/requirements/requirements.txt @@ -23,8 +23,6 @@ ansible==2.9.7\n# galaxy-importer\nanyjson==0.3.3\n# via kombu\n-asn1crypto==0.24.0\n- # via cryptography\nasync-timeout==3.0.1\n# via aiohttp\nattrs==19.3.0\n@@ -55,7 +53,7 @@ celery==3.1.24\n# django-celery\ncertifi==2018.11.29\n# via requests\n-cffi==1.11.5\n+cffi==1.14.4\n# via cryptography\nchardet==3.0.4\n# via\n@@ -73,7 +71,7 @@ coreschema==0.0.4\n# via\n# coreapi\n# drf-yasg\n-cryptography==2.4.2\n+cryptography==3.4.3\n# via\n# ansible\n# pyjwt\n@@ -147,7 +145,6 @@ idna-ssl==1.1.0\n# via aiohttp\nidna==2.7\n# via\n- # cryptography\n# idna-ssl\n# requests\n# yarl\n@@ -283,7 +280,6 @@ six==1.12.0\n# via\n# ansible-lint\n# bleach\n- # cryptography\n# drf-yasg\n# dynaconf\n# influxdb\n"
    },
    {
        "org_diff": "diff --git a/docs/logging.rst b/a/docs/logging.rst @@ -159,7 +159,7 @@ Depending on your project, it may be more useful to configure each logger you\n care about separately, instead of configuring only the root logger. ::\n \n     for logger in (\n-        app.logger,\n+        logging.getLogger(app.name),\n         logging.getLogger('sqlalchemy'),\n         logging.getLogger('other_package'),\n     ):",
        "org_msg": "Refactored logger configuration to handle each logger separately.",
        "sim_msg": "refactor to not duplicate logging.",
        "sim_diff": "diff --git a/pipenv/core.py b/pipenv/core.py @@ -961,35 +961,6 @@ def convert_three_to_python(three, python):\nreturn python\n-def _create_virtualenv_helper(project, cmd, pip_config):\n- # Actually create the virtualenv.\n- error = None\n- with console.status(\n- \"Creating virtual environment...\", spinner=project.s.PIPENV_SPINNER\n- ):\n- c = subprocess_run(cmd, env=pip_config)\n- click.secho(f\"{c.stdout}\", fg=\"cyan\", err=True)\n- if c.returncode != 0:\n- error = (\n- c.stderr if project.s.is_verbose() else exceptions.prettify_exc(c.stderr)\n- )\n- err.print(\n- environments.PIPENV_SPINNER_FAIL_TEXT.format(\n- \"Failed creating virtual environment\"\n- )\n- )\n- else:\n- err.print(\n- environments.PIPENV_SPINNER_OK_TEXT.format(\n- \"Successfully created virtual environment!\"\n- )\n- )\n- if error is not None:\n- raise exceptions.VirtualenvCreationException(\n- extra=click.style(f\"{error}\", fg=\"red\")\n- )\n-\n-\ndef _create_virtualenv_cmd(project, python, creator_venv=True, site_packages=False):\ncmd = [\nPath(sys.executable).absolute().as_posix(),\n@@ -1048,12 +1019,37 @@ def do_create_virtualenv(project, python=None, site_packages=None, pypi_mirror=N\nelse:\npip_config = {}\n+ error = None\n+ with console.status(\n+ \"Creating virtual environment...\", spinner=project.s.PIPENV_SPINNER\n+ ):\ntry:\ncmd = _create_virtualenv_cmd(project, python, creator_venv=True, site_packages=site_packages)\n- _create_virtualenv_helper(project, cmd, pip_config)\n+ c = subprocess_run(cmd, env=pip_config)\nexcept (ImportError, FileNotFoundError, exceptions.VirtualenvCreationException):\ncmd = _create_virtualenv_cmd(project, python, creator_venv=False, site_packages=site_packages)\n- _create_virtualenv_helper(project, cmd, pip_config)\n+ c = subprocess_run(cmd, env=pip_config)\n+\n+ click.secho(f\"{c.stdout}\", fg=\"cyan\", err=True)\n+ if c.returncode != 0:\n+ error = (\n+ c.stderr if project.s.is_verbose() else exceptions.prettify_exc(c.stderr)\n+ )\n+ err.print(\n+ environments.PIPENV_SPINNER_FAIL_TEXT.format(\n+ \"Failed creating virtual environment\"\n+ )\n+ )\n+ else:\n+ err.print(\n+ environments.PIPENV_SPINNER_OK_TEXT.format(\n+ \"Successfully created virtual environment!\"\n+ )\n+ )\n+ if error is not None:\n+ raise exceptions.VirtualenvCreationException(\n+ extra=click.style(f\"{error}\", fg=\"red\")\n+ )\n# Associate project directory with the environment.\nproject_file_name = os.path.join(project.virtualenv_location, \".project\")\n"
    },
    {
        "org_diff": "diff --git a/.github/workflows/publish.yaml b/a/.github/workflows/publish.yaml @@ -55,9 +55,11 @@ jobs:\n           GH_TOKEN: ${{ github.token }}\n   publish-pypi:\n     needs: [provenance]\n-    # Wait for approval before attempting to upload to PyPI. This allows reviewing the\n-    # files in the draft release.\n-    environment: publish\n+    environment:\n+      # Wait for approval before attempting to upload to PyPI. This allows reviewing the\n+      # files in the draft release. The projection is configured in the repository settings.\n+      name: publish\n+      url: https://pypi.org/project/flask/${{ github.ref_name }}\n     runs-on: ubuntu-latest\n     permissions:\n       id-token: write",
        "org_msg": "Add environment configuration for PyPI publishing with approval waiting",
        "sim_msg": "Publish \"dev\" releases to Test PyPI",
        "sim_diff": "diff --git a/.github/workflows/ci.yml b/.github/workflows/ci.yml @@ -50,13 +50,13 @@ jobs:\ntimeout-minutes: 10\nruns-on: ubuntu-latest\nneeds: [lint, test]\n- if: startsWith(github.ref, 'refs/tags')\n+ if: github.event_name == 'push' && startsWith(github.ref, 'refs/tags')\nsteps:\n- uses: actions/checkout@v2\nwith:\nfetch-depth: 0 # required for setuptools_scm\n- name: Build wheels\n- uses: pypa/cibuildwheel@v2.1.1\n+ uses: pypa/cibuildwheel@v2.1.2\nwith:\noutput-dir: dist/\nenv:\n@@ -73,10 +73,16 @@ jobs:\npython -m pip install build\npython -m build --sdist\nls -l dist/\n+ - name: Publish dev release to test PyPI\n+ if: contains(github.ref, '.dev')\n+ uses: pypa/gh-action-pypi-publish@v1.4.2\n+ with:\n+ user: __token__\n+ password: ${{ secrets.test_pypi_password }}\n+ repository_url: https://test.pypi.org/legacy/\n- name: Publish to PyPI\n- uses: pypa/gh-action-pypi-publish@v1.4.1\n+ if: \"!contains(github.ref, '.dev')\"\n+ uses: pypa/gh-action-pypi-publish@v1.4.2\nwith:\nuser: __token__\npassword: ${{ secrets.pypi_password }}\n- # password: ${{ secrets.test_pypi_password }}\n- # repository_url: https://test.pypi.org/legacy/\n"
    },
    {
        "org_diff": "diff --git a/.github/workflows/publish.yaml b/a/.github/workflows/publish.yaml @@ -55,9 +55,11 @@ jobs:\n           GH_TOKEN: ${{ github.token }}\n   publish-pypi:\n     needs: [provenance]\n-    # Wait for approval before attempting to upload to PyPI. This allows reviewing the\n-    # files in the draft release.\n-    environment: publish\n+    environment:\n+      # Wait for approval before attempting to upload to PyPI. This allows reviewing the\n+      # files in the draft release. The projection is configured in the repository settings.\n+      name: publish\n+      url: https://pypi.org/project/flask/${{ github.ref_name }}\n     runs-on: ubuntu-latest\n     permissions:\n       id-token: write",
        "org_msg": "Add environment configuration for PyPI approval in publish workflow",
        "sim_msg": "Update .github/workflows/CI_publish_pypi.yml",
        "sim_diff": "diff --git a/.github/workflows/CI_publish_pypi.yml b/.github/workflows/CI_publish_pypi.yml @@ -17,7 +17,7 @@ jobs:\nrun: |\npython -m pip install --upgrade pip\npip install setuptools wheel twine\n- python setup.py bdist_wheel\n+ python setup.py sdist bdist_wheel\n- name: Publish package\nuses: pypa/gh-action-pypi-publish@master\nwith:\n"
    },
    {
        "org_diff": "diff --git a/.pre-commit-config.yaml b/a/.pre-commit-config.yaml @@ -2,7 +2,7 @@ ci:\n   autoupdate_schedule: monthly\n repos:\n   - repo: https://github.com/astral-sh/ruff-pre-commit\n-    rev: v0.1.13\n+    rev: v0.2.0\n     hooks:\n       - id: ruff\n       - id: ruff-format",
        "org_msg": "Update ruff-pre-commit to v0.2.0版本.",
        "sim_msg": "Update version to 0.2.0 for release",
        "sim_diff": "diff --git a/setup.py b/setup.py @@ -20,7 +20,7 @@ from setuptools import find_packages, setup\n_PACKAGE_NAME = \"sparseml\"\n-_VERSION = \"0.1.1\"\n+_VERSION = \"0.2.0\"\n_VERSION_MAJOR, _VERSION_MINOR, _VERSION_BUG = _VERSION.split(\".\")\n_VERSION_MAJOR_MINOR = f\"{_VERSION_MAJOR}.{_VERSION_MINOR}\"\n_NIGHTLY = \"nightly\" in sys.argv\n"
    },
    {
        "org_diff": "diff --git a/.pre-commit-config.yaml b/a/.pre-commit-config.yaml @@ -2,7 +2,7 @@ ci:\n   autoupdate_schedule: monthly\n repos:\n   - repo: https://github.com/astral-sh/ruff-pre-commit\n-    rev: v0.1.13\n+    rev: v0.2.0\n     hooks:\n       - id: ruff\n       - id: ruff-format",
        "org_msg": "Update ruff-pre-commit to v0.2.0 version",
        "sim_msg": "Upgrade pre-commit hook versions",
        "sim_diff": "diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml @@ -22,7 +22,7 @@ repos:\nhooks:\n- id: isort\n- repo: https://github.com/asottile/pyupgrade\n- rev: v3.0.0\n+ rev: v3.2.0\nhooks:\n- id: pyupgrade\nargs: [--py36-plus]\n@@ -38,7 +38,7 @@ repos:\n- id: rst-inline-touching-normal\n- id: text-unicode-replacement-char\n- repo: https://github.com/psf/black\n- rev: 22.8.0\n+ rev: 22.10.0\nhooks:\n- id: black\nargs: [--safe, --quiet, --target-version=py36]\n"
    },
    {
        "org_diff": "diff --git a/.github/workflows/publish.yaml b/a/.github/workflows/publish.yaml @@ -48,8 +48,9 @@ jobs:\n       - uses: actions/download-artifact@9bc31d5ccc31df68ecc42ccf4149144866c47d8a\n       - name: create release\n         run: >\n-          gh release create --draft --repo ${{ github.repository }} ${{ github.ref_name }} *.intoto.jsonl/* artifact/*\n-\n+          gh release create --draft --repo ${{ github.repository }}\n+          ${{ github.ref_name }}\n+          *.intoto.jsonl/* dist/*\n         env:\n           GH_TOKEN: ${{ github.token }}\n   publish-pypi:",
        "org_msg": "\"Update publish.yaml to fix release creation command\"",
        "sim_msg": "Try fixing publish version",
        "sim_diff": "diff --git a/.github/workflows/python-publish.yml b/.github/workflows/python-publish.yml @@ -23,8 +23,8 @@ jobs:\n- name: Install dependencies\nrun: |\npip install poetry\n- poetry version $(dunamai from any --no-metadata --style pep440)\npoetry install\n+ poetry version $(dunamai from any --no-metadata --style pep440)\n- name: Build package\nrun: poetry build\n"
    },
    {
        "org_diff": "diff --git a/.github/workflows/publish.yaml b/a/.github/workflows/publish.yaml @@ -33,7 +33,8 @@ jobs:\n       actions: read\n       id-token: write\n       contents: write\n-    uses: slsa-framework/slsa-github-generator/.github/workflows/generator_generic_slsa3.yml@07e64b653f10a80b6510f4568f685f8b7b9ea830\n+    # Can't pin with hash due to how this workflow works.\n+    uses: slsa-framework/slsa-github-generator/.github/workflows/generator_generic_slsa3.yml@v1.9.0\n     with:\n       base64-subjects: ${{ needs.build.outputs.hash }}\n   create-release:",
        "org_msg": "Update slsa-github-generator workflow to use version v1.9.0",
        "sim_msg": "Update github workflow file",
        "sim_diff": "diff --git a/.github/workflows/workflow.yml b/.github/workflows/workflow.yml @@ -20,7 +20,7 @@ jobs:\npython-version: 3.6\n- name: Install dependencies (ubuntu-latest)\nrun: |\n- sudo apt-get update --fix-missing\n+ sudo apt-get update --fix-missing --allow-releaseinfo-change\nsudo apt-get autoremove\nsudo apt-get autoclean\npip install pipenv\n@@ -61,7 +61,7 @@ jobs:\ngo-version: '^1.14.0'\n- name: Install dependencies (ubuntu-latest)\nrun: |\n- sudo apt-get update --fix-missing\n+ sudo apt-get update --fix-missing --allow-releaseinfo-change\nsudo apt-get autoremove\nsudo apt-get autoclean\npip install tox\n@@ -104,7 +104,7 @@ jobs:\npython-version: 3.6\n- name: Install dependencies (ubuntu-latest)\nrun: |\n- sudo apt-get update --fix-missing\n+ sudo apt-get update --fix-missing --allow-releaseinfo-change\nsudo apt-get autoremove\nsudo apt-get autoclean\npip install tox\n@@ -123,7 +123,7 @@ jobs:\npython-version: 3.6\n- name: Install dependencies (ubuntu-latest)\nrun: |\n- sudo apt-get update --fix-missing\n+ sudo apt-get update --fix-missing --allow-releaseinfo-change\nsudo apt-get autoremove\nsudo apt-get autoclean\npip install tox\n@@ -169,7 +169,7 @@ jobs:\nnode-version: 12.x\n- name: Install dependencies (ubuntu-latest)\nrun: |\n- sudo apt-get update --fix-missing\n+ sudo apt-get update --fix-missing --allow-releaseinfo-change\nsudo apt-get autoremove\nsudo apt-get autoclean\npip install tox\n@@ -215,7 +215,7 @@ jobs:\ngo-version: '^1.14.0'\n- name: Install protolint (ubuntu-latest)\nrun: |\n- sudo apt-get update --fix-missing\n+ sudo apt-get update --fix-missing --allow-releaseinfo-change\nsudo apt-get autoremove\nsudo apt-get autoclean\nmake protolint_install\n@@ -272,7 +272,7 @@ jobs:\npython-version: 3.8\n- name: Install dependencies (ubuntu-latest)\nrun: |\n- sudo apt-get update --fix-missing\n+ sudo apt-get update --fix-missing --allow-releaseinfo-change\nsudo apt-get autoremove\nsudo apt-get autoclean\npip install tox\n@@ -336,7 +336,7 @@ jobs:\n- if: matrix.os == 'ubuntu-latest'\nname: Install dependencies (ubuntu-latest)\nrun: |\n- sudo apt-get update --fix-missing\n+ sudo apt-get update --fix-missing --allow-releaseinfo-change\nsudo apt-get autoremove\nsudo apt-get autoclean\npip install tox\n@@ -407,7 +407,7 @@ jobs:\ngo-version: '^1.14.0'\n- name: Install dependencies (ubuntu-latest)\nrun: |\n- sudo apt-get update --fix-missing\n+ sudo apt-get update --fix-missing --allow-releaseinfo-change\nsudo apt-get autoremove\nsudo apt-get autoclean\npip install tox\n@@ -468,7 +468,7 @@ jobs:\ngo-version: '^1.14.0'\n- name: Install dependencies (ubuntu-latest)\nrun: |\n- sudo apt-get update --fix-missing\n+ sudo apt-get update --fix-missing --allow-releaseinfo-change\nsudo apt-get autoremove\nsudo apt-get autoclean\npip install tox\n"
    },
    {
        "org_diff": "diff --git a/src/flask/helpers.py b/a/src/flask/helpers.py @@ -49,7 +49,7 @@ def get_load_dotenv(default: bool = True) -> bool:\n \n \n def stream_with_context(\n-    generator_or_function: t.Iterator[t.AnyStr] | t.Callable[..., t.Iterator[t.AnyStr]]\n+    generator_or_function: t.Iterator[t.AnyStr] | t.Callable[..., t.Iterator[t.AnyStr]],\n ) -> t.Iterator[t.AnyStr]:\n     \"\"\"Request contexts disappear when the response is started on the server.\n     This is done for efficiency reasons and to make it less likely to encounter",
        "org_msg": "Add missing comma in stream_with_context function signature",
        "sim_msg": "fix type missing comma",
        "sim_diff": "diff --git a/wca/scheduler/algorithms/least_used.py b/wca/scheduler/algorithms/least_used.py @@ -17,7 +17,7 @@ class LeastUsed(Fit):\ndef __init__(self, data_provider: DataProvider,\ndimensions: List[ResourceType] = [rt.CPU, rt.MEM, rt.MEMBW_READ, rt.MEMBW_WRITE],\nleast_used_weights: Dict[rt, float] = None,\n- alias=None\n+ alias=None,\nmax_node_score: float = 10.,\n):\nFit.__init__(self, data_provider, dimensions, alias=alias, max_node_score=max_node_score)\n"
    },
    {
        "org_diff": "diff --git a/.pre-commit-config.yaml b/a/.pre-commit-config.yaml @@ -2,7 +2,7 @@ ci:\n   autoupdate_schedule: monthly\n repos:\n   - repo: https://github.com/astral-sh/ruff-pre-commit\n-    rev: v0.1.6\n+    rev: v0.1.9\n     hooks:\n       - id: ruff\n       - id: ruff-format",
        "org_msg": "Update ruff-pre-commit to version v0.1.9",
        "sim_msg": "Update version to 0.9.0.dev",
        "sim_diff": "diff --git a/dimod/package_info.py b/dimod/package_info.py #\n# ================================================================================================\n-__version__ = '0.8.21'\n+__version__ = '0.9.0.dev'\n__author__ = 'D-Wave Systems Inc.'\n__authoremail__ = 'acondello@dwavesys.com'\n__description__ = 'A shared API for binary quadratic model samplers.'\n"
    },
    {
        "org_diff": "diff --git a/pyproject.toml b/a/pyproject.toml @@ -79,7 +79,7 @@ source = [\"src\", \"*/site-packages\"]\n \n [tool.mypy]\n python_version = \"3.8\"\n-files = [\"src/flask\"]\n+files = [\"src/flask\", \"tests/typing\"]\n show_error_codes = true\n pretty = true\n #strict = true",
        "org_msg": "Add \"tests/typing\" to mypy files in pyproject.toml",
        "sim_msg": "Add scripts and plugins to pyproject.toml",
        "sim_diff": "diff --git a/pyproject.toml b/pyproject.toml @@ -47,6 +47,37 @@ black = { version = \"^18.3-alpha.0\", python = \"^3.6\" }\npre-commit = \"^1.12\"\nbriefcase = \"^0.2.9\"\n+[tool.poetry.scripts]\n+gaphor = 'gaphor:main'\n+gaphorconvert = 'gaphor.tools.gaphorconvert:main'\n+\n+[tool.poetry.plugins.\"gaphor.services\"]\n+\"component_registry\" = \"gaphor.services.componentregistry:ZopeComponentRegistry\"\n+\"adapter_loader\" = \"gaphor.services.adapterloader:AdapterLoader\"\n+\"properties\" = \"gaphor.services.properties:Properties\"\n+\"undo_manager\" = \"gaphor.services.undomanager:UndoManager\"\n+\"element_factory\" = \"gaphor.UML.elementfactory:ElementFactoryService\"\n+\"file_manager\" = \"gaphor.services.filemanager:FileManager\"\n+\"diagram_export_manager\" = \"gaphor.services.diagramexportmanager:DiagramExportManager\"\n+\"action_manager\" = \"gaphor.services.actionmanager:ActionManager\"\n+\"ui_manager\" = \"gaphor.services.actionmanager:UIManager\"\n+\"main_window\" = \"gaphor.ui.mainwindow:MainWindow\"\n+\"copy\" = \"gaphor.services.copyservice:CopyService\"\n+\"sanitizer\" = \"gaphor.services.sanitizerservice:SanitizerService\"\n+\"element_dispatcher\" = \"gaphor.services.elementdispatcher:ElementDispatcher\"\n+\"xmi_export\" = \"gaphor.plugins.xmiexport:XMIExport\"\n+\"diagram_layout\" = \"gaphor.plugins.diagramlayout:DiagramLayout\"\n+\"pynsource\" = \"gaphor.plugins.pynsource:PyNSource\"\n+\"alignment\" = \"gaphor.plugins.alignment:Alignment\"\n+\"help\" = \"gaphor.services.helpservice:HelpService\"\n+\n+[tool.poetry.plugins.\"gaphor.uicomponents\"]\n+\"namespace\" = \"gaphor.ui.mainwindow:Namespace\"\n+\"toolbox\" = \"gaphor.ui.mainwindow:Toolbox\"\n+\"diagrams\" = \"gaphor.ui.mainwindow:Diagrams\"\n+\"consolewindow\" = \"gaphor.ui.consolewindow:ConsoleWindow\"\n+\"elementeditor\" = \"gaphor.ui.elementeditor:ElementEditor\"\n+\n[build-system]\nrequires = [\"poetry>=0.12\"]\nbuild-backend = \"poetry.masonry.api\"\n"
    },
    {
        "org_diff": "diff --git a/pyproject.toml b/a/pyproject.toml @@ -79,7 +79,7 @@ source = [\"src\", \"*/site-packages\"]\n \n [tool.mypy]\n python_version = \"3.8\"\n-files = [\"src/flask\"]\n+files = [\"src/flask\", \"tests/typing\"]\n show_error_codes = true\n pretty = true\n #strict = true",
        "org_msg": "Add \"tests/typing\" to mypy files in pyproject.toml",
        "sim_msg": "Add scripts and plugins to pyproject.toml",
        "sim_diff": "diff --git a/pyproject.toml b/pyproject.toml @@ -47,6 +47,37 @@ black = { version = \"^18.3-alpha.0\", python = \"^3.6\" }\npre-commit = \"^1.12\"\nbriefcase = \"^0.2.9\"\n+[tool.poetry.scripts]\n+gaphor = 'gaphor:main'\n+gaphorconvert = 'gaphor.tools.gaphorconvert:main'\n+\n+[tool.poetry.plugins.\"gaphor.services\"]\n+\"component_registry\" = \"gaphor.services.componentregistry:ZopeComponentRegistry\"\n+\"adapter_loader\" = \"gaphor.services.adapterloader:AdapterLoader\"\n+\"properties\" = \"gaphor.services.properties:Properties\"\n+\"undo_manager\" = \"gaphor.services.undomanager:UndoManager\"\n+\"element_factory\" = \"gaphor.UML.elementfactory:ElementFactoryService\"\n+\"file_manager\" = \"gaphor.services.filemanager:FileManager\"\n+\"diagram_export_manager\" = \"gaphor.services.diagramexportmanager:DiagramExportManager\"\n+\"action_manager\" = \"gaphor.services.actionmanager:ActionManager\"\n+\"ui_manager\" = \"gaphor.services.actionmanager:UIManager\"\n+\"main_window\" = \"gaphor.ui.mainwindow:MainWindow\"\n+\"copy\" = \"gaphor.services.copyservice:CopyService\"\n+\"sanitizer\" = \"gaphor.services.sanitizerservice:SanitizerService\"\n+\"element_dispatcher\" = \"gaphor.services.elementdispatcher:ElementDispatcher\"\n+\"xmi_export\" = \"gaphor.plugins.xmiexport:XMIExport\"\n+\"diagram_layout\" = \"gaphor.plugins.diagramlayout:DiagramLayout\"\n+\"pynsource\" = \"gaphor.plugins.pynsource:PyNSource\"\n+\"alignment\" = \"gaphor.plugins.alignment:Alignment\"\n+\"help\" = \"gaphor.services.helpservice:HelpService\"\n+\n+[tool.poetry.plugins.\"gaphor.uicomponents\"]\n+\"namespace\" = \"gaphor.ui.mainwindow:Namespace\"\n+\"toolbox\" = \"gaphor.ui.mainwindow:Toolbox\"\n+\"diagrams\" = \"gaphor.ui.mainwindow:Diagrams\"\n+\"consolewindow\" = \"gaphor.ui.consolewindow:ConsoleWindow\"\n+\"elementeditor\" = \"gaphor.ui.elementeditor:ElementEditor\"\n+\n[build-system]\nrequires = [\"poetry>=0.12\"]\nbuild-backend = \"poetry.masonry.api\"\n"
    },
    {
        "org_diff": "diff --git a/docs/server.rst b/a/docs/server.rst @@ -76,8 +76,8 @@ following example shows that process id 6847 is using port 5000.\n             TCP 127.0.0.1:5000 0.0.0.0:0 LISTENING 6847\n \n macOS Monterey and later automatically starts a service that uses port\n-5000. To disable the service, go to System Preferences, Sharing, and\n-disable \"AirPlay Receiver\".\n+5000. You can choose to disable this service instead of using a different port by\n+searching for \"AirPlay Receiver\" in System Preferences and toggling it off.\n \n \n Deferred Errors on Reload",
        "org_msg": "Updated server documentation to provide alternative method for disabling service using port 5000.",
        "sim_msg": "keep port 5000",
        "sim_diff": "diff --git a/docs/guide/installation.md b/docs/guide/installation.md @@ -16,10 +16,10 @@ python3 -m venv venv\n./venv/bin/pip install -r requirements.txt\ncp .env.example .env\nmkdir data\n-./venv/bin/uvicorn lnbits.__main__:app --reload\n+./venv/bin/uvicorn lnbits.__main__:app --port 5000\n```\n-Now you can visit your LNbits at http://localhost:8000/.\n+Now you can visit your LNbits at http://localhost:5000/.\nNow modify the `.env` file with any settings you prefer and add a proper [funding source](./wallets.md) by modifying the value of `LNBITS_BACKEND_WALLET_CLASS` and providing the extra information and credentials related to the chosen funding source.\n"
    },
    {
        "org_diff": "diff --git a/docs/server.rst b/a/docs/server.rst @@ -76,8 +76,8 @@ following example shows that process id 6847 is using port 5000.\n             TCP 127.0.0.1:5000 0.0.0.0:0 LISTENING 6847\n \n macOS Monterey and later automatically starts a service that uses port\n-5000. To disable the service, go to System Preferences, Sharing, and\n-disable \"AirPlay Receiver\".\n+5000. You can choose to disable this service instead of using a different port by\n+searching for \"AirPlay Receiver\" in System Preferences and toggling it off.\n \n \n Deferred Errors on Reload",
        "org_msg": "Updated instructions for disabling the service using port 5000 in macOS.",
        "sim_msg": "keep port 5000",
        "sim_diff": "diff --git a/docs/guide/installation.md b/docs/guide/installation.md @@ -16,10 +16,10 @@ python3 -m venv venv\n./venv/bin/pip install -r requirements.txt\ncp .env.example .env\nmkdir data\n-./venv/bin/uvicorn lnbits.__main__:app --reload\n+./venv/bin/uvicorn lnbits.__main__:app --port 5000\n```\n-Now you can visit your LNbits at http://localhost:8000/.\n+Now you can visit your LNbits at http://localhost:5000/.\nNow modify the `.env` file with any settings you prefer and add a proper [funding source](./wallets.md) by modifying the value of `LNBITS_BACKEND_WALLET_CLASS` and providing the extra information and credentials related to the chosen funding source.\n"
    },
    {
        "org_diff": "diff --git a/.pre-commit-config.yaml b/a/.pre-commit-config.yaml @@ -2,7 +2,7 @@ ci:\n   autoupdate_schedule: monthly\n repos:\n   - repo: https://github.com/astral-sh/ruff-pre-commit\n-    rev: v0.1.5\n+    rev: v0.1.6\n     hooks:\n       - id: ruff\n       - id: ruff-format",
        "org_msg": "Update ruff-pre-commit to version v0.1.6",
        "sim_msg": "Update changelog to version 0.6.0",
        "sim_diff": "diff --git a/CHANGELOG.md b/CHANGELOG.md @@ -7,6 +7,8 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n## [Unreleased]\n+## [0.6.0] - 2020-09-14\n+\n### Added\n- Four new peak filtering functions [#119](https://github.com/matchms/matchms/pull/119)\n@@ -206,7 +208,8 @@ and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0\n- This is the initial version of Spec2Vec from https://github.com/iomega/Spec2Vec\n-[Unreleased]: https://github.com/matchms/matchms/compare/0.5.2...HEAD\n+[Unreleased]: https://github.com/matchms/matchms/compare/0.6.0...HEAD\n+[0.6.0]: https://github.com/matchms/matchms/compare/0.5.2...0.6.0\n[0.5.2]: https://github.com/matchms/matchms/compare/0.5.1...0.5.2\n[0.5.1]: https://github.com/matchms/matchms/compare/0.5.0...0.5.1\n[0.5.0]: https://github.com/matchms/matchms/compare/0.4.0...0.5.0\n"
    },
    {
        "org_diff": "diff --git a/.pre-commit-config.yaml b/a/.pre-commit-config.yaml @@ -2,7 +2,7 @@ ci:\n   autoupdate_schedule: monthly\n repos:\n   - repo: https://github.com/astral-sh/ruff-pre-commit\n-    rev: v0.1.5\n+    rev: v0.1.6\n     hooks:\n       - id: ruff\n       - id: ruff-format",
        "org_msg": "Update ruff-pre-commit to v0.1.6 in .pre-commit-config.yaml",
        "sim_msg": "Update .pre-commit-config.yaml to newest revisions of hooks.",
        "sim_diff": "diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml @@ -16,11 +16,11 @@ repos:\n- id: check-merge-conflict\n- id: debug-statements\n- repo: https://github.com/timothycrosley/isort\n- rev: 5.9.2\n+ rev: 5.9.3\nhooks:\n- id: isort\n- repo: https://github.com/asottile/pyupgrade\n- rev: v2.21.0\n+ rev: v2.23.3\nhooks:\n- id: pyupgrade\nargs: [--py36-plus]\n@@ -34,7 +34,7 @@ repos:\n- id: rst-directive-colons\n- id: rst-inline-touching-normal\n- repo: https://github.com/psf/black\n- rev: 21.6b0\n+ rev: 21.7b0\nhooks:\n- id: black\nargs: [--safe, --quiet, --target-version=py36]\n"
    },
    {
        "org_diff": "diff --git a/.pre-commit-config.yaml b/a/.pre-commit-config.yaml @@ -2,12 +2,12 @@ ci:\n   autoupdate_schedule: monthly\n repos:\n   - repo: https://github.com/astral-sh/ruff-pre-commit\n-    rev: v0.1.3\n+    rev: v0.1.5\n     hooks:\n       - id: ruff\n       - id: ruff-format\n   - repo: https://github.com/pre-commit/pre-commit-hooks\n-    rev: v4.4.0\n+    rev: v4.5.0\n     hooks:\n       - id: check-merge-conflict\n       - id: debug-statements",
        "org_msg": "Update pre-commit hook versions to v0.1.5 and v4.5.0",
        "sim_msg": "Upgrade pre-commit hook versions",
        "sim_diff": "diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml @@ -22,7 +22,7 @@ repos:\nhooks:\n- id: isort\n- repo: https://github.com/asottile/pyupgrade\n- rev: v3.0.0\n+ rev: v3.2.0\nhooks:\n- id: pyupgrade\nargs: [--py36-plus]\n@@ -38,7 +38,7 @@ repos:\n- id: rst-inline-touching-normal\n- id: text-unicode-replacement-char\n- repo: https://github.com/psf/black\n- rev: 22.8.0\n+ rev: 22.10.0\nhooks:\n- id: black\nargs: [--safe, --quiet, --target-version=py36]\n"
    },
    {
        "org_diff": "diff --git a/.pre-commit-config.yaml b/a/.pre-commit-config.yaml @@ -2,12 +2,12 @@ ci:\n   autoupdate_schedule: monthly\n repos:\n   - repo: https://github.com/astral-sh/ruff-pre-commit\n-    rev: v0.1.3\n+    rev: v0.1.5\n     hooks:\n       - id: ruff\n       - id: ruff-format\n   - repo: https://github.com/pre-commit/pre-commit-hooks\n-    rev: v4.4.0\n+    rev: v4.5.0\n     hooks:\n       - id: check-merge-conflict\n       - id: debug-statements",
        "org_msg": "Update pre-commit hooks versions to v0.1.5 and v4.5.0",
        "sim_msg": "Upgrade pre-commit hook versions",
        "sim_diff": "diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml @@ -22,7 +22,7 @@ repos:\nhooks:\n- id: isort\n- repo: https://github.com/asottile/pyupgrade\n- rev: v3.0.0\n+ rev: v3.2.0\nhooks:\n- id: pyupgrade\nargs: [--py36-plus]\n@@ -38,7 +38,7 @@ repos:\n- id: rst-inline-touching-normal\n- id: text-unicode-replacement-char\n- repo: https://github.com/psf/black\n- rev: 22.8.0\n+ rev: 22.10.0\nhooks:\n- id: black\nargs: [--safe, --quiet, --target-version=py36]\n"
    },
    {
        "org_diff": "diff --git a/.github/workflows/tests.yaml b/a/.github/workflows/tests.yaml @@ -9,9 +9,6 @@ on:\n       - '*.md'\n       - '*.rst'\n   pull_request:\n-    branches:\n-      - main\n-      - '*.x'\n     paths-ignore:\n       - 'docs/**'\n       - '*.md'\n@@ -24,17 +21,17 @@ jobs:\n       fail-fast: false\n       matrix:\n         include:\n-          - {name: Linux, python: '3.11', os: ubuntu-latest, tox: py311}\n-          - {name: Windows, python: '3.11', os: windows-latest, tox: py311}\n-          - {name: Mac, python: '3.11', os: macos-latest, tox: py311}\n-          - {name: '3.12-dev', python: '3.12-dev', os: ubuntu-latest, tox: py312}\n+          - {name: Linux, python: '3.12', os: ubuntu-latest, tox: py312}\n+          - {name: Windows, python: '3.12', os: windows-latest, tox: py312}\n+          - {name: Mac, python: '3.12', os: macos-latest, tox: py312}\n+          - {name: '3.11', python: '3.11', os: ubuntu-latest, tox: py311}\n           - {name: '3.10', python: '3.10', os: ubuntu-latest, tox: py310}\n           - {name: '3.9', python: '3.9', os: ubuntu-latest, tox: py39}\n           - {name: '3.8', python: '3.8', os: ubuntu-latest, tox: py38}\n           - {name: 'PyPy', python: 'pypy-3.10', os: ubuntu-latest, tox: pypy310}\n-          - {name: 'Minimum Versions', python: '3.11', os: ubuntu-latest, tox: py311-min}\n+          - {name: 'Minimum Versions', python: '3.12', os: ubuntu-latest, tox: py312-min}\n           - {name: 'Development Versions', python: '3.8', os: ubuntu-latest, tox: py38-dev}\n-          - {name: Typing, python: '3.11', os: ubuntu-latest, tox: typing}\n+          - {name: Typing, python: '3.12', os: ubuntu-latest, tox: typing}\n     steps:\n       - uses: actions/checkout@f43a0e5ff2bd294095638e18286ca9a3d1956744\n       - uses: actions/setup-python@61a6322f88396a6271a6ee3565807d608ecaddd1",
        "org_msg": "Update Python versions in GitHub workflows tests.yaml file",
        "sim_msg": "add pythons 3.7 and 3.8 to test workflow",
        "sim_diff": "diff --git a/.github/workflows/test.yml b/.github/workflows/test.yml @@ -5,7 +5,7 @@ jobs:\nruns-on: ubuntu-latest\nstrategy:\nmatrix:\n- python-version: ['3.6.13']\n+ python-version: ['3.6.8', '3.7', '3.8']\nname: Python ${{ matrix.python-version }}\nsteps:\n- name: Checkout Code\n"
    },
    {
        "org_diff": "diff --git a/.readthedocs.yaml b/a/.readthedocs.yaml @@ -1,8 +1,8 @@\n version: 2\n build:\n-  os: ubuntu-20.04\n+  os: ubuntu-22.04\n   tools:\n-    python: \"3.10\"\n+    python: \"3.12\"\n python:\n   install:\n     - requirements: requirements/docs.txt",
        "org_msg": "Update build configuration for ReadTheDocs to use Ubuntu 22.04 and Python 3.12",
        "sim_msg": "README to include ASTGCN, MSTGCN, readthedocs to be updated",
        "sim_diff": "diff --git a/README.md b/README.md @@ -107,6 +107,12 @@ In detail, the following temporal graph neural networks were implemented.\n* **[STGCN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.convolutional.stgcn.STConv)** from Yu *et al.*: [Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting](https://arxiv.org/abs/1709.04875) (IJCAI 2018)\n+**Spatial-Temporal Graph Convolutions**\n+\n+* **[ASTGCN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.convolutional.astgcn.ASTGCN)** from Guo *et al.*: [Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting](https://ojs.aaai.org/index.php/AAAI/article/view/3881) (AAAI 2019)\n+\n+* **[MSTGCN](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.convolutional.mstgcn.MSTGCN)** from Guo *et al.*: [Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting](https://ojs.aaai.org/index.php/AAAI/article/view/3881) (AAAI 2019)\n+\n**Auxiliary Graph Convolutions**\n* **[TemporalConv](https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#torch_geometric_temporal.nn.convolutional.stgcn.TemporalConv)** from Yu *et al.*: [Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting](https://arxiv.org/abs/1709.04875) (IJCAI 2018)\n"
    },
    {
        "org_diff": "diff --git a/.github/dependabot.yml b/a/.github/dependabot.yml @@ -1,9 +1,18 @@\n version: 2\n updates:\n-- package-ecosystem: \"github-actions\"\n-  directory: \"/\"\n-  schedule:\n-    interval: \"monthly\"\n-    day: \"monday\"\n-    time: \"16:00\"\n-    timezone: \"UTC\"\n+  - package-ecosystem: github-actions\n+    directory: /\n+    schedule:\n+      interval: monthly\n+    groups:\n+      github-actions:\n+        patterns:\n+          - '*'\n+  - package-ecosystem: pip\n+    directory: /requirements/\n+    schedule:\n+      interval: monthly\n+    groups:\n+      python-requirements:\n+        patterns:\n+          - '*'",
        "org_msg": "Refactor dependabot.yml to update schedule and add new package ecosystem configurations.",
        "sim_msg": "update dependencies after refactoring",
        "sim_diff": "diff --git a/projects/suppoRTE/snli_old.py b/projects/suppoRTE/snli_old.py +# -*- coding: utf-8 -*-\n+\nimport json\n-from pprint import pprint\n-from time import time, sleep\nfrom os import path\n-from jtr.sisyphos.batch import get_feed_dicts\n-from jtr.sisyphos.vocab import Vocab, NeuralVocab\n-from jtr.sisyphos.map import tokenize, lower, deep_map, deep_seq_map\n-from jtr.sisyphos.models import conditional_reader_model\n-from jtr.sisyphos.train import train\n-from jtr.io.embeddings.embeddings import load_embeddings\nimport tensorflow as tf\n-import numpy as np\n-import random\n-from jtr.sisyphos.hooks import ExamplesPerSecHook, AccuracyHook, LossHook\n+from jtr.preprocess.batch import get_feed_dicts\n+from jtr.preprocess.vocab import Vocab, NeuralVocab\n+from jtr.preprocess.map import tokenize, lower, deep_map, deep_seq_map\n+from jtr.nn.models import conditional_reader_model\n+from jtr.train import train\n+from jtr.load.embeddings.embeddings import load_embeddings\n+from jtr.util.hooks import ExamplesPerSecHook, AccuracyHook, LossHook\n+from time import time\ntf.set_random_seed(1337)\n-from time import time\n-\nclass Duration(object):\ndef __init__(self):\nself.t0 = time()\nself.t = time()\n+\ndef __call__(self):\nprint('Time since last checkpoint : %.2fmin'%((time()-self.t)/60.))\nself.t = time()\n@@ -31,7 +29,6 @@ class Duration(object):\ncheckpoint = Duration()\n-\ndef load(path, max_count=None):\nseq1s = []\nseq2s = []\n"
    },
    {
        "org_diff": "diff --git a/.github/workflows/tests.yaml b/a/.github/workflows/tests.yaml @@ -9,9 +9,6 @@ on:\n       - '*.md'\n       - '*.rst'\n   pull_request:\n-    branches:\n-      - main\n-      - '*.x'\n     paths-ignore:\n       - 'docs/**'\n       - '*.md'\n@@ -24,17 +21,17 @@ jobs:\n       fail-fast: false\n       matrix:\n         include:\n-          - {name: Linux, python: '3.11', os: ubuntu-latest, tox: py311}\n-          - {name: Windows, python: '3.11', os: windows-latest, tox: py311}\n-          - {name: Mac, python: '3.11', os: macos-latest, tox: py311}\n-          - {name: '3.12-dev', python: '3.12-dev', os: ubuntu-latest, tox: py312}\n+          - {name: Linux, python: '3.12', os: ubuntu-latest, tox: py312}\n+          - {name: Windows, python: '3.12', os: windows-latest, tox: py312}\n+          - {name: Mac, python: '3.12', os: macos-latest, tox: py312}\n+          - {name: '3.11', python: '3.11', os: ubuntu-latest, tox: py311}\n           - {name: '3.10', python: '3.10', os: ubuntu-latest, tox: py310}\n           - {name: '3.9', python: '3.9', os: ubuntu-latest, tox: py39}\n           - {name: '3.8', python: '3.8', os: ubuntu-latest, tox: py38}\n           - {name: 'PyPy', python: 'pypy-3.10', os: ubuntu-latest, tox: pypy310}\n-          - {name: 'Minimum Versions', python: '3.11', os: ubuntu-latest, tox: py311-min}\n+          - {name: 'Minimum Versions', python: '3.12', os: ubuntu-latest, tox: py312-min}\n           - {name: 'Development Versions', python: '3.8', os: ubuntu-latest, tox: py38-dev}\n-          - {name: Typing, python: '3.11', os: ubuntu-latest, tox: typing}\n+          - {name: Typing, python: '3.12', os: ubuntu-latest, tox: typing}\n     steps:\n       - uses: actions/checkout@f43a0e5ff2bd294095638e18286ca9a3d1956744\n       - uses: actions/setup-python@61a6322f88396a6271a6ee3565807d608ecaddd1",
        "org_msg": "Update python versions in test workflow.",
        "sim_msg": "updates python versions in docs",
        "sim_diff": "diff --git a/docs/guide.rst b/docs/guide.rst @@ -157,8 +157,8 @@ This project template supports Python 3.7, 3.8, 3.9, and 3.10.\n$ pyenv install 3.7.12\n$ pyenv install 3.8.12\n- $ pyenv install 3.9.9\n- $ pyenv install 3.10.0\n+ $ pyenv install 3.9.10\n+ $ pyenv install 3.10.2\nAfter creating your project (see :ref:`below <Creating a project>`),\nyou can make these Python versions accessible in the project directory,\n@@ -166,7 +166,7 @@ using the following command:\n.. code:: console\n- $ pyenv local 3.10.0 3.9.9 3.8.12 3.7.12\n+ $ pyenv local 3.10.2 3.9.10 3.8.12 3.7.12\nThe first version listed is the one used when you type plain ``python``.\nEvery version can be used by invoking ``python<major.minor>``.\n"
    },
    {
        "org_diff": "diff --git a/.github/workflows/lock.yaml b/a/.github/workflows/lock.yaml @@ -1,8 +1,8 @@\n-name: 'Lock threads'\n-# Lock closed issues that have not received any further activity for\n-# two weeks. This does not close open issues, only humans may do that.\n-# We find that it is easier to respond to new issues with fresh examples\n-# rather than continuing discussions on old issues.\n+name: 'Lock inactive closed issues'\n+# Lock closed issues that have not received any further activity for two weeks.\n+# This does not close open issues, only humans may do that. We find that it is\n+# easier to respond to new issues with fresh examples rather than continuing\n+# discussions on old issues.\n \n on:\n   schedule:",
        "org_msg": "Update workflow to lock inactive closed issues for better issue management.",
        "sim_msg": "Release workflows fixes",
        "sim_diff": "diff --git a/Makefile b/Makefile export PYTHONPATH := .:$(CURDIR)/test\nPYTHON := python\n+KODI_PYTHON_ABIS := 3.0.0 2.25.0\nname = $(shell xmllint --xpath 'string(/addon/@id)' addon.xml)\nversion = $(shell xmllint --xpath 'string(/addon/@version)' addon.xml)\ngit_branch = $(shell git rev-parse --abbrev-ref HEAD)\ngit_hash = $(shell git rev-parse --short HEAD)\n+matrix = $(findstring $(shell xmllint --xpath 'string(/addon/requires/import[@addon=\"xbmc.python\"]/@version)' addon.xml), $(word 1,$(KODI_PYTHON_ABIS)))\n+ifdef release\n+ zip_name = $(name)-$(version).zip\n+else\nzip_name = $(name)-$(version)-$(git_branch)-$(git_hash).zip\n-include_files = addon.py addon.xml LICENSE.txt README.md resources/ service.py\n+endif\n+\n+include_files = addon.py addon.xml LICENSE.txt README.md resources/ service.py packages/\ninclude_paths = $(patsubst %,$(name)/%,$(include_files))\nexclude_files = \\*.new \\*.orig \\*.pyc \\*.pyo\nzip_dir = $(name)/\n@@ -83,6 +90,15 @@ build: clean\ncd ..; zip -r $(zip_name) $(include_paths) -x $(exclude_files)\n@echo -e \"$(white)=$(blue) Successfully wrote package as: $(white)../$(zip_name)$(reset)\"\n+multizip: clean\n+ @-$(foreach abi,$(KODI_PYTHON_ABIS), \\\n+ echo \"cd /addon/requires/import[@addon='xbmc.python']/@version\\nset $(abi)\\nsave\\nbye\" | xmllint --shell addon.xml; \\\n+ matrix=$(findstring $(abi), $(word 1,$(KODI_PYTHON_ABIS))); \\\n+ if [ $$matrix ]; then version=$(version)+matrix.1; else version=$(version); fi; \\\n+ echo \"cd /addon/@version\\nset $$version\\nsave\\nbye\" | xmllint --shell addon.xml; \\\n+ make build; \\\n+ )\n+\nclean:\n@echo -e \"$(white)=$(blue) Cleaning up$(reset)\"\nfind . -name '*.py[cod]' -type f -delete\n"
    },
    {
        "org_diff": "diff --git a/.readthedocs.yaml b/a/.readthedocs.yaml @@ -1,8 +1,8 @@\n version: 2\n build:\n-  os: ubuntu-20.04\n+  os: ubuntu-22.04\n   tools:\n-    python: \"3.10\"\n+    python: \"3.12\"\n python:\n   install:\n     - requirements: requirements/docs.txt",
        "org_msg": "Update .readthedocs.yaml for Ubuntu and Python versions",
        "sim_msg": "Update the .readthedocs.yml file",
        "sim_diff": "diff --git a/.readthedocs.yml b/.readthedocs.yml version: 2\n-# Location of sphinx configuration settings\n+# Build documentation in the docs/ directory with Sphinx\nsphinx:\nconfiguration: docs/conf.py\n-formats:\n- - pdf\n- - epub\n+formats: all\n+# Optionally set the version of Python and requirements required to build your docs\npython:\nversion: 3.5\ninstall:\n- requirements: docs/requirements.txt\n\\ No newline at end of file\n- - method: pip\n\\ No newline at end of file\n"
    },
    {
        "org_diff": "diff --git a/docs/deploying/asgi.rst b/a/docs/deploying/asgi.rst @@ -20,7 +20,7 @@ wrapping the Flask app,\n     asgi_app = WsgiToAsgi(app)\n \n and then serving the ``asgi_app`` with the ASGI server, e.g. using\n-`Hypercorn <https://gitlab.com/pgjones/hypercorn>`_,\n+`Hypercorn <https://github.com/pgjones/hypercorn>`_,\n \n .. sourcecode:: text\n ",
        "org_msg": "Update Hypercorn link to use Github instead of Gitlab.",
        "sim_msg": "changed the docs so that the examples are hyperlinks to github",
        "sim_diff": "diff --git a/docs/source/index.rst b/docs/source/index.rst @@ -28,11 +28,19 @@ The core APIs above are ordered following the machine learning pipeline rather t\n.. toctree::\n:maxdepth: 1\n- :caption: Examples API\n+ :caption: Example Projects\n- examples/examples.cifar_cnntransformer\n- examples/examples.cifar_isonet\n- examples/examples.digits_dann_lightn\n+ .. the two dots here mean that these lines are commented out.\n+ .. replaced the three doc generating lines below with links to the\n+ .. actual github pages of the examples.\n+\n+ .. examples/examples.cifar_cnntransformer\n+ .. examples/examples.cifar_isonet\n+ .. examples/examples.digits_dann_lightn\n+\n+ CIFAR - CNN Transformer <https://github.com/pykale/pykale/tree/master/examples/cifar_cnntransformer>\n+ CIFAR - ISONet <https://github.com/pykale/pykale/tree/master/examples/cifar_isonet>\n+ Digits - Domain Adaptation <https://github.com/pykale/pykale/tree/master/examples/digits_dann_lightn>\n.. toctree::\n:maxdepth: 1\n"
    },
    {
        "org_diff": "diff --git a/docs/deploying/asgi.rst b/a/docs/deploying/asgi.rst @@ -20,7 +20,7 @@ wrapping the Flask app,\n     asgi_app = WsgiToAsgi(app)\n \n and then serving the ``asgi_app`` with the ASGI server, e.g. using\n-`Hypercorn <https://gitlab.com/pgjones/hypercorn>`_,\n+`Hypercorn <https://github.com/pgjones/hypercorn>`_,\n \n .. sourcecode:: text\n ",
        "org_msg": "Update Hypercorn link to point to GitHub instead of GitLab",
        "sim_msg": "update links in readme to point to github pages",
        "sim_diff": "diff --git a/README.md b/README.md @@ -12,14 +12,14 @@ Multi-vendor library to simplify Paramiko SSH connections to network devices\n## Quick Links\n-- [Supported Platforms](#SupportedPlatforms)\n-- [Installation](#Installation)\n-- [Tutorials/Examples/Getting Started](#TutorialsExamplesGetting-Started)\n-- [Common Issues/FAQ](#Common-IssuesFAQ)\n-- [API-Documentation](#API-Documentation)\n-- [TextFSM Integration](#TextFSM-Integration)\n-- [Contributing](#Contributing)\n-- [Questions/Discussion](#QuestionsDiscussion)\n+- [Supported Platforms](https://ktbyers.github.io/netmiko/#supported-platforms)\n+- [Installation](https://ktbyers.github.io/netmiko/#installation)\n+- [Tutorials/Examples/Getting Started](https://ktbyers.github.io/netmiko/#tutorialsexamplesgetting-started)\n+- [Common Issues/FAQ](https://ktbyers.github.io/netmiko/#common-issuesfaq)\n+- [API-Documentation](https://ktbyers.github.io/netmiko/#api-documentation)\n+- [TextFSM Integration](https://ktbyers.github.io/netmiko/#textfsm-integration)\n+- [Contributing](https://ktbyers.github.io/netmiko/#contributing)\n+- [Questions/Discussion](https://ktbyers.github.io/netmiko/#questionsdiscussion)\n## Supported Platforms\n"
    },
    {
        "org_diff": "diff --git a/docs/testing.rst b/a/docs/testing.rst @@ -192,7 +192,7 @@ which records the request that produced that response.\n .. code-block:: python\n \n     def test_logout_redirect(client):\n-        response = client.get(\"/logout\")\n+        response = client.get(\"/logout\", follow_redirects=True)\n         # Check that there was one redirect response.\n         assert len(response.history) == 1\n         # Check that the second request was to the index page.",
        "org_msg": "Add follow_redirects parameter to client.get in test_logout_redirect function",
        "sim_msg": "Add follow redirects",
        "sim_diff": "diff --git a/augur/tasks/github/util/github_paginator.py b/augur/tasks/github/util/github_paginator.py @@ -27,7 +27,7 @@ def hit_api(key_manager, url: str, logger: logging.Logger, timeout: float = 10,\ntry:\nresponse = client.request(\n- method=method, url=url, auth=key_manager, timeout=timeout)\n+ method=method, url=url, auth=key_manager, timeout=timeout, follow_redirects=True)\nexcept TimeoutError:\nlogger.info(f\"Request timed out. Sleeping {round(timeout)} seconds and trying again...\\n\")\n"
    },
    {
        "org_diff": "diff --git a/docs/testing.rst b/a/docs/testing.rst @@ -192,7 +192,7 @@ which records the request that produced that response.\n .. code-block:: python\n \n     def test_logout_redirect(client):\n-        response = client.get(\"/logout\")\n+        response = client.get(\"/logout\", follow_redirects=True)\n         # Check that there was one redirect response.\n         assert len(response.history) == 1\n         # Check that the second request was to the index page.",
        "org_msg": "Add follow_redirects parameter to client.get(\"/logout\") in test_logout_redirect function",
        "sim_msg": "Add follow redirects",
        "sim_diff": "diff --git a/augur/tasks/github/util/github_paginator.py b/augur/tasks/github/util/github_paginator.py @@ -27,7 +27,7 @@ def hit_api(key_manager, url: str, logger: logging.Logger, timeout: float = 10,\ntry:\nresponse = client.request(\n- method=method, url=url, auth=key_manager, timeout=timeout)\n+ method=method, url=url, auth=key_manager, timeout=timeout, follow_redirects=True)\nexcept TimeoutError:\nlogger.info(f\"Request timed out. Sleeping {round(timeout)} seconds and trying again...\\n\")\n"
    },
    {
        "org_diff": "diff --git a/examples/javascript/README.rst b/a/examples/javascript/README.rst @@ -15,7 +15,7 @@ page. Demonstrates using |fetch|_, |XMLHttpRequest|_,  and\n .. |jQuery.ajax| replace:: ``jQuery.ajax``\n .. _jQuery.ajax: https://api.jquery.com/jQuery.ajax/\n \n-.. _Flask docs: https://flask.palletsprojects.com/patterns/jquery/\n+.. _Flask docs: https://flask.palletsprojects.com/patterns/javascript/\n \n \n Install",
        "org_msg": "Update Flask documentation link to point to patterns/javascript instead of patterns/jquery",
        "sim_msg": "API docs - changed flask.request description\n- the link should point to the flask's Request\n- longer description is unnecessary, as it's all explained above in flask.Request",
        "sim_diff": "diff --git a/docs/api.rst b/docs/api.rst @@ -41,9 +41,7 @@ Incoming Request Data\nThis is a proxy. See :ref:`notes-on-proxies` for more information.\n- The request object is an instance of a :class:`~werkzeug.wrappers.Request`\n- subclass and provides all of the attributes Werkzeug defines. This\n- just shows a quick overview of the most important ones.\n+ The request object is an instance of a :class:`~flask.Request`.\nResponse Objects\n"
    },
    {
        "org_diff": "diff --git a/docs/index.rst b/a/docs/index.rst @@ -6,7 +6,11 @@ Welcome to Flask\n .. image:: _static/flask-horizontal.png\n     :align: center\n \n-Welcome to Flask's documentation. Get started with :doc:`installation`\n+Welcome to Flask's documentation. Flask is a lightweight WSGI web application framework.\n+It is designed to make getting started quick and easy, with the ability to scale up to\n+complex applications.\n+\n+Get started with :doc:`installation`\n and then get an overview with the :doc:`quickstart`. There is also a\n more detailed :doc:`tutorial/index` that shows how to create a small but\n complete application with Flask. Common patterns are described in the",
        "org_msg": "Update introduction in index.rst to include information about Flask being a lightweight WSGI web application framework.",
        "sim_msg": "Mention Flask instead of Jinja in the issue template\nFor people not really comfortable with the flask ecosystem this can be really confusing.",
        "sim_diff": "diff --git a/.github/ISSUE_TEMPLATE.rst b/.github/ISSUE_TEMPLATE.rst **This issue tracker is a tool to address bugs in Flask itself.\nPlease use the #pocoo IRC channel on freenode or Stack Overflow for general\n-questions about using Jinja or issues not related to Jinja.**\n+questions about using Flask or issues not related to Flask.**\nIf you'd like to report a bug in Flask, fill out the template below. Provide\nany any extra information that may be useful / related to your problem.\n"
    },
    {
        "org_diff": "diff --git a/docs/patterns/javascript.rst b/a/docs/patterns/javascript.rst @@ -197,7 +197,7 @@ in the previous section. The following example shows how to replace a\n         const geology_div = getElementById(\"geology-fact\")\n         fetch(geology_url)\n             .then(response => response.text)\n-            .then(text => geology_div.innerHtml = text)\n+            .then(text => geology_div.innerHTML = text)\n     </script>\n \n ",
        "org_msg": "Fix typo in innerHTML assignment in JavaScript example",
        "sim_msg": "fix javascript code typo",
        "sim_diff": "diff --git a/user-guide/getting-started.md b/user-guide/getting-started.md @@ -24,7 +24,7 @@ kubectl -n ambassador wait --for condition=available --timeout=60s deploy -lprod\nfunction copy_to_clipboard(the_id) {\nvar copyText = document.getElementById(the_id).innerText;\nconst el = document.createElement('textarea'); // Create a <textarea> element\n- el.value = str; // Set its value to the string that you want copied\n+ el.value = copyText; // Set its value to the string that you want copied\nel.setAttribute('readonly', ''); // Make it readonly to be tamper-proof\nel.style.position = 'absolute';\nel.style.left = '-9999px'; // Move outside the screen to make it invisible\n"
    },
    {
        "org_diff": "diff --git a/.github/workflows/publish.yaml b/a/.github/workflows/publish.yaml @@ -63,10 +63,10 @@ jobs:\n     steps:\n       - uses: actions/download-artifact@9bc31d5ccc31df68ecc42ccf4149144866c47d8a\n       # Try uploading to Test PyPI first, in case something fails.\n-      - uses: pypa/gh-action-pypi-publish@f8c70e705ffc13c3b4d1221169b84f12a75d6ca8\n+      - uses: pypa/gh-action-pypi-publish@b7f401de30cb6434a1e19f805ff006643653240e\n         with:\n           repository-url: https://test.pypi.org/legacy/\n           packages-dir: artifact/\n-      - uses: pypa/gh-action-pypi-publish@f8c70e705ffc13c3b4d1221169b84f12a75d6ca8\n+      - uses: pypa/gh-action-pypi-publish@b7f401de30cb6434a1e19f805ff006643653240e\n         with:\n           packages-dir: artifact/",
        "org_msg": "Update pypa/gh-action-pypi-publish action to version b7f401de30cb6434a1e19f805ff006643653240e",
        "sim_msg": "Update pypi release action.",
        "sim_diff": "diff --git a/.github/workflows/pypi_upload.yml b/.github/workflows/pypi_upload.yml @@ -47,7 +47,7 @@ jobs:\n# to upload to test pypi, pass repository_url: https://test.pypi.org/legacy/ and use secrets.TEST_PYPI_TOKEN\n- name: Publish a Python distribution to PyPI\n- uses: pypa/gh-action-pypi-publish@master\n+ uses: pypa/gh-action-pypi-publish@release/v1\nwith:\nuser: __token__\npassword: ${{ secrets.PYPI_TOKEN }}\n"
    },
    {
        "org_diff": "diff --git a/.github/workflows/publish.yaml b/a/.github/workflows/publish.yaml @@ -33,7 +33,7 @@ jobs:\n       id-token: write\n       contents: write\n     # Can't pin with hash due to how this workflow works.\n-    uses: slsa-framework/slsa-github-generator/.github/workflows/generator_generic_slsa3.yml@v1.7.0\n+    uses: slsa-framework/slsa-github-generator/.github/workflows/generator_generic_slsa3.yml@v1.9.0\n     with:\n       base64-subjects: ${{ needs.build.outputs.hash }}\n   create-release:",
        "org_msg": "Update slsa-github-generator to v1.9.0 in publish.yaml workflow",
        "sim_msg": "Update .github/workflows/CI_publish_pypi.yml",
        "sim_diff": "diff --git a/.github/workflows/CI_publish_pypi.yml b/.github/workflows/CI_publish_pypi.yml @@ -17,7 +17,7 @@ jobs:\nrun: |\npython -m pip install --upgrade pip\npip install setuptools wheel twine\n- python setup.py bdist_wheel\n+ python setup.py sdist bdist_wheel\n- name: Publish package\nuses: pypa/gh-action-pypi-publish@master\nwith:\n"
    },
    {
        "org_diff": "diff --git a/.github/workflows/publish.yaml b/a/.github/workflows/publish.yaml @@ -63,10 +63,10 @@ jobs:\n     steps:\n       - uses: actions/download-artifact@9bc31d5ccc31df68ecc42ccf4149144866c47d8a\n       # Try uploading to Test PyPI first, in case something fails.\n-      - uses: pypa/gh-action-pypi-publish@f8c70e705ffc13c3b4d1221169b84f12a75d6ca8\n+      - uses: pypa/gh-action-pypi-publish@b7f401de30cb6434a1e19f805ff006643653240e\n         with:\n           repository-url: https://test.pypi.org/legacy/\n           packages-dir: artifact/\n-      - uses: pypa/gh-action-pypi-publish@f8c70e705ffc13c3b4d1221169b84f12a75d6ca8\n+      - uses: pypa/gh-action-pypi-publish@b7f401de30cb6434a1e19f805ff006643653240e\n         with:\n           packages-dir: artifact/",
        "org_msg": "Update pypa/gh-action-pypi-publish to b7f401de30cb6434a1e19f805ff006643653240e",
        "sim_msg": "Update pypi release action.",
        "sim_diff": "diff --git a/.github/workflows/pypi_upload.yml b/.github/workflows/pypi_upload.yml @@ -47,7 +47,7 @@ jobs:\n# to upload to test pypi, pass repository_url: https://test.pypi.org/legacy/ and use secrets.TEST_PYPI_TOKEN\n- name: Publish a Python distribution to PyPI\n- uses: pypa/gh-action-pypi-publish@master\n+ uses: pypa/gh-action-pypi-publish@release/v1\nwith:\nuser: __token__\npassword: ${{ secrets.PYPI_TOKEN }}\n"
    },
    {
        "org_diff": "diff --git a/.github/workflows/publish.yaml b/a/.github/workflows/publish.yaml @@ -33,7 +33,7 @@ jobs:\n       id-token: write\n       contents: write\n     # Can't pin with hash due to how this workflow works.\n-    uses: slsa-framework/slsa-github-generator/.github/workflows/generator_generic_slsa3.yml@v1.7.0\n+    uses: slsa-framework/slsa-github-generator/.github/workflows/generator_generic_slsa3.yml@v1.9.0\n     with:\n       base64-subjects: ${{ needs.build.outputs.hash }}\n   create-release:",
        "org_msg": "Update slsa-github-generator to v1.9.0 in publish.yaml workflow",
        "sim_msg": "Update .github/workflows/CI_publish_pypi.yml",
        "sim_diff": "diff --git a/.github/workflows/CI_publish_pypi.yml b/.github/workflows/CI_publish_pypi.yml @@ -17,7 +17,7 @@ jobs:\nrun: |\npython -m pip install --upgrade pip\npip install setuptools wheel twine\n- python setup.py bdist_wheel\n+ python setup.py sdist bdist_wheel\n- name: Publish package\nuses: pypa/gh-action-pypi-publish@master\nwith:\n"
    },
    {
        "org_diff": "diff --git a/CHANGES.rst b/a/CHANGES.rst @@ -11,7 +11,7 @@ Unreleased\n Version 2.3.3\n -------------\n \n-Unreleased\n+Released 2023-08-21\n \n -   Python 3.12 compatibility.\n -   Require Werkzeug >= 2.3.7.",
        "org_msg": "\"Released version 2.3.3 on 2023-08-21 with Python 3.12 compatibility and requirement of Werkzeug >= 2.3.7.\"",
        "sim_msg": "docs: python 3.3, 3.4 -> 3.5. py2.7 supports ends on March 1, 2021",
        "sim_diff": "diff --git a/src/python/Readme.md b/src/python/Readme.md @@ -57,15 +57,15 @@ Other useful resources:\nPython version compatibility\n----------------------------\n-Code going into the Python codebase should be written in Python 3.3 style, and should be compatible with Python 3.3, 3.4,\n-and 2.7. To facilitate Python 2 compatibility, we have the compat module in https://github.com/dnanexus/dx-toolkit/blob/master/src/python/dxpy/compat.py. Also, the following boilerplate should be\n+Code going into the Python codebase should be written in Python 3.5 style, and should be compatible with Python 2.7. Python 2.7 support will end on March 1, 2021.\n+To facilitate Python 2 compatibility, we have the compat module in https://github.com/dnanexus/dx-toolkit/blob/master/src/python/dxpy/compat.py. Also, the following boilerplate should be\ninserted into all Python source files:\n```\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n```\n-- `dxpy.compat` has some simple shims that mirror Python 3.3 builtins and redirect them to Python 2.7 equivalents when on 2.7. Most critically, `from dxpy.compat import str` will import the `unicode` builtin on 2.7 and the `str` builtin on 3.3. Use `str` wherever you would have used `unicode`. To convert unicode strings to bytes, use `.encode('utf-8')`.\n+- `dxpy.compat` has some simple shims that mirror Python 3 builtins and redirect them to Python 2.7 equivalents when on 2.7. Most critically, `from dxpy.compat import str` will import the `unicode` builtin on 2.7 and the `str` builtin on python 3. Use `str` wherever you would have used `unicode`. To convert unicode strings to bytes, use `.encode('utf-8')`.\n- Use `from __future__ import print_function` and use print as a function. Instead of `print >>sys.stderr`, write `print(..., file=sys.stderr)`.\n- The next most troublesome gotcha after the bytes/unicode conversions is that many iterables operators return generators in Python 3. For example, `map()` returns a generator. This breaks places that expect a list, and requires either explicit casting with `list()`, or the use of list comprehensions (usually preferred).\n- Instead of `raw_input`, use `from dxpy.compat import input`.\n"
    },
    {
        "org_diff": "diff --git a/src/flask/app.py b/a/src/flask/app.py @@ -409,7 +409,6 @@ def create_url_adapter(self, request: Request | None) -> MapAdapter | None:\n             else:\n                 subdomain = None\n \n-            print(self.config[\"SERVER_NAME\"], subdomain)\n             return self.url_map.bind_to_environ(\n                 request.environ,\n                 server_name=self.config[\"SERVER_NAME\"],",
        "org_msg": "Remove print statement in create_url_adapter method",
        "sim_msg": "STY: remove print statement",
        "sim_diff": "diff --git a/pysat/tests/test_instruments.py b/pysat/tests/test_instruments.py @@ -58,10 +58,6 @@ def generate_instrument_list(exclude_list, exclude_tags):\nif name not in exclude_list:\ntemp.append(name)\ninstrument_names = temp\n-\n- print('The following instrument modules will be tested : ',\n- instrument_names)\n-\ninstruments = []\ninstrument_modules = []\n"
    },
    {
        "org_diff": "diff --git a/CHANGES.rst b/a/CHANGES.rst @@ -4,6 +4,8 @@ Version 3.0.0\n Unreleased\n \n -   Remove previously deprecated code. :pr:`5223`\n+-   Restructure the code such that the Flask (app) and Blueprint\n+    classes have Sans-IO bases. :pr:`5127`\n \n \n Version 2.3.3",
        "org_msg": "Restructure code to have Sans-IO bases for Flask (app) and Blueprint classes. :pr:`5127`",
        "sim_msg": "fixed typo in src/flask/blueprints.py",
        "sim_diff": "diff --git a/src/flask/blueprints.py b/src/flask/blueprints.py @@ -121,7 +121,7 @@ class Blueprint(Scaffold):\ndefault.\n:param url_defaults: A dict of default values that blueprint routes\nwill receive by default.\n- :param root_path: By default, the blueprint will automatically this\n+ :param root_path: By default, the blueprint will automatically set this\nbased on ``import_name``. In certain situations this automatic\ndetection can fail, so the path can be specified manually\ninstead.\n"
    },
    {
        "org_diff": "diff --git a/src/flask/typing.py b/a/src/flask/typing.py @@ -77,7 +77,10 @@\n # https://github.com/pallets/flask/issues/4095\n # https://github.com/pallets/flask/issues/4295\n # https://github.com/pallets/flask/issues/4297\n-ErrorHandlerCallable = t.Callable[[t.Any], ResponseReturnValue]\n+ErrorHandlerCallable = t.Union[\n+    t.Callable[[t.Any], ResponseReturnValue],\n+    t.Callable[[t.Any], t.Awaitable[ResponseReturnValue]],\n+]\n \n RouteCallable = t.Union[\n     t.Callable[..., ResponseReturnValue],",
        "org_msg": "Refactor ErrorHandlerCallable and RouteCallable to allow for asynchronous error handling and route handling.",
        "sim_msg": "Refactor errorhandler.py",
        "sim_diff": "diff --git a/gaphor/ui/errorhandler.py b/gaphor/ui/errorhandler.py @@ -27,11 +27,10 @@ def error_handler(message, secondary_message=\"\", window=None):\ndialog.props.secondary_text = (\"\\n\\n\" if secondary_message else \"\") + gettext(\n\"It looks like Gaphor is started from the command line.\\nDo you want to open a debug session?\"\n)\n- dialog.add_button(gettext(\"Close\"), 0)\n- dialog.add_button(gettext(\"Start debug session\"), 100)\n+ dialog.add_buttons(Gtk.STOCK_CLOSE, 0, gettext(\"Start debug session\"), 100)\nelse:\ndialog.props.secondary_text = secondary_message\n- dialog.add_button(gettext(\"OK\"), 0)\n+ dialog.add_button(Gtk.STOCK_OK, 0)\ndialog.set_transient_for(window)\nanswer = dialog.run()\n"
    },
    {
        "org_diff": "diff --git a/src/flask/typing.py b/a/src/flask/typing.py @@ -5,7 +5,7 @@\n if t.TYPE_CHECKING:  # pragma: no cover\n     from _typeshed.wsgi import WSGIApplication  # noqa: F401\n     from werkzeug.datastructures import Headers  # noqa: F401\n-    from werkzeug.wrappers import Response  # noqa: F401\n+    from werkzeug.sansio.response import Response  # noqa: F401\n \n # The possible types that are directly convertible or are a Response object.\n ResponseValue = t.Union[",
        "org_msg": "\"Update werkzeug.wrappers import to werkzeug.sansio.response\"",
        "sim_msg": "BUG: update imports",
        "sim_diff": "diff --git a/pysat/tests/test_instrument.py b/pysat/tests/test_instrument.py @@ -12,7 +12,7 @@ import pysat\nimport pysat.instruments.pysat_testing\nimport pysat.instruments.pysat_testing_xarray\nimport pysat.instruments.pysat_testing2d\n-from pysat.tests.instrument_test_class import generate_instrument_list\n+from pysat.utils import generate_instrument_list\nxarray_epoch_name = 'time'\n"
    },
    {
        "org_diff": "diff --git a/docs/patterns/appdispatch.rst b/a/docs/patterns/appdispatch.rst @@ -146,7 +146,7 @@ the ``Host`` header to figure out the subdomain one simply looks at the\n request path up to the first slash::\n \n     from threading import Lock\n-    from werkzeug.wsgi import pop_path_info, peek_path_info\n+    from wsgiref.util import shift_path_info\n \n     class PathDispatcher:\n \n@@ -166,13 +166,20 @@ request path up to the first slash::\n                 return app\n \n         def __call__(self, environ, start_response):\n-            app = self.get_application(peek_path_info(environ))\n+            app = self.get_application(self._peek_path_info(environ))\n             if app is not None:\n-                pop_path_info(environ)\n+                shift_path_info(environ)\n             else:\n                 app = self.default_app\n             return app(environ, start_response)\n \n+    def _peek_path_info(environ):\n+        segments = environ.get(\"PATH_INFO\", \"\").lstrip(\"/\").split(\"/\", 1)\n+        if segments:\n+            return segments[0]\n+\n+        return None\n+\n The big difference between this and the subdomain one is that this one\n falls back to another application if the creator function returns ``None``::\n ",
        "org_msg": "Refactor path handling in appdispatch.rst to use wsgiref.util.shift_path_info instead of werkzeug.wsgi.pop_path_info and peek_path_info. Added new _peek_path_info function for improved path handling.",
        "sim_msg": "Implements info paths in model_info utility.",
        "sim_diff": "diff --git a/revscoring/utilities/model_info.py b/revscoring/utilities/model_info.py Usage:\nmodule_info -h | --help\n- module_info <model-file> [--as-json]\n+ module_info <model-file> <path>... --formatting=<type>\nOptions:\n-h --help Prints this documentation\n<model-file> Path to a model file\n- --as-json Output model info as a JSON blob\n+ <path> A model information path. If no path is provided,\n+ all default fields will be in the output.\n+ --formatting=<type> What format to output the information? \"str\" or\n+ \"json\" [default: str]\n\"\"\"\nimport json\n@@ -23,14 +26,16 @@ from ..scoring import Model\ndef main(argv=None):\nargs = docopt.docopt(__doc__, argv=argv)\n- scorer_model = Model.load(open(args['<model-file>'], 'rb'))\n- as_json = args['--as-json']\n+ model = Model.load(open(args['<model-file>'], 'rb'))\n+ paths = args['path']\n+ formatting = args['--formatting']\n- run(scorer_model, as_json)\n+ run(model, paths, formatting)\n-def run(scorer_model, as_json):\n- if not as_json:\n- print(scorer_model.format())\n- else:\n- print(json.dumps(scorer_model.format(formatting=\"json\")))\n+def run(model, paths, formatting):\n+ formatted = model.info.format(paths, formatting=formatting)\n+ if formatting == \"json\":\n+ formatted = json.dumps(formatted, indent=2)\n+\n+ print(formatted)\n"
    },
    {
        "org_diff": "diff --git a/tests/test_basic.py b/a/tests/test_basic.py @@ -431,9 +431,9 @@ def dump_session_contents():\n         client.get(\"/\")\n         s = flask.session\n         assert s[\"t\"] == (1, 2, 3)\n-        assert type(s[\"b\"]) == bytes\n+        assert type(s[\"b\"]) is bytes\n         assert s[\"b\"] == b\"\\xff\"\n-        assert type(s[\"m\"]) == Markup\n+        assert type(s[\"m\"]) is Markup\n         assert s[\"m\"] == Markup(\"<html>\")\n         assert s[\"u\"] == the_uuid\n         assert s[\"d\"] == now\n@@ -760,7 +760,7 @@ def test_teardown_request_handler_error(app, client):\n \n     @app.teardown_request\n     def teardown_request1(exc):\n-        assert type(exc) == ZeroDivisionError\n+        assert type(exc) is ZeroDivisionError\n         called.append(True)\n         # This raises a new error and blows away sys.exc_info(), so we can\n         # test that all teardown_requests get passed the same original\n@@ -772,7 +772,7 @@ def teardown_request1(exc):\n \n     @app.teardown_request\n     def teardown_request2(exc):\n-        assert type(exc) == ZeroDivisionError\n+        assert type(exc) is ZeroDivisionError\n         called.append(True)\n         # This raises a new error and blows away sys.exc_info(), so we can\n         # test that all teardown_requests get passed the same original",
        "org_msg": "Refactor type assertions to use \"is\" instead of \"==\" in test_basic.py",
        "sim_msg": "refactor tests/test_basic_ops_list.py:test_append_invalid_element_type",
        "sim_diff": "diff --git a/tests/test_basic_ops_list.py b/tests/test_basic_ops_list.py @@ -326,69 +326,42 @@ def test_list_append() -> None:\n@mark.parametrize(\n- \"lc,element,expected\",\n+ \"lc,element,err\",\n[\nparam(\nListConfig(content=[], element_type=int),\n\"foo\",\n- raises(\n- ValidationError,\n- match=re.escape(\n- \"Value 'foo' of type 'str' could not be converted to Integer\"\n- ),\n- ),\n+ \"Value 'foo' of type 'str' could not be converted to Integer\",\nid=\"append_str_to_list[int]\",\n),\nparam(\nListConfig(content=[], element_type=Color),\n\"foo\",\n- raises(\n- ValidationError,\n- match=re.escape(\n- \"Invalid value 'foo', expected one of [RED, GREEN, BLUE]\"\n- ),\n- ),\n+ \"Invalid value 'foo', expected one of [RED, GREEN, BLUE]\",\nid=\"append_str_to_list[Color]\",\n),\nparam(\nListConfig(content=[], element_type=User),\n\"foo\",\n- raises(\n- ValidationError,\n- match=re.escape(\n- \"Invalid type assigned: str is not a subclass of User. value: foo\"\n- ),\n- ),\n+ \"Invalid type assigned: str is not a subclass of User. value: foo\",\nid=\"append_str_to_list[User]\",\n),\nparam(\nListConfig(content=[], element_type=User),\n{\"name\": \"Bond\", \"age\": 7},\n- raises(\n- ValidationError,\n- match=re.escape(\n- \"Invalid type assigned: dict is not a subclass of User. value: {'name': 'Bond', 'age': 7}\"\n- ),\n- ),\n+ \"Invalid type assigned: dict is not a subclass of User. value: {'name': 'Bond', 'age': 7}\",\nid=\"list:convert_dict_to_user\",\n),\nparam(\nListConfig(content=[], element_type=User),\n{},\n- raises(\n- ValidationError,\n- match=re.escape(\n- \"Invalid type assigned: dict is not a subclass of User. value: {}\"\n- ),\n- ),\n+ \"Invalid type assigned: dict is not a subclass of User. value: {}\",\nid=\"list:convert_empty_dict_to_user\",\n),\n],\n)\n-def test_append_invalid_element_type(\n- lc: ListConfig, element: Any, expected: Any\n-) -> None:\n- with expected:\n+def test_append_invalid_element_type(lc: ListConfig, element: Any, err: Any) -> None:\n+ with raises(ValidationError, match=re.escape(err)):\nlc.append(element)\n"
    },
    {
        "org_diff": "diff --git a/tests/test_basic.py b/a/tests/test_basic.py @@ -431,9 +431,9 @@ def dump_session_contents():\n         client.get(\"/\")\n         s = flask.session\n         assert s[\"t\"] == (1, 2, 3)\n-        assert type(s[\"b\"]) == bytes\n+        assert type(s[\"b\"]) is bytes\n         assert s[\"b\"] == b\"\\xff\"\n-        assert type(s[\"m\"]) == Markup\n+        assert type(s[\"m\"]) is Markup\n         assert s[\"m\"] == Markup(\"<html>\")\n         assert s[\"u\"] == the_uuid\n         assert s[\"d\"] == now\n@@ -760,7 +760,7 @@ def test_teardown_request_handler_error(app, client):\n \n     @app.teardown_request\n     def teardown_request1(exc):\n-        assert type(exc) == ZeroDivisionError\n+        assert type(exc) is ZeroDivisionError\n         called.append(True)\n         # This raises a new error and blows away sys.exc_info(), so we can\n         # test that all teardown_requests get passed the same original\n@@ -772,7 +772,7 @@ def teardown_request1(exc):\n \n     @app.teardown_request\n     def teardown_request2(exc):\n-        assert type(exc) == ZeroDivisionError\n+        assert type(exc) is ZeroDivisionError\n         called.append(True)\n         # This raises a new error and blows away sys.exc_info(), so we can\n         # test that all teardown_requests get passed the same original",
        "org_msg": "Refactor type assertion to use 'is' keyword instead of '==' in test_basic.py",
        "sim_msg": "refactor tests/test_basic_ops_list.py:test_append_invalid_element_type",
        "sim_diff": "diff --git a/tests/test_basic_ops_list.py b/tests/test_basic_ops_list.py @@ -326,69 +326,42 @@ def test_list_append() -> None:\n@mark.parametrize(\n- \"lc,element,expected\",\n+ \"lc,element,err\",\n[\nparam(\nListConfig(content=[], element_type=int),\n\"foo\",\n- raises(\n- ValidationError,\n- match=re.escape(\n- \"Value 'foo' of type 'str' could not be converted to Integer\"\n- ),\n- ),\n+ \"Value 'foo' of type 'str' could not be converted to Integer\",\nid=\"append_str_to_list[int]\",\n),\nparam(\nListConfig(content=[], element_type=Color),\n\"foo\",\n- raises(\n- ValidationError,\n- match=re.escape(\n- \"Invalid value 'foo', expected one of [RED, GREEN, BLUE]\"\n- ),\n- ),\n+ \"Invalid value 'foo', expected one of [RED, GREEN, BLUE]\",\nid=\"append_str_to_list[Color]\",\n),\nparam(\nListConfig(content=[], element_type=User),\n\"foo\",\n- raises(\n- ValidationError,\n- match=re.escape(\n- \"Invalid type assigned: str is not a subclass of User. value: foo\"\n- ),\n- ),\n+ \"Invalid type assigned: str is not a subclass of User. value: foo\",\nid=\"append_str_to_list[User]\",\n),\nparam(\nListConfig(content=[], element_type=User),\n{\"name\": \"Bond\", \"age\": 7},\n- raises(\n- ValidationError,\n- match=re.escape(\n- \"Invalid type assigned: dict is not a subclass of User. value: {'name': 'Bond', 'age': 7}\"\n- ),\n- ),\n+ \"Invalid type assigned: dict is not a subclass of User. value: {'name': 'Bond', 'age': 7}\",\nid=\"list:convert_dict_to_user\",\n),\nparam(\nListConfig(content=[], element_type=User),\n{},\n- raises(\n- ValidationError,\n- match=re.escape(\n- \"Invalid type assigned: dict is not a subclass of User. value: {}\"\n- ),\n- ),\n+ \"Invalid type assigned: dict is not a subclass of User. value: {}\",\nid=\"list:convert_empty_dict_to_user\",\n),\n],\n)\n-def test_append_invalid_element_type(\n- lc: ListConfig, element: Any, expected: Any\n-) -> None:\n- with expected:\n+def test_append_invalid_element_type(lc: ListConfig, element: Any, err: Any) -> None:\n+ with raises(ValidationError, match=re.escape(err)):\nlc.append(element)\n"
    },
    {
        "org_diff": "diff --git a/src/flask/cli.py b/a/src/flask/cli.py @@ -302,7 +302,7 @@ def load_app(self) -> Flask:\n         else:\n             if self.app_import_path:\n                 path, name = (\n-                    re.split(r\":(?![\\\\/])\", self.app_import_path, 1) + [None]\n+                    re.split(r\":(?![\\\\/])\", self.app_import_path, maxsplit=1) + [None]\n                 )[:2]\n                 import_name = prepare_import(path)\n                 app = locate_app(import_name, name)",
        "org_msg": "Fix maxsplit parameter in app_import_path split function",
        "sim_msg": "Added maxsplit parameter",
        "sim_diff": "diff --git a/src/astral/__init__.py b/src/astral/__init__.py @@ -317,4 +317,5 @@ class LocationInfo:\n@property\ndef timezone_group(self):\n- return self.timezone.split(\"/\")[0]\n+ \"\"\"Return the group a timezone is in\"\"\"\n+ return self.timezone.split(\"/\", maxsplit=1)[0]\n"
    },
    {
        "org_diff": "diff --git a/src/flask/cli.py b/a/src/flask/cli.py @@ -302,7 +302,7 @@ def load_app(self) -> Flask:\n         else:\n             if self.app_import_path:\n                 path, name = (\n-                    re.split(r\":(?![\\\\/])\", self.app_import_path, 1) + [None]\n+                    re.split(r\":(?![\\\\/])\", self.app_import_path, maxsplit=1) + [None]\n                 )[:2]\n                 import_name = prepare_import(path)\n                 app = locate_app(import_name, name)",
        "org_msg": "Fixing maxsplit parameter in app_import_path split function",
        "sim_msg": "fixing import string",
        "sim_diff": "diff --git a/kg_emerging_viruses/transform.py b/kg_emerging_viruses/transform.py #!/usr/bin/env python\n# -*- coding: utf-8 -*-\n+\nfrom kg_emerging_viruses.transform_utils import zhou_transform\n-from kg_emerging_viruses.transform_utils.drug_central.drug_central import \\\n- DrugCentralTransform\n+from kg_emerging_viruses.transform_utils.drug_central.drug_central import DrugCentralTransform\ndef transform(input_dir: str, output_dir: str) -> None:\n"
    },
    {
        "org_diff": "diff --git a/src/flask/cli.py b/a/src/flask/cli.py @@ -302,7 +302,7 @@ def load_app(self) -> Flask:\n         else:\n             if self.app_import_path:\n                 path, name = (\n-                    re.split(r\":(?![\\\\/])\", self.app_import_path, 1) + [None]\n+                    re.split(r\":(?![\\\\/])\", self.app_import_path, maxsplit=1) + [None]\n                 )[:2]\n                 import_name = prepare_import(path)\n                 app = locate_app(import_name, name)",
        "org_msg": "Fix maxsplit parameter in app_import_path split function",
        "sim_msg": "Added maxsplit parameter",
        "sim_diff": "diff --git a/src/astral/__init__.py b/src/astral/__init__.py @@ -317,4 +317,5 @@ class LocationInfo:\n@property\ndef timezone_group(self):\n- return self.timezone.split(\"/\")[0]\n+ \"\"\"Return the group a timezone is in\"\"\"\n+ return self.timezone.split(\"/\", maxsplit=1)[0]\n"
    },
    {
        "org_diff": "diff --git a/.github/workflows/publish.yaml b/a/.github/workflows/publish.yaml @@ -63,10 +63,10 @@ jobs:\n     steps:\n       - uses: actions/download-artifact@9bc31d5ccc31df68ecc42ccf4149144866c47d8a\n       # Try uploading to Test PyPI first, in case something fails.\n-      - uses: pypa/gh-action-pypi-publish@f5622bde02b04381239da3573277701ceca8f6a0\n+      - uses: pypa/gh-action-pypi-publish@f8c70e705ffc13c3b4d1221169b84f12a75d6ca8\n         with:\n           repository-url: https://test.pypi.org/legacy/\n           packages-dir: artifact/\n-      - uses: pypa/gh-action-pypi-publish@f5622bde02b04381239da3573277701ceca8f6a0\n+      - uses: pypa/gh-action-pypi-publish@f8c70e705ffc13c3b4d1221169b84f12a75d6ca8\n         with:\n           packages-dir: artifact/",
        "org_msg": "Update PyPI publish action to use latest version.",
        "sim_msg": "Update pypi release action.",
        "sim_diff": "diff --git a/.github/workflows/pypi_upload.yml b/.github/workflows/pypi_upload.yml @@ -47,7 +47,7 @@ jobs:\n# to upload to test pypi, pass repository_url: https://test.pypi.org/legacy/ and use secrets.TEST_PYPI_TOKEN\n- name: Publish a Python distribution to PyPI\n- uses: pypa/gh-action-pypi-publish@master\n+ uses: pypa/gh-action-pypi-publish@release/v1\nwith:\nuser: __token__\npassword: ${{ secrets.PYPI_TOKEN }}\n"
    },
    {
        "org_diff": "diff --git a/.github/workflows/publish.yaml b/a/.github/workflows/publish.yaml @@ -63,10 +63,10 @@ jobs:\n     steps:\n       - uses: actions/download-artifact@9bc31d5ccc31df68ecc42ccf4149144866c47d8a\n       # Try uploading to Test PyPI first, in case something fails.\n-      - uses: pypa/gh-action-pypi-publish@f5622bde02b04381239da3573277701ceca8f6a0\n+      - uses: pypa/gh-action-pypi-publish@f8c70e705ffc13c3b4d1221169b84f12a75d6ca8\n         with:\n           repository-url: https://test.pypi.org/legacy/\n           packages-dir: artifact/\n-      - uses: pypa/gh-action-pypi-publish@f5622bde02b04381239da3573277701ceca8f6a0\n+      - uses: pypa/gh-action-pypi-publish@f8c70e705ffc13c3b4d1221169b84f12a75d6ca8\n         with:\n           packages-dir: artifact/",
        "org_msg": "Update PyPI publish action to use latest version.",
        "sim_msg": "Update pypi release action.",
        "sim_diff": "diff --git a/.github/workflows/pypi_upload.yml b/.github/workflows/pypi_upload.yml @@ -47,7 +47,7 @@ jobs:\n# to upload to test pypi, pass repository_url: https://test.pypi.org/legacy/ and use secrets.TEST_PYPI_TOKEN\n- name: Publish a Python distribution to PyPI\n- uses: pypa/gh-action-pypi-publish@master\n+ uses: pypa/gh-action-pypi-publish@release/v1\nwith:\nuser: __token__\npassword: ${{ secrets.PYPI_TOKEN }}\n"
    },
    {
        "org_diff": "diff --git a/.pre-commit-config.yaml b/a/.pre-commit-config.yaml @@ -3,7 +3,7 @@ ci:\n   autoupdate_schedule: monthly\n repos:\n   - repo: https://github.com/asottile/pyupgrade\n-    rev: v3.8.0\n+    rev: v3.10.1\n     hooks:\n       - id: pyupgrade\n         args: [\"--py38-plus\"]\n@@ -15,11 +15,11 @@ repos:\n         files: \"^(?!examples/)\"\n         args: [\"--application-directories\", \"src\"]\n   - repo: https://github.com/psf/black\n-    rev: 23.3.0\n+    rev: 23.7.0\n     hooks:\n       - id: black\n   - repo: https://github.com/PyCQA/flake8\n-    rev: 6.0.0\n+    rev: 6.1.0\n     hooks:\n       - id: flake8\n         additional_dependencies:",
        "org_msg": "Update pre-commit hooks to latest versions of pyupgrade, black, and flake8. Updated pyupgrade to v3.10.1, black to v23.7.0, and flake8 to v6.1.0.",
        "sim_msg": "Update pre-commit config: mypy, pyupgrade",
        "sim_diff": "diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml @@ -5,7 +5,7 @@ repos:\n- id: black\nlanguage_version: python3\n- repo: https://github.com/pre-commit/mirrors-mypy\n- rev: v0.931\n+ rev: v0.941\nhooks:\n- id: mypy\nadditional_dependencies:\n@@ -36,6 +36,6 @@ repos:\n- id: shellcheck\nargs: [--exclude, SC1017]\n- repo: https://github.com/asottile/pyupgrade\n- rev: v2.31.0\n+ rev: v2.31.1\nhooks:\n- id: pyupgrade\n"
    },
    {
        "org_diff": "diff --git a/.pre-commit-config.yaml b/a/.pre-commit-config.yaml @@ -3,7 +3,7 @@ ci:\n   autoupdate_schedule: monthly\n repos:\n   - repo: https://github.com/asottile/pyupgrade\n-    rev: v3.8.0\n+    rev: v3.10.1\n     hooks:\n       - id: pyupgrade\n         args: [\"--py38-plus\"]\n@@ -15,11 +15,11 @@ repos:\n         files: \"^(?!examples/)\"\n         args: [\"--application-directories\", \"src\"]\n   - repo: https://github.com/psf/black\n-    rev: 23.3.0\n+    rev: 23.7.0\n     hooks:\n       - id: black\n   - repo: https://github.com/PyCQA/flake8\n-    rev: 6.0.0\n+    rev: 6.1.0\n     hooks:\n       - id: flake8\n         additional_dependencies:",
        "org_msg": "Update pre-commit hooks to latest versions of pyupgrade, black, and flake8.",
        "sim_msg": "Update .pre-commit-config.yaml to newest revisions of hooks.",
        "sim_diff": "diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml @@ -16,11 +16,11 @@ repos:\n- id: check-merge-conflict\n- id: debug-statements\n- repo: https://github.com/timothycrosley/isort\n- rev: 5.9.2\n+ rev: 5.9.3\nhooks:\n- id: isort\n- repo: https://github.com/asottile/pyupgrade\n- rev: v2.21.0\n+ rev: v2.23.3\nhooks:\n- id: pyupgrade\nargs: [--py36-plus]\n@@ -34,7 +34,7 @@ repos:\n- id: rst-directive-colons\n- id: rst-inline-touching-normal\n- repo: https://github.com/psf/black\n- rev: 21.6b0\n+ rev: 21.7b0\nhooks:\n- id: black\nargs: [--safe, --quiet, --target-version=py36]\n"
    },
    {
        "org_diff": "diff --git a/.pre-commit-config.yaml b/a/.pre-commit-config.yaml @@ -3,7 +3,7 @@ ci:\n   autoupdate_schedule: monthly\n repos:\n   - repo: https://github.com/asottile/pyupgrade\n-    rev: v3.7.0\n+    rev: v3.8.0\n     hooks:\n       - id: pyupgrade\n         args: [\"--py38-plus\"]",
        "org_msg": "Update pyupgrade to version v3.8.0 in .pre-commit-config.yaml",
        "sim_msg": "Update pyupgrade version to avoid ImportError for Python 3.10.8",
        "sim_diff": "diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml @@ -7,7 +7,7 @@ repos:\n- id: end-of-file-fixer\n- id: requirements-txt-fixer\n- repo: https://github.com/asottile/pyupgrade\n- rev: v2.23.3\n+ rev: v3.1.0\nhooks:\n- id: pyupgrade\nargs: [\"--py37-plus\"]\n"
    },
    {
        "org_diff": "diff --git a/.pre-commit-config.yaml b/a/.pre-commit-config.yaml @@ -3,7 +3,7 @@ ci:\n   autoupdate_schedule: monthly\n repos:\n   - repo: https://github.com/asottile/pyupgrade\n-    rev: v3.7.0\n+    rev: v3.8.0\n     hooks:\n       - id: pyupgrade\n         args: [\"--py38-plus\"]",
        "org_msg": "Update pyupgrade to version v3.8.0",
        "sim_msg": "Update pyupgrade version to avoid ImportError for Python 3.10.8",
        "sim_diff": "diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml @@ -7,7 +7,7 @@ repos:\n- id: end-of-file-fixer\n- id: requirements-txt-fixer\n- repo: https://github.com/asottile/pyupgrade\n- rev: v2.23.3\n+ rev: v3.1.0\nhooks:\n- id: pyupgrade\nargs: [\"--py37-plus\"]\n"
    },
    {
        "org_diff": "diff --git a/docs/tutorial/install.rst b/a/docs/tutorial/install.rst @@ -35,6 +35,7 @@ The ``pyproject.toml`` file describes your project and how to build it.\n     [project]\n     name = \"flaskr\"\n     version = \"1.0.0\"\n+    description = \"The basic blog app built in the Flask tutorial.\"\n     dependencies = [\n         \"flask\",\n     ]",
        "org_msg": "Add description to project in pyproject.toml",
        "sim_msg": "Add scripts and plugins to pyproject.toml",
        "sim_diff": "diff --git a/pyproject.toml b/pyproject.toml @@ -47,6 +47,37 @@ black = { version = \"^18.3-alpha.0\", python = \"^3.6\" }\npre-commit = \"^1.12\"\nbriefcase = \"^0.2.9\"\n+[tool.poetry.scripts]\n+gaphor = 'gaphor:main'\n+gaphorconvert = 'gaphor.tools.gaphorconvert:main'\n+\n+[tool.poetry.plugins.\"gaphor.services\"]\n+\"component_registry\" = \"gaphor.services.componentregistry:ZopeComponentRegistry\"\n+\"adapter_loader\" = \"gaphor.services.adapterloader:AdapterLoader\"\n+\"properties\" = \"gaphor.services.properties:Properties\"\n+\"undo_manager\" = \"gaphor.services.undomanager:UndoManager\"\n+\"element_factory\" = \"gaphor.UML.elementfactory:ElementFactoryService\"\n+\"file_manager\" = \"gaphor.services.filemanager:FileManager\"\n+\"diagram_export_manager\" = \"gaphor.services.diagramexportmanager:DiagramExportManager\"\n+\"action_manager\" = \"gaphor.services.actionmanager:ActionManager\"\n+\"ui_manager\" = \"gaphor.services.actionmanager:UIManager\"\n+\"main_window\" = \"gaphor.ui.mainwindow:MainWindow\"\n+\"copy\" = \"gaphor.services.copyservice:CopyService\"\n+\"sanitizer\" = \"gaphor.services.sanitizerservice:SanitizerService\"\n+\"element_dispatcher\" = \"gaphor.services.elementdispatcher:ElementDispatcher\"\n+\"xmi_export\" = \"gaphor.plugins.xmiexport:XMIExport\"\n+\"diagram_layout\" = \"gaphor.plugins.diagramlayout:DiagramLayout\"\n+\"pynsource\" = \"gaphor.plugins.pynsource:PyNSource\"\n+\"alignment\" = \"gaphor.plugins.alignment:Alignment\"\n+\"help\" = \"gaphor.services.helpservice:HelpService\"\n+\n+[tool.poetry.plugins.\"gaphor.uicomponents\"]\n+\"namespace\" = \"gaphor.ui.mainwindow:Namespace\"\n+\"toolbox\" = \"gaphor.ui.mainwindow:Toolbox\"\n+\"diagrams\" = \"gaphor.ui.mainwindow:Diagrams\"\n+\"consolewindow\" = \"gaphor.ui.consolewindow:ConsoleWindow\"\n+\"elementeditor\" = \"gaphor.ui.elementeditor:ElementEditor\"\n+\n[build-system]\nrequires = [\"poetry>=0.12\"]\nbuild-backend = \"poetry.masonry.api\"\n"
    },
    {
        "org_diff": "diff --git a/docs/tutorial/install.rst b/a/docs/tutorial/install.rst @@ -35,6 +35,7 @@ The ``pyproject.toml`` file describes your project and how to build it.\n     [project]\n     name = \"flaskr\"\n     version = \"1.0.0\"\n+    description = \"The basic blog app built in the Flask tutorial.\"\n     dependencies = [\n         \"flask\",\n     ]",
        "org_msg": "Add description to pyproject.toml for Flaskr project.",
        "sim_msg": "flesh out pyproject.toml",
        "sim_diff": "diff --git a/pyproject.toml b/pyproject.toml requires = [\n\"setuptools\",\n+ \"setuptools_scm\",\n\"wheel\",\n- \"extension-helpers\"\n+ \"extension-helpers\",\n+ \"oldest-supported-numpy\",\n+ \"cython==0.29.14\"\n]\nbuild-backend = 'setuptools.build_meta'\n"
    },
    {
        "org_diff": "diff --git a/.github/workflows/publish.yaml b/a/.github/workflows/publish.yaml @@ -33,7 +33,7 @@ jobs:\n       id-token: write\n       contents: write\n     # Can't pin with hash due to how this workflow works.\n-    uses: slsa-framework/slsa-github-generator/.github/workflows/generator_generic_slsa3.yml@v1.6.0\n+    uses: slsa-framework/slsa-github-generator/.github/workflows/generator_generic_slsa3.yml@v1.7.0\n     with:\n       base64-subjects: ${{ needs.build.outputs.hash }}\n   create-release:",
        "org_msg": "Update slsa-github-generator to v1.7.0 in publish.yaml workflow",
        "sim_msg": "Update .github/workflows/CI_publish_pypi.yml",
        "sim_diff": "diff --git a/.github/workflows/CI_publish_pypi.yml b/.github/workflows/CI_publish_pypi.yml @@ -17,7 +17,7 @@ jobs:\nrun: |\npython -m pip install --upgrade pip\npip install setuptools wheel twine\n- python setup.py bdist_wheel\n+ python setup.py sdist bdist_wheel\n- name: Publish package\nuses: pypa/gh-action-pypi-publish@master\nwith:\n"
    },
    {
        "org_diff": "diff --git a/.github/workflows/lock.yaml b/a/.github/workflows/lock.yaml @@ -19,7 +19,7 @@ jobs:\n   lock:\n     runs-on: ubuntu-latest\n     steps:\n-      - uses: dessant/lock-threads@c1b35aecc5cdb1a34539d14196df55838bb2f836\n+      - uses: dessant/lock-threads@be8aa5be94131386884a6da4189effda9b14aa21\n         with:\n           issue-inactive-days: 14\n           pr-inactive-days: 14",
        "org_msg": "Update lock-threads action to version be8aa5be94131386884a6da4189effda9b14aa21",
        "sim_msg": "Fix thread locking",
        "sim_diff": "diff --git a/batchflow/models/torch/base.py b/batchflow/models/torch/base.py @@ -919,6 +919,8 @@ class TorchModel:\nsplitted_inputs = [inputs]\nsplitted_targets = [targets]\n+ if use_lock:\n+ self.train_lock.acquire()\nif self.model is None:\nif isinstance(splitted_inputs[0], (list, tuple)):\n@@ -936,9 +938,6 @@ class TorchModel:\nself.model.train()\n- if use_lock:\n- self.train_lock.acquire()\n-\noutputs = []\nfor i in range(steps):\n_inputs = splitted_inputs[i]\n"
    },
    {
        "org_diff": "diff --git a/.github/workflows/publish.yaml b/a/.github/workflows/publish.yaml @@ -63,10 +63,10 @@ jobs:\n     steps:\n       - uses: actions/download-artifact@9bc31d5ccc31df68ecc42ccf4149144866c47d8a\n       # Try uploading to Test PyPI first, in case something fails.\n-      - uses: pypa/gh-action-pypi-publish@a56da0b891b3dc519c7ee3284aff1fad93cc8598\n+      - uses: pypa/gh-action-pypi-publish@f5622bde02b04381239da3573277701ceca8f6a0\n         with:\n           repository-url: https://test.pypi.org/legacy/\n           packages-dir: artifact/\n-      - uses: pypa/gh-action-pypi-publish@a56da0b891b3dc519c7ee3284aff1fad93cc8598\n+      - uses: pypa/gh-action-pypi-publish@f5622bde02b04381239da3573277701ceca8f6a0\n         with:\n           packages-dir: artifact/",
        "org_msg": "Update pypa/gh-action-pypi-publish to latest version",
        "sim_msg": "Update pypi release action.",
        "sim_diff": "diff --git a/.github/workflows/pypi_upload.yml b/.github/workflows/pypi_upload.yml @@ -47,7 +47,7 @@ jobs:\n# to upload to test pypi, pass repository_url: https://test.pypi.org/legacy/ and use secrets.TEST_PYPI_TOKEN\n- name: Publish a Python distribution to PyPI\n- uses: pypa/gh-action-pypi-publish@master\n+ uses: pypa/gh-action-pypi-publish@release/v1\nwith:\nuser: __token__\npassword: ${{ secrets.PYPI_TOKEN }}\n"
    },
    {
        "org_diff": "diff --git a/.github/workflows/publish.yaml b/a/.github/workflows/publish.yaml @@ -33,7 +33,7 @@ jobs:\n       id-token: write\n       contents: write\n     # Can't pin with hash due to how this workflow works.\n-    uses: slsa-framework/slsa-github-generator/.github/workflows/generator_generic_slsa3.yml@v1.6.0\n+    uses: slsa-framework/slsa-github-generator/.github/workflows/generator_generic_slsa3.yml@v1.7.0\n     with:\n       base64-subjects: ${{ needs.build.outputs.hash }}\n   create-release:",
        "org_msg": "Update slsa-github-generator to v1.7.0 in publish.yaml workflow.",
        "sim_msg": "Update .github/workflows/CI_publish_pypi.yml",
        "sim_diff": "diff --git a/.github/workflows/CI_publish_pypi.yml b/.github/workflows/CI_publish_pypi.yml @@ -17,7 +17,7 @@ jobs:\nrun: |\npython -m pip install --upgrade pip\npip install setuptools wheel twine\n- python setup.py bdist_wheel\n+ python setup.py sdist bdist_wheel\n- name: Publish package\nuses: pypa/gh-action-pypi-publish@master\nwith:\n"
    },
    {
        "org_diff": "diff --git a/.github/workflows/lock.yaml b/a/.github/workflows/lock.yaml @@ -19,7 +19,7 @@ jobs:\n   lock:\n     runs-on: ubuntu-latest\n     steps:\n-      - uses: dessant/lock-threads@c1b35aecc5cdb1a34539d14196df55838bb2f836\n+      - uses: dessant/lock-threads@be8aa5be94131386884a6da4189effda9b14aa21\n         with:\n           issue-inactive-days: 14\n           pr-inactive-days: 14",
        "org_msg": "Update lock-threads action to version be8aa5be94131386884a6da4189effda9b14aa21",
        "sim_msg": "Fix thread locking",
        "sim_diff": "diff --git a/batchflow/models/torch/base.py b/batchflow/models/torch/base.py @@ -919,6 +919,8 @@ class TorchModel:\nsplitted_inputs = [inputs]\nsplitted_targets = [targets]\n+ if use_lock:\n+ self.train_lock.acquire()\nif self.model is None:\nif isinstance(splitted_inputs[0], (list, tuple)):\n@@ -936,9 +938,6 @@ class TorchModel:\nself.model.train()\n- if use_lock:\n- self.train_lock.acquire()\n-\noutputs = []\nfor i in range(steps):\n_inputs = splitted_inputs[i]\n"
    },
    {
        "org_diff": "diff --git a/.github/workflows/publish.yaml b/a/.github/workflows/publish.yaml @@ -63,10 +63,10 @@ jobs:\n     steps:\n       - uses: actions/download-artifact@9bc31d5ccc31df68ecc42ccf4149144866c47d8a\n       # Try uploading to Test PyPI first, in case something fails.\n-      - uses: pypa/gh-action-pypi-publish@a56da0b891b3dc519c7ee3284aff1fad93cc8598\n+      - uses: pypa/gh-action-pypi-publish@f5622bde02b04381239da3573277701ceca8f6a0\n         with:\n           repository-url: https://test.pypi.org/legacy/\n           packages-dir: artifact/\n-      - uses: pypa/gh-action-pypi-publish@a56da0b891b3dc519c7ee3284aff1fad93cc8598\n+      - uses: pypa/gh-action-pypi-publish@f5622bde02b04381239da3573277701ceca8f6a0\n         with:\n           packages-dir: artifact/",
        "org_msg": "Update PyPI publish action to use latest version.",
        "sim_msg": "Update pypi release action.",
        "sim_diff": "diff --git a/.github/workflows/pypi_upload.yml b/.github/workflows/pypi_upload.yml @@ -47,7 +47,7 @@ jobs:\n# to upload to test pypi, pass repository_url: https://test.pypi.org/legacy/ and use secrets.TEST_PYPI_TOKEN\n- name: Publish a Python distribution to PyPI\n- uses: pypa/gh-action-pypi-publish@master\n+ uses: pypa/gh-action-pypi-publish@release/v1\nwith:\nuser: __token__\npassword: ${{ secrets.PYPI_TOKEN }}\n"
    },
    {
        "org_diff": "diff --git a/docs/errorhandling.rst b/a/docs/errorhandling.rst @@ -231,7 +231,7 @@ responses, you could also pass them through directly.\n Error handlers still respect the exception class hierarchy. If you\n register handlers for both ``HTTPException`` and ``Exception``, the\n ``Exception`` handler will not handle ``HTTPException`` subclasses\n-because it the ``HTTPException`` handler is more specific.\n+because the ``HTTPException`` handler is more specific.\n \n \n Unhandled Exceptions",
        "org_msg": "Refactored error handling to respect exception class hierarchy. Updated handling of HTTPException subclasses.",
        "sim_msg": "Tightening requests exception handling logic.",
        "sim_diff": "diff --git a/workers/worker_git_integration.py b/workers/worker_git_integration.py @@ -242,10 +242,12 @@ class WorkerGitInterfaceable(Worker):\nstacker = traceback.format_exc()\nself.logger.debug(f\"\\n\\n{stacker}\\n\\n\")\ntime.sleep(10)\n+ continue\nexcept Exception as e:\nself.logger.debug(f'unanticipated error in requests \\n\\n\\n\\n\\n\\n {e}\\n\\n\\n\\n\\n\\n\\n\\n\\n')\nstacker = traceback.format_exc()\nself.logger.debug(f\"\\n\\n{stacker}\\n\\n\")\n+ continue\nif len(self.oauths) == 0:\nself.logger.info(\n"
    },
    {
        "org_diff": "diff --git a/docs/errorhandling.rst b/a/docs/errorhandling.rst @@ -231,7 +231,7 @@ responses, you could also pass them through directly.\n Error handlers still respect the exception class hierarchy. If you\n register handlers for both ``HTTPException`` and ``Exception``, the\n ``Exception`` handler will not handle ``HTTPException`` subclasses\n-because it the ``HTTPException`` handler is more specific.\n+because the ``HTTPException`` handler is more specific.\n \n \n Unhandled Exceptions",
        "org_msg": "Fix typo in error handling documentation.",
        "sim_msg": "Fix confusing documentation typo.",
        "sim_diff": "diff --git a/jax/api.py b/jax/api.py @@ -919,7 +919,7 @@ def pmap(fun: Callable, axis_name: Optional[AxisName] = None,\nEach host passes in a different length-4 array, corresponding to its 4 local\ndevices, and the psum operates over all 8 values. Conceptually, the two\n- length-4 arrays can be thought of as sharded length-16 array (in this example\n+ length-4 arrays can be thought of as sharded length-8 array (in this example\nequivalent to np.arange(8)) that is mapped over, with the length-8 mapped axis\ngiven name 'i'. The pmap call on each host then returns the corresponding\nlength-4 output shard.\n"
    },
    {
        "org_diff": "diff --git a/tox.ini b/a/tox.ini @@ -34,17 +34,9 @@ skip_install = true\n commands = pre-commit run --all-files\n \n [testenv:typing]\n-package = wheel\n-wheel_build_env = .pkg\n-constrain_package_deps = true\n-use_frozen_constraints = true\n deps = -r requirements/typing.txt\n commands = mypy\n \n [testenv:docs]\n-package = wheel\n-wheel_build_env = .pkg\n-constrain_package_deps = true\n-use_frozen_constraints = true\n deps = -r requirements/docs.txt\n commands = sphinx-build -W -b html -d {envtmpdir}/doctrees docs {envtmpdir}/html",
        "org_msg": "Refactor tox.ini for testenv configurations",
        "sim_msg": "modified to include deps in tox.ini",
        "sim_diff": "diff --git a/tox.ini b/tox.ini @@ -798,6 +798,8 @@ commands =\n[testenv:stack.test.integration.oblv]\ndescription = Integration Tests for Oblv Enclave\n+deps =\n+ {[testenv:syft]deps}\nchangedir = {toxinidir}\nallowlist_externals =\ndocker\n@@ -815,9 +817,17 @@ commands =\npython -c \"import platform; import os; os.system('pip install jaxlib==0.3.14 -f https://whls.blob.core.windows.net/unstable/index.html') if platform.system().lower() == 'windows' else ''\"\n# run at start to kill any process started beforehand\nbash -c 'chmod +x scripts/kill_process_in_port.sh && ./scripts/kill_process_in_port.sh $LOCAL_ENCLAVE_PORT'\n- pip install -e packages/syft[dev]\npip install -e packages/hagrid\n+ bash -c 'rm -rf ~/.syft/syft-enclave'\n+ bash -c 'git clone https://github.com/OpenMined/syft-enclave.git ~/.syft/syft-enclave || true'\n+ bash -c 'cd ~/.syft/syft-enclave && git fetch && git checkout dev && git pull && pip install -r requirements_test.txt || true'\n+\n+ bash -c 'cd ~/.syft/syft-enclave/src && uvicorn app:app --port $LOCAL_ENCLAVE_PORT --host 0.0.0.0 &'\n+\n+ #wait time for FastAPI to start.\n+ bash -c 'sleep 10'\n+\ndocker --version\ndocker compose version\nbash -c \"docker volume rm test_domain_1_mongo-data --force || true\"\n@@ -831,14 +841,7 @@ commands =\nbash -c 'DOMAIN_CONNECTION_PORT=$LOCAL_ENCLAVE_PORT hagrid launch test_domain_1 domain to docker:9082 $HAGRID_FLAGS --no-health-checks --oblv'\nbash -c 'DOMAIN_CONNECTION_PORT=$LOCAL_ENCLAVE_PORT hagrid launch test_domain_2 domain to docker:9083 $HAGRID_FLAGS --no-health-checks --oblv'\n- bash -c 'rm -rf ~/.syft/syft-enclave'\n- bash -c 'git clone https://github.com/OpenMined/syft-enclave.git ~/.syft/syft-enclave || true'\n- bash -c 'cd ~/.syft/syft-enclave && git fetch && git checkout dev && git pull && pip install -r requirements_test.txt || true'\n-\n- bash -c 'cd ~/.syft/syft-enclave/src && uvicorn app:app --port $LOCAL_ENCLAVE_PORT --host 0.0.0.0 &'\n- #wait time for FastAPI to start.\n- sleep 15\nbash -c '(docker logs test_domain_1-backend_stream-1 -f &) | grep -q \"Application startup complete\" || true'\nbash -c '(docker logs test_domain_2-backend_stream-1 -f &) | grep -q \"Application startup complete\" || true'\n"
    },
    {
        "org_diff": "diff --git a/tox.ini b/a/tox.ini @@ -34,17 +34,9 @@ skip_install = true\n commands = pre-commit run --all-files\n \n [testenv:typing]\n-package = wheel\n-wheel_build_env = .pkg\n-constrain_package_deps = true\n-use_frozen_constraints = true\n deps = -r requirements/typing.txt\n commands = mypy\n \n [testenv:docs]\n-package = wheel\n-wheel_build_env = .pkg\n-constrain_package_deps = true\n-use_frozen_constraints = true\n deps = -r requirements/docs.txt\n commands = sphinx-build -W -b html -d {envtmpdir}/doctrees docs {envtmpdir}/html",
        "org_msg": "Refactor tox.ini to remove duplicate package and constraints in testenv configurations",
        "sim_msg": "Manage constraints with testenv install_command\nThis prevents having to duplicate the -c{} deps argument\nacross all environments in tox.ini",
        "sim_diff": "diff --git a/tox.ini b/tox.ini @@ -9,11 +9,10 @@ basepython = python3\nusedevelop = True\nwhitelist_externals = find\nrm\n-install_command = pip install {opts} {packages}\n+install_command = pip install -c{env:TOX_CONSTRAINTS_FILE:https://releases.openstack.org/constraints/upper/master} {opts} {packages}\nsetenv =\nVIRTUAL_ENV={envdir}\ndeps =\n- -c{env:UPPER_CONSTRAINTS_FILE:https://releases.openstack.org/constraints/upper/master}\n-r{toxinidir}/test-requirements.txt\n-r{toxinidir}/requirements.txt\ncommands =\n@@ -31,7 +30,6 @@ commands =\n[testenv:venv]\nsetenv = PYTHONHASHSEED=0\ndeps =\n- -c{env:UPPER_CONSTRAINTS_FILE:https://releases.openstack.org/constraints/upper/master}\n-r{toxinidir}/doc/requirements.txt\n-r{toxinidir}/test-requirements.txt\n-r{toxinidir}/requirements.txt\n@@ -50,7 +48,6 @@ commands =\n[testenv:docs]\nsetenv = PYTHONHASHSEED=0\ndeps =\n- -c{env:UPPER_CONSTRAINTS_FILE:https://releases.openstack.org/constraints/upper/master}\n-r{toxinidir}/doc/requirements.txt\ncommands =\nrm -fr doc/build doc/source/api/ .autogenerated\n@@ -75,6 +72,28 @@ commands =\ncommands =\noslopolicy-sample-generator --config-file etc/watcher/oslo-policy-generator/watcher-policy-generator.conf\n+[testenv:wheel]\n+commands = python setup.py bdist_wheel\n+\n+[testenv:pdf-docs]\n+envdir = {toxworkdir}/docs\n+deps = {[testenv:docs]deps}\n+whitelist_externals =\n+ rm\n+ make\n+commands =\n+ rm -rf doc/build/pdf\n+ sphinx-build -W --keep-going -b latex doc/source doc/build/pdf\n+ make -C doc/build/pdf\n+\n+[testenv:releasenotes]\n+deps = -r{toxinidir}/doc/requirements.txt\n+commands = sphinx-build -a -W -E -d releasenotes/build/doctrees --keep-going -b html releasenotes/source releasenotes/build/html\n+\n+[testenv:bandit]\n+deps = -r{toxinidir}/test-requirements.txt\n+commands = bandit -r watcher -x watcher/tests/* -n5 -ll -s B320\n+\n[flake8]\nfilename = *.py,app.wsgi\nshow-source=True\n@@ -84,9 +103,6 @@ builtins= _\nenable-extensions = H106,H203,H904\nexclude=.venv,.git,.tox,dist,doc,*lib/python*,*egg,build,*sqlalchemy/alembic/versions/*,demo/,releasenotes\n-[testenv:wheel]\n-commands = python setup.py bdist_wheel\n-\n[hacking]\nimport_exceptions = watcher._i18n\n@@ -110,27 +126,7 @@ extension =\nN366 = checks:import_stock_mock\npaths = ./watcher/hacking\n-\n[doc8]\nextension=.rst\n# todo: stop ignoring doc/source/man when https://bugs.launchpad.net/doc8/+bug/1502391 is fixed\nignore-path=doc/source/image_src,doc/source/man,doc/source/api\n-\n-[testenv:pdf-docs]\n-envdir = {toxworkdir}/docs\n-deps = {[testenv:docs]deps}\n-whitelist_externals =\n- rm\n- make\n-commands =\n- rm -rf doc/build/pdf\n- sphinx-build -W --keep-going -b latex doc/source doc/build/pdf\n- make -C doc/build/pdf\n-\n-[testenv:releasenotes]\n-deps = -r{toxinidir}/doc/requirements.txt\n-commands = sphinx-build -a -W -E -d releasenotes/build/doctrees --keep-going -b html releasenotes/source releasenotes/build/html\n-\n-[testenv:bandit]\n-deps = -r{toxinidir}/test-requirements.txt\n-commands = bandit -r watcher -x watcher/tests/* -n5 -ll -s B320\n"
    },
    {
        "org_diff": "diff --git a/docs/patterns/sqlalchemy.rst b/a/docs/patterns/sqlalchemy.rst @@ -34,8 +34,7 @@ official documentation on the `declarative`_ extension.\n Here's the example :file:`database.py` module for your application::\n \n     from sqlalchemy import create_engine\n-    from sqlalchemy.orm import scoped_session, sessionmaker\n-    from sqlalchemy.ext.declarative import declarative_base\n+    from sqlalchemy.orm import scoped_session, sessionmaker, declarative_base\n \n     engine = create_engine('sqlite:////tmp/test.db')\n     db_session = scoped_session(sessionmaker(autocommit=False,",
        "org_msg": "Refactor database.py module to import declarative_base directly from sqlalchemy.orm.",
        "sim_msg": "feat: remove sqlalchemy dependency and use db_conn for sql lookup",
        "sim_diff": "diff --git a/app/api/domains/api.py b/app/api/domains/api.py @@ -14,7 +14,6 @@ from fastapi.param_functions import Depends\nfrom fastapi.param_functions import Query\nfrom fastapi.responses import ORJSONResponse\nfrom fastapi.responses import StreamingResponse\n-from sqlalchemy import text\nimport app.packets\nimport app.state\n@@ -117,29 +116,14 @@ async def api_search_players(\n):\n\"\"\"Search for users on the server by name.\"\"\"\n- # we're using text() here because we are going to\n- # assign placeholders (bindparams()) to these queries,\n- # and this will let us bind those safely and efficiently.\n- results_query = text(\n- \"SELECT COUNT(id) FROM users WHERE name LIKE :search_clause AND priv & 3 = 3\",\n- ) # we will only need to lookup users who have Privileges.VERIFIED\n- rows_query = text(\n- \"SELECT id, name FROM users WHERE name LIKE :search_clause AND priv & 3 = 3 ORDER BY id ASC\",\n- ) # to save on host resources and prevent useless requests\n-\n- # using parameterised queries helps prevent against sql injections\n- # and will also allow the database engine to cache our queries,\n- # making these lookups faster in the future\n- search_clause = f\"%{search}%\" if search else None\n- results_query = results_query.bindparams(search_clause=search_clause)\n- rows_query = rows_query.bindparams(search_clause=search_clause)\n-\n- # execute the queries using the database connection\n- results = await db_conn.fetch_val(results_query)\n- rows = await db_conn.fetch_all(rows_query)\n+ # execute the query using the database connection\n+ rows = await db_conn.fetch_all(\n+ \"SELECT id, name FROM users WHERE name LIKE :search AND priv >= 3 ORDER BY id ASC\",\n+ {\"search\": f\"%{search}%\"}\n+ )\n# if the search returns nothing, we tell that to the user.\n- if results < 1:\n+ if len(rows) == 0:\nreturn ORJSONResponse(\n{\n\"status\": \"error\",\n@@ -151,7 +135,7 @@ async def api_search_players(\nreturn ORJSONResponse(\n{\n\"status\": \"success\",\n- \"results\": results,\n+ \"results\": len(rows),\n\"result\": [dict(row) for row in rows],\n},\n)\n"
    },
    {
        "org_diff": "diff --git a/docs/patterns/sqlalchemy.rst b/a/docs/patterns/sqlalchemy.rst @@ -34,8 +34,7 @@ official documentation on the `declarative`_ extension.\n Here's the example :file:`database.py` module for your application::\n \n     from sqlalchemy import create_engine\n-    from sqlalchemy.orm import scoped_session, sessionmaker\n-    from sqlalchemy.ext.declarative import declarative_base\n+    from sqlalchemy.orm import scoped_session, sessionmaker, declarative_base\n \n     engine = create_engine('sqlite:////tmp/test.db')\n     db_session = scoped_session(sessionmaker(autocommit=False,",
        "org_msg": "Refactored database.py module to import declarative_base directly from sqlalchemy.orm.",
        "sim_msg": "feat: remove sqlalchemy dependency and use db_conn for sql lookup",
        "sim_diff": "diff --git a/app/api/domains/api.py b/app/api/domains/api.py @@ -14,7 +14,6 @@ from fastapi.param_functions import Depends\nfrom fastapi.param_functions import Query\nfrom fastapi.responses import ORJSONResponse\nfrom fastapi.responses import StreamingResponse\n-from sqlalchemy import text\nimport app.packets\nimport app.state\n@@ -117,29 +116,14 @@ async def api_search_players(\n):\n\"\"\"Search for users on the server by name.\"\"\"\n- # we're using text() here because we are going to\n- # assign placeholders (bindparams()) to these queries,\n- # and this will let us bind those safely and efficiently.\n- results_query = text(\n- \"SELECT COUNT(id) FROM users WHERE name LIKE :search_clause AND priv & 3 = 3\",\n- ) # we will only need to lookup users who have Privileges.VERIFIED\n- rows_query = text(\n- \"SELECT id, name FROM users WHERE name LIKE :search_clause AND priv & 3 = 3 ORDER BY id ASC\",\n- ) # to save on host resources and prevent useless requests\n-\n- # using parameterised queries helps prevent against sql injections\n- # and will also allow the database engine to cache our queries,\n- # making these lookups faster in the future\n- search_clause = f\"%{search}%\" if search else None\n- results_query = results_query.bindparams(search_clause=search_clause)\n- rows_query = rows_query.bindparams(search_clause=search_clause)\n-\n- # execute the queries using the database connection\n- results = await db_conn.fetch_val(results_query)\n- rows = await db_conn.fetch_all(rows_query)\n+ # execute the query using the database connection\n+ rows = await db_conn.fetch_all(\n+ \"SELECT id, name FROM users WHERE name LIKE :search AND priv >= 3 ORDER BY id ASC\",\n+ {\"search\": f\"%{search}%\"}\n+ )\n# if the search returns nothing, we tell that to the user.\n- if results < 1:\n+ if len(rows) == 0:\nreturn ORJSONResponse(\n{\n\"status\": \"error\",\n@@ -151,7 +135,7 @@ async def api_search_players(\nreturn ORJSONResponse(\n{\n\"status\": \"success\",\n- \"results\": results,\n+ \"results\": len(rows),\n\"result\": [dict(row) for row in rows],\n},\n)\n"
    },
    {
        "org_diff": "diff --git a/pyproject.toml b/a/pyproject.toml @@ -54,8 +54,8 @@ version = {attr = \"flask.__version__\"}\n testpaths = [\"tests\"]\n filterwarnings = [\n     \"error\",\n-    # change in Python 3.12 alpha causes warning from inside pytest\n-    \"ignore:onerror argument:DeprecationWarning\",\n+    # change in Python 3.12 causes warning from inside pytest\n+    \"ignore:ast:DeprecationWarning\",\n ]\n \n [tool.coverage.run]",
        "org_msg": "Fix ignore warning in pytest caused by Python 3.12 change",
        "sim_msg": "Add deprecation warning ignores to pytest.ini",
        "sim_diff": "diff --git a/pytest.ini b/pytest.ini @@ -3,3 +3,6 @@ addopts = -ra -n auto\ntestpaths = tests/\n; Add vendor and patched in addition to the default list of ignored dirs\nnorecursedirs = .* build dist CVS _darcs {arch} *.egg vendor patched news tasks docs\n+filterwarnings =\n+ ignore::DeprecationWarning\n+ ignore::PendingDeprecationWarning\n"
    },
    {
        "org_diff": "diff --git a/.pre-commit-config.yaml b/a/.pre-commit-config.yaml @@ -7,7 +7,7 @@ repos:\n     hooks:\n       - id: pyupgrade\n         args: [\"--py38-plus\"]\n-  - repo: https://github.com/asottile/reorder_python_imports\n+  - repo: https://github.com/asottile/reorder-python-imports\n     rev: v3.9.0\n     hooks:\n       - id: reorder-python-imports",
        "org_msg": "Fix typo in repository URL for reorder-python-imports plugin",
        "sim_msg": "Fix module import typo",
        "sim_diff": "diff --git a/gaphor/services/properties.py b/gaphor/services/properties.py @@ -8,7 +8,7 @@ from builtins import object\nfrom logging import getLogger\nfrom zope import interface\n-from gaphor.decorators import AsyncIO\n+from gaphas.decorators import AsyncIO\nfrom zope.interface import implementer\nfrom gaphor.core import inject\n"
    },
    {
        "org_diff": "diff --git a/.pre-commit-config.yaml b/a/.pre-commit-config.yaml @@ -1,5 +1,5 @@\n ci:\n-  autoupdate_branch: \"2.2.x\"\n+  autoupdate_branch: \"2.3.x\"\n   autoupdate_schedule: monthly\n repos:\n   - repo: https://github.com/asottile/pyupgrade",
        "org_msg": "Update autoupdate_branch to \"2.3.x\" in ci section of .pre-commit-config.yaml",
        "sim_msg": "ci: pre-commit autoupdate",
        "sim_diff": "diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml @@ -15,7 +15,7 @@ repos:\n- id: python-check-mock-methods\n- id: python-check-blanket-type-ignore\n- repo: https://github.com/python-jsonschema/check-jsonschema\n- rev: 0.20.0\n+ rev: 0.21.0\nhooks:\n- id: check-github-workflows\n- id: check-dependabot\n@@ -25,7 +25,7 @@ repos:\n- id: prettier\ntypes_or: [markdown, yaml]\n- repo: https://github.com/pycqa/isort\n- rev: 5.11.4\n+ rev: 5.12.0\nhooks:\n- id: isort\n- repo: https://github.com/PyCQA/autoflake\n"
    },
    {
        "org_diff": "diff --git a/.pre-commit-config.yaml b/a/.pre-commit-config.yaml @@ -1,5 +1,5 @@\n ci:\n-  autoupdate_branch: \"2.2.x\"\n+  autoupdate_branch: \"2.3.x\"\n   autoupdate_schedule: monthly\n repos:\n   - repo: https://github.com/asottile/pyupgrade",
        "org_msg": "Update autoupdate_branch to \"2.3.x\" in ci configuration",
        "sim_msg": "ci: pre-commit autoupdate",
        "sim_diff": "diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml @@ -15,7 +15,7 @@ repos:\n- id: python-check-mock-methods\n- id: python-check-blanket-type-ignore\n- repo: https://github.com/python-jsonschema/check-jsonschema\n- rev: 0.20.0\n+ rev: 0.21.0\nhooks:\n- id: check-github-workflows\n- id: check-dependabot\n@@ -25,7 +25,7 @@ repos:\n- id: prettier\ntypes_or: [markdown, yaml]\n- repo: https://github.com/pycqa/isort\n- rev: 5.11.4\n+ rev: 5.12.0\nhooks:\n- id: isort\n- repo: https://github.com/PyCQA/autoflake\n"
    },
    {
        "org_diff": "diff --git a/.github/workflows/publish.yaml b/a/.github/workflows/publish.yaml @@ -63,10 +63,10 @@ jobs:\n     steps:\n       - uses: actions/download-artifact@9bc31d5ccc31df68ecc42ccf4149144866c47d8a\n       # Try uploading to Test PyPI first, in case something fails.\n-      - uses: pypa/gh-action-pypi-publish@0bf742be3ebe032c25dd15117957dc15d0cfc38d\n+      - uses: pypa/gh-action-pypi-publish@a56da0b891b3dc519c7ee3284aff1fad93cc8598\n         with:\n           repository-url: https://test.pypi.org/legacy/\n           packages-dir: artifact/\n-      - uses: pypa/gh-action-pypi-publish@0bf742be3ebe032c25dd15117957dc15d0cfc38d\n+      - uses: pypa/gh-action-pypi-publish@a56da0b891b3dc519c7ee3284aff1fad93cc8598\n         with:\n           packages-dir: artifact/",
        "org_msg": "Update pypa/gh-action-pypi-publish to use a specific version for Test PyPI and packages-dir.",
        "sim_msg": "Update pypi release action.",
        "sim_diff": "diff --git a/.github/workflows/pypi_upload.yml b/.github/workflows/pypi_upload.yml @@ -47,7 +47,7 @@ jobs:\n# to upload to test pypi, pass repository_url: https://test.pypi.org/legacy/ and use secrets.TEST_PYPI_TOKEN\n- name: Publish a Python distribution to PyPI\n- uses: pypa/gh-action-pypi-publish@master\n+ uses: pypa/gh-action-pypi-publish@release/v1\nwith:\nuser: __token__\npassword: ${{ secrets.PYPI_TOKEN }}\n"
    },
    {
        "org_diff": "diff --git a/.github/workflows/publish.yaml b/a/.github/workflows/publish.yaml @@ -33,7 +33,7 @@ jobs:\n       id-token: write\n       contents: write\n     # Can't pin with hash due to how this workflow works.\n-    uses: slsa-framework/slsa-github-generator/.github/workflows/generator_generic_slsa3.yml@v1.5.0\n+    uses: slsa-framework/slsa-github-generator/.github/workflows/generator_generic_slsa3.yml@v1.6.0\n     with:\n       base64-subjects: ${{ needs.build.outputs.hash }}\n   create-release:",
        "org_msg": "Update slsa-github-generator to v1.6.0 in publish.yaml workflow",
        "sim_msg": "Update .github/workflows/CI_publish_pypi.yml",
        "sim_diff": "diff --git a/.github/workflows/CI_publish_pypi.yml b/.github/workflows/CI_publish_pypi.yml @@ -17,7 +17,7 @@ jobs:\nrun: |\npython -m pip install --upgrade pip\npip install setuptools wheel twine\n- python setup.py bdist_wheel\n+ python setup.py sdist bdist_wheel\n- name: Publish package\nuses: pypa/gh-action-pypi-publish@master\nwith:\n"
    },
    {
        "org_diff": "diff --git a/.github/workflows/publish.yaml b/a/.github/workflows/publish.yaml @@ -63,10 +63,10 @@ jobs:\n     steps:\n       - uses: actions/download-artifact@9bc31d5ccc31df68ecc42ccf4149144866c47d8a\n       # Try uploading to Test PyPI first, in case something fails.\n-      - uses: pypa/gh-action-pypi-publish@0bf742be3ebe032c25dd15117957dc15d0cfc38d\n+      - uses: pypa/gh-action-pypi-publish@a56da0b891b3dc519c7ee3284aff1fad93cc8598\n         with:\n           repository-url: https://test.pypi.org/legacy/\n           packages-dir: artifact/\n-      - uses: pypa/gh-action-pypi-publish@0bf742be3ebe032c25dd15117957dc15d0cfc38d\n+      - uses: pypa/gh-action-pypi-publish@a56da0b891b3dc519c7ee3284aff1fad93cc8598\n         with:\n           packages-dir: artifact/",
        "org_msg": "Update pypi-publish action to use latest version",
        "sim_msg": "Update pypi release action.",
        "sim_diff": "diff --git a/.github/workflows/pypi_upload.yml b/.github/workflows/pypi_upload.yml @@ -47,7 +47,7 @@ jobs:\n# to upload to test pypi, pass repository_url: https://test.pypi.org/legacy/ and use secrets.TEST_PYPI_TOKEN\n- name: Publish a Python distribution to PyPI\n- uses: pypa/gh-action-pypi-publish@master\n+ uses: pypa/gh-action-pypi-publish@release/v1\nwith:\nuser: __token__\npassword: ${{ secrets.PYPI_TOKEN }}\n"
    },
    {
        "org_diff": "diff --git a/.github/workflows/publish.yaml b/a/.github/workflows/publish.yaml @@ -33,7 +33,7 @@ jobs:\n       id-token: write\n       contents: write\n     # Can't pin with hash due to how this workflow works.\n-    uses: slsa-framework/slsa-github-generator/.github/workflows/generator_generic_slsa3.yml@v1.5.0\n+    uses: slsa-framework/slsa-github-generator/.github/workflows/generator_generic_slsa3.yml@v1.6.0\n     with:\n       base64-subjects: ${{ needs.build.outputs.hash }}\n   create-release:",
        "org_msg": "Update slsa-github-generator to v1.6.0 in publish workflow",
        "sim_msg": "Update .github/workflows/CI_publish_pypi.yml",
        "sim_diff": "diff --git a/.github/workflows/CI_publish_pypi.yml b/.github/workflows/CI_publish_pypi.yml @@ -17,7 +17,7 @@ jobs:\nrun: |\npython -m pip install --upgrade pip\npip install setuptools wheel twine\n- python setup.py bdist_wheel\n+ python setup.py sdist bdist_wheel\n- name: Publish package\nuses: pypa/gh-action-pypi-publish@master\nwith:\n"
    },
    {
        "org_diff": "diff --git a/pyproject.toml b/a/pyproject.toml @@ -4,7 +4,6 @@ description = \"A simple framework for building complex web applications.\"\n readme = \"README.rst\"\n license = {text = \"BSD-3-Clause\"}\n maintainers = [{name = \"Pallets\", email = \"contact@palletsprojects.com\"}]\n-authors = [{name = \"Armin Ronacher\", email = \"armin.ronacher@active-4.com\"}]\n classifiers = [\n     \"Development Status :: 5 - Production/Stable\",\n     \"Environment :: Web Environment\",",
        "org_msg": "Remove author information from pyproject.toml",
        "sim_msg": "Remove errorous bit from pyproject.toml",
        "sim_diff": "diff --git a/pyproject.toml b/pyproject.toml @@ -50,13 +50,3 @@ pre-commit = \"^1.12\"\n[build-system]\nrequires = [\"poetry>=0.12\"]\nbuild-backend = \"poetry.masonry.api\"\n-\n-[testenv]\n-whitelist_externals =\n- poetry\n- xvfb-run\n-skip_install = true\n-commands =\n- poetry install -v\n- xvfb-run -a pytest\n-\"\"\"\n"
    },
    {
        "org_diff": "diff --git a/docs/config.rst b/a/docs/config.rst @@ -410,8 +410,8 @@ from a TOML file:\n \n .. code-block:: python\n \n-    import toml\n-    app.config.from_file(\"config.toml\", load=toml.load)\n+    import tomllib\n+    app.config.from_file(\"config.toml\", load=tomllib.load, text=False)\n \n Or from a JSON file:\n ",
        "org_msg": "Use tomllib for loading TOML files and set text to False.",
        "sim_msg": "Pass decoder to `toml.loads`",
        "sim_diff": "diff --git a/pipenv/project.py b/pipenv/project.py @@ -600,7 +600,7 @@ class Project(object):\n# Fallback to toml parser, for large files.\nexcept Exception:\ntoml_decoder = toml.decoder.TomlDecoder()\n- return toml.loads(contents, encoder=toml_decoder)\n+ return toml.loads(contents, decoder=toml_decoder)\ndef _read_pyproject(self):\npyproject = self.path_to(\"pyproject.toml\")\n"
    },
    {
        "org_diff": "diff --git a/docs/config.rst b/a/docs/config.rst @@ -410,8 +410,8 @@ from a TOML file:\n \n .. code-block:: python\n \n-    import toml\n-    app.config.from_file(\"config.toml\", load=toml.load)\n+    import tomllib\n+    app.config.from_file(\"config.toml\", load=tomllib.load, text=False)\n \n Or from a JSON file:\n ",
        "org_msg": "\"Update config file loading to use tomllib and add option to load as text\"",
        "sim_msg": "Changed tomli.loads",
        "sim_diff": "diff --git a/bin/data.py b/bin/data.py @@ -355,7 +355,7 @@ class TestsTOML:\n@classmethod\ndef load(cls, toml_path: Path):\nwith toml_path.open(\"rb\") as f:\n- data = tomli.load(f)\n+ data = tomli.loads(f)\nreturn cls({uuid: TestCaseTOML(uuid, *opts) for\nuuid, opts in\ndata.items() if\n"
    },
    {
        "org_diff": "diff --git a/docs/patterns/javascript.rst b/a/docs/patterns/javascript.rst @@ -125,8 +125,8 @@ in a Flask view.\n .. code-block:: javascript\n \n     let data = new FormData()\n-    data.append(\"name\": \"Flask Room\")\n-    data.append(\"description\": \"Talk about Flask here.\")\n+    data.append(\"name\", \"Flask Room\")\n+    data.append(\"description\", \"Talk about Flask here.\")\n     fetch(room_url, {\n         \"method\": \"POST\",\n         \"body\": data,",
        "org_msg": "Fix syntax error in JavaScript code in Flask view documentation",
        "sim_msg": "docs: Fix python syntax error in the user guide",
        "sim_diff": "diff --git a/docs/guide/coroutines.rst b/docs/guide/coroutines.rst @@ -210,7 +210,7 @@ In decorated coroutines, it is possible to ``yield`` the list or dict directly::\n@gen.coroutine\ndef parallel_fetch_decorated(url1, url2):\nresp1, resp2 = yield [http_client.fetch(url1),\n- http_client.fetch(url2)])\n+ http_client.fetch(url2)]\nInterleaving\n^^^^^^^^^^^^\n"
    },
    {
        "org_diff": "diff --git a/src/flask/config.py b/a/src/flask/config.py @@ -72,7 +72,9 @@ class Config(dict):\n     :param defaults: an optional dictionary of default values\n     \"\"\"\n \n-    def __init__(self, root_path: str, defaults: dict | None = None) -> None:\n+    def __init__(\n+        self, root_path: str | os.PathLike, defaults: dict | None = None\n+    ) -> None:\n         super().__init__(defaults or {})\n         self.root_path = root_path\n \n@@ -164,7 +166,7 @@ def from_prefixed_env(\n \n         return True\n \n-    def from_pyfile(self, filename: str, silent: bool = False) -> bool:\n+    def from_pyfile(self, filename: str | os.PathLike, silent: bool = False) -> bool:\n         \"\"\"Updates the values in the config from a Python file.  This function\n         behaves as if the file was imported as module with the\n         :meth:`from_object` function.\n@@ -233,7 +235,7 @@ class and has ``@property`` attributes, it needs to be\n \n     def from_file(\n         self,\n-        filename: str,\n+        filename: str | os.PathLike,\n         load: t.Callable[[t.IO[t.Any]], t.Mapping],\n         silent: bool = False,\n         text: bool = True,",
        "org_msg": "Update Config class to support os.PathLike for root_path and filenames",
        "sim_msg": "Update ROOT_PATH dir",
        "sim_diff": "diff --git a/src/sparseml/utils/helpers.py b/src/sparseml/utils/helpers.py @@ -72,7 +72,7 @@ ALL_PRUNABLE_TOKEN = \"__ALL_PRUNABLE__\"\nFROM_PARAM_TOKEN = \"__FROM_PARAM__\"\nRECIPE_METADATA_KEY = \"__metadata__\"\nFRAMEWORK_METADATA_KEY = \"framework_metadata\"\n-ROOT_PATH = Path(__file__).resolve().parents[3]\n+ROOT_PATH = Path(__file__).resolve().parents[1]\n_LOGGER = logging.getLogger(__name__)\n"
    },
    {
        "org_diff": "diff --git a/src/flask/config.py b/a/src/flask/config.py @@ -72,7 +72,9 @@ class Config(dict):\n     :param defaults: an optional dictionary of default values\n     \"\"\"\n \n-    def __init__(self, root_path: str, defaults: dict | None = None) -> None:\n+    def __init__(\n+        self, root_path: str | os.PathLike, defaults: dict | None = None\n+    ) -> None:\n         super().__init__(defaults or {})\n         self.root_path = root_path\n \n@@ -164,7 +166,7 @@ def from_prefixed_env(\n \n         return True\n \n-    def from_pyfile(self, filename: str, silent: bool = False) -> bool:\n+    def from_pyfile(self, filename: str | os.PathLike, silent: bool = False) -> bool:\n         \"\"\"Updates the values in the config from a Python file.  This function\n         behaves as if the file was imported as module with the\n         :meth:`from_object` function.\n@@ -233,7 +235,7 @@ class and has ``@property`` attributes, it needs to be\n \n     def from_file(\n         self,\n-        filename: str,\n+        filename: str | os.PathLike,\n         load: t.Callable[[t.IO[t.Any]], t.Mapping],\n         silent: bool = False,\n         text: bool = True,",
        "org_msg": "Refactor config.py to use os.PathLike for root_path and filename parameters",
        "sim_msg": "minor update, now includes getting folders from config.py",
        "sim_diff": "diff --git a/notebooks/Spec2Vec_03_Similarities_based_on_molecular_fingerprints.ipynb b/notebooks/Spec2Vec_03_Similarities_based_on_molecular_fingerprints.ipynb \"import sys\\n\",\n\"sys.path.insert(0, os.path.dirname(os.getcwd()))\\n\",\n\"\\n\",\n+ \"# Locations (import paths from config.py)\\n\",\n+ \"from config import ROOT, PATH_SPEC2VEC, PATH_MS_DATA, PATH_SAVE_MODEL, PATH_COMPUTED, PATH_OUTPUT\\n\",\n+ \"\\n\",\n+ \"sys.path.insert(0, PATH_SPEC2VEC)\\n\",\n+ \"\\n\",\n\"import helper_functions as functions\\n\",\n\"import MS_functions\"\n]\n},\n- {\n- \"cell_type\": \"code\",\n- \"execution_count\": 2,\n- \"metadata\": {},\n- \"outputs\": [],\n- \"source\": [\n- \"# Locations (import paths from config.py)\\n\",\n- \"from config import ROOT, PATH_SPEC2VEC, PATH_MS_DATA, PATH_SAVE_MODEL, PATH_COMPUTED, PATH_OUTPUT\"\n- ]\n- },\n{\n\"cell_type\": \"markdown\",\n\"metadata\": {},\n"
    },
    {
        "org_diff": "diff --git a/CHANGES.rst b/a/CHANGES.rst @@ -69,6 +69,15 @@ Released 2023-04-25\n -   Use postponed evaluation of annotations. :pr:`5071`\n \n \n+Version 2.2.5\n+-------------\n+\n+Released 2023-05-02\n+\n+-   Update for compatibility with Werkzeug 2.3.3.\n+-   Set ``Vary: Cookie`` header when the session is accessed, modified, or refreshed.\n+\n+\n Version 2.2.4\n -------------\n ",
        "org_msg": "Update for compatibility with Werkzeug 2.3.3 and set \"Vary: Cookie\" header when session is accessed, modified, or refreshed. Released version 2.2.5 on 2023-05-02.",
        "sim_msg": "[honggfuzz] Update version",
        "sim_diff": "diff --git a/fuzzers/honggfuzz/builder.Dockerfile b/fuzzers/honggfuzz/builder.Dockerfile @@ -23,14 +23,14 @@ RUN apt-get update -y && \\\nlibblocksruntime-dev \\\nliblzma-dev\n-# Download honggfuz version 2.1 + f316276ee58c2339ce8505d58eaf63baa967ed1c\n+# Download honggfuz version 2.1 + 539ea048d6273864e396ad95b08911c69cd2ac51\n# Set CFLAGS use honggfuzz's defaults except for -mnative which can build CPU\n# dependent code that may not work on the machines we actually fuzz on.\n# Create an empty object file which will become the FUZZER_LIB lib (since\n# honggfuzz doesn't need this when hfuzz-clang(++) is used).\nRUN git clone https://github.com/google/honggfuzz.git /honggfuzz && \\\ncd /honggfuzz && \\\n- git checkout f316276ee58c2339ce8505d58eaf63baa967ed1c && \\\n+ git checkout 539ea048d6273864e396ad95b08911c69cd2ac51 && \\\nCFLAGS=\"-O3 -funroll-loops\" make && \\\ntouch empty_lib.c && \\\ncc -c -o empty_lib.o empty_lib.c\n"
    },
    {
        "org_diff": "diff --git a/.github/workflows/publish.yaml b/a/.github/workflows/publish.yaml @@ -63,10 +63,10 @@ jobs:\n     steps:\n       - uses: actions/download-artifact@9bc31d5ccc31df68ecc42ccf4149144866c47d8a\n       # Try uploading to Test PyPI first, in case something fails.\n-      - uses: pypa/gh-action-pypi-publish@29930c9cf57955dc1b98162d0d8bc3ec80d9e75c\n+      - uses: pypa/gh-action-pypi-publish@0bf742be3ebe032c25dd15117957dc15d0cfc38d\n         with:\n           repository_url: https://test.pypi.org/legacy/\n           packages_dir: artifact/\n-      - uses: pypa/gh-action-pypi-publish@29930c9cf57955dc1b98162d0d8bc3ec80d9e75c\n+      - uses: pypa/gh-action-pypi-publish@0bf742be3ebe032c25dd15117957dc15d0cfc38d\n         with:\n           packages_dir: artifact/",
        "org_msg": "Update pypa/gh-action-pypi-publish to version 0bf742be3ebe032c25dd15117957dc15d0cfc38d",
        "sim_msg": "docs: update supported Python version list.",
        "sim_diff": "diff --git a/docs/ansible.rst b/docs/ansible.rst @@ -53,8 +53,8 @@ Installation\n------------\n1. Thoroughly review the documented behavioural differences.\n-2. Verify Ansible 2.3/2.4/2.5 and Python 2.7 are listed in ``ansible --version``\n- output.\n+2. Verify Ansible 2.3-2.5 and Python 2.6, 2.7 or 3.6 are listed in ``ansible\n+ --version`` output.\n3. Download and extract https://github.com/dw/mitogen/archive/master.zip\n4. Modify ``ansible.cfg``:\n"
    },
    {
        "org_diff": "diff --git a/.github/workflows/publish.yaml b/a/.github/workflows/publish.yaml @@ -63,10 +63,10 @@ jobs:\n     steps:\n       - uses: actions/download-artifact@9bc31d5ccc31df68ecc42ccf4149144866c47d8a\n       # Try uploading to Test PyPI first, in case something fails.\n-      - uses: pypa/gh-action-pypi-publish@29930c9cf57955dc1b98162d0d8bc3ec80d9e75c\n+      - uses: pypa/gh-action-pypi-publish@0bf742be3ebe032c25dd15117957dc15d0cfc38d\n         with:\n           repository_url: https://test.pypi.org/legacy/\n           packages_dir: artifact/\n-      - uses: pypa/gh-action-pypi-publish@29930c9cf57955dc1b98162d0d8bc3ec80d9e75c\n+      - uses: pypa/gh-action-pypi-publish@0bf742be3ebe032c25dd15117957dc15d0cfc38d\n         with:\n           packages_dir: artifact/",
        "org_msg": "Update pypa/gh-action-pypi-publish to version 0bf742be3ebe032c25dd15117957dc15d0cfc38d",
        "sim_msg": "docs: update supported Python version list.",
        "sim_diff": "diff --git a/docs/ansible.rst b/docs/ansible.rst @@ -53,8 +53,8 @@ Installation\n------------\n1. Thoroughly review the documented behavioural differences.\n-2. Verify Ansible 2.3/2.4/2.5 and Python 2.7 are listed in ``ansible --version``\n- output.\n+2. Verify Ansible 2.3-2.5 and Python 2.6, 2.7 or 3.6 are listed in ``ansible\n+ --version`` output.\n3. Download and extract https://github.com/dw/mitogen/archive/master.zip\n4. Modify ``ansible.cfg``:\n"
    },
    {
        "org_diff": "diff --git a/CHANGES.rst b/a/CHANGES.rst @@ -1,7 +1,7 @@\n Version 2.3.2\n -------------\n \n-Released 2022-05-01\n+Released 2023-05-01\n \n -   Set ``Vary: Cookie`` header when the session is accessed, modified, or refreshed.\n -   Update Werkzeug requirement to >=2.3.3 to apply recent bug fixes.",
        "org_msg": "Update release date to 2023-05-01 in CHANGES.rst",
        "sim_msg": "build: set next release number to 3.0.0",
        "sim_diff": "diff --git a/jina/__init__.py b/jina/__init__.py @@ -60,7 +60,7 @@ elif _sys.version_info >= (3, 8, 0) and _platform.system() == 'Darwin':\n# this is managed by git tag and updated on every release\n# NOTE: this represents the NEXT release version\n-__version__ = '2.7.1'\n+__version__ = '3.0.0'\n# do not change this line manually\n# this is managed by proto/build-proto.sh and updated on every execution\n"
    },
    {
        "org_diff": "diff --git a/CHANGES.rst b/a/CHANGES.rst @@ -3,6 +3,9 @@ Version 2.3.2\n \n Unreleased\n \n+-   Session cookie sets ``Vary: Cookie`` header when it is accessed, modified, cleared,\n+    or refreshed.\n+\n \n Version 2.3.1\n -------------",
        "org_msg": "Add functionality to set \"Vary: Cookie\" header when session cookie is accessed, modified, cleared, or refreshed.",
        "sim_msg": "Adding ability to pass cookies",
        "sim_diff": "diff --git a/atlassian/rest_client.py b/atlassian/rest_client.py @@ -17,7 +17,7 @@ class AtlassianRestAPI(object):\n'X-Atlassian-Token': 'no-check'}\ndef __init__(self, url, username=None, password=None, timeout=60, api_root='rest/api', api_version='latest',\n- verify_ssl=True, session=None, oauth=None):\n+ verify_ssl=True, session=None, oauth=None, cookies=None):\nself.url = url\nself.username = username\nself.password = password\n@@ -25,6 +25,7 @@ class AtlassianRestAPI(object):\nself.verify_ssl = verify_ssl\nself.api_root = api_root\nself.api_version = api_version\n+ self.cookies = cookies\nif session is None:\nself._session = requests.Session()\nelse:\n@@ -53,7 +54,7 @@ class AtlassianRestAPI(object):\n\"\"\"\nself._session.headers.update({key: value})\n- def log_curl_debug(self, method, path, data=None, headers=None, trailing=None, level=logging.DEBUG):\n+ def log_curl_debug(self, method, path, data=None, headers=None, trailing=None, level=logging.DEBUG, cookies=None):\n\"\"\"\n:param method:\n@@ -62,6 +63,7 @@ class AtlassianRestAPI(object):\n:param headers:\n:param trailing: bool flag for trailing /\n:param level:\n+ :param cookies:\n:return:\n\"\"\"\nheaders = headers or self.default_headers\n"
    },
    {
        "org_diff": "diff --git a/src/flask/app.py b/a/src/flask/app.py @@ -1852,7 +1852,7 @@ def make_response(self, rv: ft.ResponseReturnValue) -> Response:\n \n         # extend existing headers with provided headers\n         if headers:\n-            rv.headers.update(headers)\n+            rv.headers.update(headers)  # type: ignore[arg-type]\n \n         return rv\n ",
        "org_msg": "Fixing type ignore for headers update in make_response function",
        "sim_msg": "Update client.py\nIgnore type errors running under Python 3.10.",
        "sim_diff": "diff --git a/api_client/python/timesketch_api_client/client.py b/api_client/python/timesketch_api_client/client.py @@ -713,7 +713,7 @@ class TimesketchApi:\nexcept ValueError:\nlogger.error(\"Parsing Error, unable to parse the Sigma rule\", exc_info=True)\n- return sigma_obj\n+ return sigma_obj # pytype: disable=name-error # py310-upgrade\ndef get_sigma_rule(self, rule_uuid):\n\"\"\"DEPRECATED please use get_sigmarule() instead: Get a sigma rule.\n@@ -752,4 +752,4 @@ class TimesketchApi:\nexcept ValueError:\nlogger.error(\"Parsing Error, unable to parse the Sigma rule\", exc_info=True)\n- return sigma_obj\n+ return sigma_obj # pytype: disable=name-error # py310-upgrade\n"
    },
    {
        "org_diff": "diff --git a/CHANGES.rst b/a/CHANGES.rst @@ -53,7 +53,7 @@ Unreleased\n Version 2.2.4\n -------------\n \n-Unreleased\n+Released 2023-04-25\n \n -   Update for compatibility with Werkzeug 2.3.\n ",
        "org_msg": "Release version 2.2.4 on 2023-04-25 and update for compatibility with Werkzeug 2.3.",
        "sim_msg": "update for 3.2.4",
        "sim_diff": "diff --git a/web/news.rst b/web/news.rst @@ -4,6 +4,10 @@ NLTK News\n2017\n----\n+NLTK 3.2.4 released: May 2017\n+ Remove load-time dependency on Python requests library,\n+ Add support for Arabic in StanfordSegmenter\n+\nNLTK 3.2.3 released: May 2017\nInterface to Stanford CoreNLP Web API, improved Lancaster stemmer,\nimproved Treebank tokenizer, support custom tab files for extending WordNet,\n"
    },
    {
        "org_diff": "diff --git a/.github/workflows/tests.yaml b/a/.github/workflows/tests.yaml @@ -34,7 +34,7 @@ jobs:\n           - {name: '3.7', python: '3.7', os: ubuntu-latest, tox: py37}\n           - {name: 'PyPy', python: 'pypy-3.9', os: ubuntu-latest, tox: pypy39}\n           - {name: 'Pallets Minimum Versions', python: '3.11', os: ubuntu-latest, tox: py311-min}\n-          - {name: 'Pallets Development Versions', python: '3.7', os: ubuntu-latest, tox: py37-dev}\n+          - {name: 'Pallets Development Versions', python: '3.8', os: ubuntu-latest, tox: py38-dev}\n           - {name: Typing, python: '3.11', os: ubuntu-latest, tox: typing}\n     steps:\n       - uses: actions/checkout@ac593985615ec2ede58e132d2e21d2b1cbd6127c",
        "org_msg": "Update Pallets Development Versions to use Python 3.8 instead of 3.7",
        "sim_msg": "Fix import for Python >= 3.8",
        "sim_diff": "diff --git a/pymatgen/io/qchem/inputs.py b/pymatgen/io/qchem/inputs.py @@ -7,7 +7,6 @@ Classes for reading/manipulating/writing QChem input files.\n\"\"\"\nimport logging\nfrom typing import Union, Dict, List, Optional\n-from typing_extensions import Literal\nfrom monty.io import zopen\nfrom monty.json import MSONable\n@@ -16,6 +15,11 @@ from pymatgen.core import Molecule\nfrom .utils import lower_and_check_unique, read_pattern, read_table_pattern\n+try:\n+ from typing import Literal\n+except ImportError:\n+ from typing_extensions import Literal\n+\n__author__ = \"Brandon Wood, Samuel Blau, Shyam Dwaraknath, Julian Self, Evan Spotte-Smith\"\n__copyright__ = \"Copyright 2018, The Materials Project\"\n__version__ = \"0.1\"\n"
    }
]