{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-25T10:28:05.119238Z",
     "start_time": "2024-03-25T10:28:05.034506Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "train_msg = open(\"../../MCMD/java/sort_random_train80_valid10_test10/train.msg.txt\", encoding='UTF-8').read().split(\"\\n\")\n",
    "len(train_msg)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "45001"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_msg = open(\"../../MCMD/java/sort_random_train80_valid10_test10/valid.msg.txt\", encoding='UTF-8').read().split(\"\\n\")\n",
    "len(valid_msg)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T13:54:05.413218Z",
     "start_time": "2024-03-25T13:54:05.356923Z"
    }
   },
   "id": "470ad0d71244ce87",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "45001"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_msg = open(\"../../MCMD/java/sort_random_train80_valid10_test10/test.msg.txt\", encoding='UTF-8').read().split(\"\\n\")\n",
    "len(test_msg)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T13:54:37.173501Z",
     "start_time": "2024-03-25T13:54:37.144239Z"
    }
   },
   "id": "c4d51fd104df8efb",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocess script"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "318ee7d84352eda0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import re\n",
    "def process_diff(diff_text):\n",
    "    # Replace <nl> with \\n\n",
    "    diff_text = diff_text.replace('<nl>', '\\n')\n",
    "    diff_text = re.sub(r'(?<=\\S)\\s*([^\\w\\s])\\s*(?=\\S)', r'\\1', diff_text)\n",
    "    diff_text = re.sub(r'\\n\\s+', r'\\n', diff_text)\n",
    "    return diff_text\n",
    "\n",
    "def process_msg(msg_text):\n",
    "    msg_text = re.sub(r'(?<=\\S)\\s*([^\\w\\s])\\s*(?=\\S)', r'\\1', msg_text)\n",
    "    return msg_text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T14:27:46.450256Z",
     "start_time": "2024-03-27T14:27:46.444745Z"
    }
   },
   "id": "ef6edb389ec698d6",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "selected_messages = []\n",
    "selected_languages = []\n",
    "selected_diffs = []\n",
    "nmt_msg = []\n",
    "nngen_msg = []\n",
    "\n",
    "def extract_diffs(indices, diff_file_path, nngen_path, nmt_path):\n",
    "    with open(diff_path, 'r', encoding='UTF-8') as diff_file:\n",
    "        diff_line = diff_file.readlines()\n",
    "    \n",
    "    with open(nngen_path, 'r', encoding='UTF-8') as nngen_file:\n",
    "        nngen_line = nngen_file.readlines()\n",
    "\n",
    "    with open(nmt_path, 'r', encoding='UTF-8') as nmt_file:\n",
    "        nmt_line = nmt_file.readlines()\n",
    "    \n",
    "    \n",
    "    for i in indices:\n",
    "        diff = process_diff(diff_line[i])\n",
    "        nngen = nngen_line[i]\n",
    "        nmt = nmt_line[i]\n",
    "        selected_diffs.append(diff)\n",
    "        nngen_msg.append(nngen)\n",
    "        nmt_msg.append(nmt)\n",
    "\n",
    "mcmd_base_path = \"../../MCMD\"\n",
    "models_base_path = \"./data\"\n",
    "\n",
    "languages = [\"java\", \"cpp\", \"csharp\", \"python\", \"javascript\"]\n",
    "file_names = [\"test.msg.txt\", \"test.diff.txt\"]\n",
    "\n",
    "test_msg_path = [f\"{mcmd_base_path}/{lang}/sort_random_train80_valid10_test10/test.msg.txt\" for lang in languages]\n",
    "test_diff_path = [f\"{mcmd_base_path}/{lang}/sort_random_train80_valid10_test10/test.diff.txt\" for lang in languages]\n",
    "\n",
    "nngen_msg_path = [f\"{models_base_path}/model_NNGen/data_MCMD/{lang}/split_random/gen.msg\" for lang in languages]\n",
    "nmt_msg_path = [f\"{models_base_path}/model_NMT/data_MCMD/{lang}/split_random/gen.msg\" for lang in languages]\n",
    "\n",
    "\n",
    "for msg_path, diff_path, nngen_path, nmt_path in zip(test_msg_path, test_diff_path, nngen_msg_path, nmt_msg_path):\n",
    "    selected_indices = []\n",
    "    with open(msg_path, 'r', encoding='UTF-8') as f:\n",
    "        count = 0\n",
    "        for i, line in enumerate(f):\n",
    "            if count>= 1000:\n",
    "                break\n",
    "            # line = process_msg(line)\n",
    "            if len(line.split()) >= 5:\n",
    "                selected_indices.append(i)\n",
    "                selected_messages.append(line.strip())\n",
    "                selected_languages.append(msg_path.split('/')[3])\n",
    "                count += 1\n",
    "            \n",
    "        extract_diffs(selected_indices, diff_path, nngen_path, nmt_path)\n",
    "\n",
    "# Create a list of dictionaries containing selected messages and diffs\n",
    "data = []\n",
    "for msg, diff, nngen, nmt, language in zip(selected_messages, selected_diffs, nngen_msg, nmt_msg, selected_languages):\n",
    "    item = {\n",
    "        'msg': msg,\n",
    "        'diff': diff,\n",
    "        'nngen': nngen,\n",
    "        'nmt': nmt,\n",
    "        'language': language\n",
    "    }\n",
    "    data.append(item)\n",
    "\n",
    "# Write the data to a JSON file\n",
    "with open('data/selected_data.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T14:28:04.584697Z",
     "start_time": "2024-03-27T14:27:53.358353Z"
    }
   },
   "id": "6a44a24a43368028",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "RobertaForCausalLM(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (lm_head): RobertaLMHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n  )\n)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaForCausalLM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\", cache_dir='./')\n",
    "model = RobertaForCausalLM.from_pretrained(\"microsoft/codebert-base\", cache_dir='./', is_decoder=True)\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T21:06:04.102066Z",
     "start_time": "2024-03-27T21:06:02.745014Z"
    }
   },
   "id": "2ce23e68a300aeb1",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_description(code):\n",
    "    inputs = tokenizer(code, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, max_length=128, num_return_sequences=1)\n",
    "    description = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return description"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T21:06:28.526495Z",
     "start_time": "2024-03-27T21:06:28.514496Z"
    }
   },
   "id": "e5dcd48fff223c0",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyfann\\anaconda3\\envs\\phi\\lib\\site-packages\\transformers\\generation\\utils.py:1363: UserWarning: Input length of input_ids is 488, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"The following is a diff which describes the code changes in a commit, Your task is to write a short commit message accordingly. diff --git a/src/gwt/src/org/rstudio/studio/client/workbench/views/source/editors/text/TextEditingTargetRenameHelper.java b/src/gwt/src/org/rstudio/studio/client/workbench/views/source/editors/text/TextEditingTargetRenameHelper.java\\nindex e8d986667..ce243ca1f 100644\\n--- a/src/gwt/src/org/rstudio/studio/client/workbench/views/source/editors/text/TextEditingTargetRenameHelper.java\\n+++ b/src/gwt/src/org/rstudio/studio/client/workbench/views/source/editors/text/TextEditingTargetRenameHelper.java\\n@@ -44,7 +44,7 @@ public class TextEditingTargetRenameHelper\\n       \\n       editor_.setCursorPosition(position);\\n       \\n-      // Validate that we're looking at an R identifier (TODO: refactor strings?)\\n+      // Validate that we're looking at an R identifier\\n       String targetValue = cursor.currentValue();\\n       String targetType = cursor.currentType();\\n       \\n@@ -104,7 +104,6 @@ public class TextEditingTargetRenameHelper\\n       }\\n       \\n       // Otherwise, just rename the variable within the current scope.\\n-      // TODO: Do we need to look into parent scopes?\\n       return renameVariablesInScope(\\n             editor_.getCurrentScope(),\\n             targetValue,\\nAccording to the diff, the commit message should be:inis\""
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_description(\"The following is a diff which describes the code changes in a commit, Your task is to write a short commit message accordingly. diff --git a/src/gwt/src/org/rstudio/studio/client/workbench/views/source/editors/text/TextEditingTargetRenameHelper.java b/src/gwt/src/org/rstudio/studio/client/workbench/views/source/editors/text/TextEditingTargetRenameHelper.java\\nindex e8d986667..ce243ca1f 100644\\n--- a/src/gwt/src/org/rstudio/studio/client/workbench/views/source/editors/text/TextEditingTargetRenameHelper.java\\n+++ b/src/gwt/src/org/rstudio/studio/client/workbench/views/source/editors/text/TextEditingTargetRenameHelper.java\\n@@ -44,7 +44,7 @@ public class TextEditingTargetRenameHelper\\n       \\n       editor_.setCursorPosition(position);\\n       \\n-      // Validate that we're looking at an R identifier (TODO: refactor strings?)\\n+      // Validate that we're looking at an R identifier\\n       String targetValue = cursor.currentValue();\\n       String targetType = cursor.currentType();\\n       \\n@@ -104,7 +104,6 @@ public class TextEditingTargetRenameHelper\\n       }\\n       \\n       // Otherwise, just rename the variable within the current scope.\\n-      // TODO: Do we need to look into parent scopes?\\n       return renameVariablesInScope(\\n             editor_.getCurrentScope(),\\n             targetValue,\\nAccording to the diff, the commit message should be:\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T21:07:10.480127Z",
     "start_time": "2024-03-27T21:07:10.201355Z"
    }
   },
   "id": "f0631b8c509117e5",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "efb3f43632915f4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
