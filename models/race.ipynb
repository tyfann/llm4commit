{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T09:56:25.470601Z",
     "start_time": "2024-07-16T09:56:25.354063Z"
    }
   },
   "source": [
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# with open('../data/final_preprocessed_data/js_baseline_test_data.json') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "with open('../data/vdo_filtered/test_data.json') as f:\n",
    "    prompt_data = json.load(f)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-07-16T09:56:25.486160Z",
     "start_time": "2024-07-16T09:56:25.472601Z"
    }
   },
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = './'"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T09:56:31.336943Z",
     "start_time": "2024-07-16T09:56:25.488286Z"
    }
   },
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text2text-generation\", model=\"JetBrains-Research/cmg-race-without-history\", device=0)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T09:27:00.482157Z",
     "start_time": "2024-06-19T09:26:58.598731Z"
    }
   },
   "cell_type": "code",
   "source": "pipe(prompt_data[0]['diff'])[0]['generated_text']",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'add setTimeouts to focus to first node and width'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-07-16T10:01:19.938751Z",
     "start_time": "2024-07-16T09:56:41.291549Z"
    }
   },
   "source": [
    "diffs = []\n",
    "generated_commit_messages = []\n",
    "\n",
    "for commit in prompt_data:\n",
    "    diff = commit['diff']\n",
    "    diffs.append(diff)\n",
    "\n",
    "for diff in tqdm(diffs, total=len(diffs), desc='Generating commit messages'):\n",
    "    prompt = f\"\"\"\n",
    "    The following is a diff which describes the code changes in a commit, Your task is to write a short commit message accordingly.\n",
    "    {diff}\n",
    "    According to the diff, the commit message should be:\n",
    "    \"\"\"\n",
    "    generated_commit_messages.append(pipe(prompt)[0]['generated_text'])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating commit messages:   0%|          | 0/1992 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fef7eca4efab4108b49ea3ac6781d768"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (569 > 512). Running this sequence through the model will result in indexing errors\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\logging\\__init__.py\", line 1100, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\logging\\__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\logging\\__init__.py\", line 678, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\logging\\__init__.py\", line 368, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\asyncio\\windows_events.py\", line 321, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\tyfann\\AppData\\Local\\Temp\\ipykernel_16224\\710242084.py\", line 14, in <module>\n",
      "    generated_commit_messages.append(pipe(prompt)[0]['generated_text'])\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py\", line 167, in __call__\n",
      "    result = super().__call__(*args, **kwargs)\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 1167, in __call__\n",
      "    logger.warning_once(\n",
      "  File \"C:\\Users\\tyfann\\anaconda3\\envs\\llm4commit\\lib\\site-packages\\transformers\\utils\\logging.py\", line 329, in warning_once\n",
      "    self.warning(*args, **kwargs)\n",
      "Message: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\n",
      "Arguments: (<class 'UserWarning'>,)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T10:01:21.677406Z",
     "start_time": "2024-07-16T10:01:21.632633Z"
    }
   },
   "source": [
    "for item, msg in zip(prompt_data, generated_commit_messages):\n",
    "    item['race'] = msg\n",
    "\n",
    "output_file = '../data/vdo_filtered/generation/test_race_v0.json'\n",
    "with open(output_file, 'w', encoding='UTF-8') as f:\n",
    "    json.dump(prompt_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# save msg to a file\n",
    "with open(\"../data/vdo_filtered/generation/test_race_v0.txt\", 'w', encoding='UTF-8') as file:\n",
    "    for item in prompt_data:\n",
    "        file.write(item['race'].replace('\\n', '\\\\n').replace('\\r', '\\\\r') + '\\n')"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:39:16.677536Z",
     "start_time": "2024-06-11T13:39:16.657261Z"
    }
   },
   "outputs": [],
   "source": ""
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T12:28:36.706080Z",
     "start_time": "2024-05-26T12:25:50.763751Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"tyfann/llm4commit-rag\")\n",
    "generated_commit_messages = []\n",
    "for data in tqdm(prompt_data, desc='Generating commit messages'):\n",
    "    messages = prompt.invoke(\n",
    "        {\"context\": data['sim_diff'], \"msg\": data['sim_msg'], \"diff\": data['org_diff']}\n",
    "    ).to_messages()\n",
    "    generated_commit_messages.append(pipe(messages[0].content, max_new_tokens=1000)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T12:34:54.706938Z",
     "start_time": "2024-05-26T12:34:54.615437Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../data/final_preprocessed_data/js_baseline_test_data.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for item, msg in zip(data[:1000], generated_commit_messages):\n",
    "    item['race'] = item['msg']\n",
    "\n",
    "output_file = '../data/final_preprocessed_data/js_baseline/js_baseline_rag_race.json'\n",
    "with open(output_file, 'w', encoding='UTF-8') as f:\n",
    "    json.dump(data[:1000], f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
