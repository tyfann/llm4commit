{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:24:05.915383Z",
     "start_time": "2024-06-20T17:24:05.893831Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def check_angular_convention(msg, has_space=True, strict=True):\n",
    "    types = '((build)|(ci)|(docs)|(feat)|(fix)|(perf)|(refactor)|(style)|(test)|(chore))'\n",
    "    # types = '((perf))'\n",
    "    if has_space:\n",
    "        pattern = f'{types}\\\\s(\\\\((\\\\s|\\\\S)+\\\\)\\\\s)?:\\\\s(\\\\s|\\\\S)+'\n",
    "    else:\n",
    "        pattern = f'{types}(\\\\((\\\\s|\\\\S)+\\\\))?:\\\\s\\\\S+(\\\\s|\\\\S)+'\n",
    "    if not strict:\n",
    "        pattern = '^((build)|(ci)|(docs)|(feat)|(fix)|(perf)|(refactor)|(style)|(test)|(chore))'\n",
    "    return re.match(pattern, msg) is not None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:27:41.686182Z",
     "start_time": "2024-06-20T17:25:56.599411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44d181a56494945ba3c9bb6bbd6b104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JavaScript:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import glob\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "\n",
    "def check_angular_convention(msg, has_space=True, strict=True):\n",
    "    types = '((build)|(ci)|(docs)|(feat)|(fix)|(perf)|(refactor)|(style)|(test)|(chore))'\n",
    "    # types = '((perf))'\n",
    "    if has_space:\n",
    "        pattern = f'{types}\\\\s(\\\\((\\\\s|\\\\S)+\\\\)\\\\s)?:\\\\s(\\\\s|\\\\S)+'\n",
    "    else:\n",
    "        pattern = f'{types}(\\\\((\\\\s|\\\\S)+\\\\))?:\\\\s\\\\S+(\\\\s|\\\\S)+'\n",
    "    if not strict:\n",
    "        pattern = '^((build)|(ci)|(docs)|(feat)|(fix)|(perf)|(refactor)|(style)|(test)|(chore))'\n",
    "    return re.match(pattern, msg) is not None\n",
    "\n",
    "# Define the types for the angular convention\n",
    "angular_types = ['build', 'ci', 'docs', 'feat', 'fix', 'perf', 'refactor', 'style', 'test', 'chore']\n",
    "# angular_types = ['perf']\n",
    "\n",
    "# Create empty dictionaries for each language and type\n",
    "languages = ['JavaScript']\n",
    "dfs = {lang: {atype: [] for atype in angular_types} for lang in languages}\n",
    "\n",
    "# Create a tqdm progress bar for each language\n",
    "bars = {lang: tqdm(total=100000, desc=lang) for lang in languages}  # Total: 10 types * 10000 each\n",
    "\n",
    "folder_path = '../rag/datasets--JetBrains-Research--commit-chronicle/snapshots/5fd076e67b812a9f3d1999e5e40f71715f84bb51/data'  # 文件夹的路径\n",
    "\n",
    "files = glob.glob(os.path.join(folder_path, 'train*')) + glob.glob(os.path.join(folder_path, 'validation*')) + glob.glob(os.path.join(folder_path, 'test*')) \n",
    "\n",
    "for file in files:\n",
    "    df = pq.read_table(file).to_pandas()\n",
    "\n",
    "    # Iterate over each language\n",
    "    for lang in languages[:]:\n",
    "        lang_suffix = '.js'\n",
    "\n",
    "        # Filter rows where language column matches the current language and additional conditions\n",
    "        lang_df = df[(df['language'] == lang) & \n",
    "                     (df['mods'].apply(len) == 1) & \n",
    "                    #  (df['message'].apply(len) <= 150) & \n",
    "                     (df['mods'].apply(lambda x: x[0]['change_type']) == 'MODIFY') &\n",
    "                     (df['mods'].apply(lambda x: len(f\"diff --git a/{x[0]['old_path']} b/{x[0]['new_path']} {x[0]['diff']}\") <= 5000))] \n",
    "                    #  (df['mods'].apply(lambda x: ( (x[0]['old_path'].count(lang_suffix) if x[0]['old_path'] else 0) == 1 and (x[0]['new_path'].count(lang_suffix) if x[0]['new_path'] else 0) == 1 and (x[0]['old_path'].count('.json') if x[0]['old_path'] else 0) == 0 and (x[0]['new_path'].count('.json') if x[0]['new_path'] else 0) == 0 )))]\n",
    "\n",
    "        # Iterate over each row in the filtered DataFrame\n",
    "        for index, row in lang_df.iterrows():\n",
    "            msg = row['message']\n",
    "            if check_angular_convention(msg, has_space=True) or check_angular_convention(msg, has_space=False):\n",
    "                diff = row['mods'][0]\n",
    "                old_path = 'a/' + diff['old_path']\n",
    "                new_path = 'b/' + diff['new_path']\n",
    "                diff_content = diff['diff']  # assume diff_content is an empty string\n",
    "                item = {\n",
    "                    'msg': row['message'],\n",
    "                    'diff': f\"diff --git {old_path} {new_path} {diff_content}\",\n",
    "                    'date': row['date'],\n",
    "                    'repo': row['repo']\n",
    "                }\n",
    "\n",
    "                # Find the type in the message\n",
    "                for atype in angular_types:\n",
    "                    if msg.startswith(atype):\n",
    "                        if len(dfs[lang][atype]) < 10000:\n",
    "                            dfs[lang][atype].append(item)\n",
    "                            bars[lang].update(1)\n",
    "                        break\n",
    "\n",
    "            # Check if all types have reached 1000 rows\n",
    "            if all(len(dfs[lang][atype]) >= 10000 for atype in angular_types):\n",
    "                print(f\"Reached 10000 rows for all types in {lang}\")\n",
    "                languages.remove(lang)  # Remove language from list to avoid further processing\n",
    "                break\n",
    "\n",
    "    # Break out of the loop if all languages have reached the required number of rows\n",
    "    if not languages:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:27:48.461843Z",
     "start_time": "2024-06-20T17:27:48.447803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: JavaScript\n",
      "  Type: build, Count: 1360\n",
      "  Type: ci, Count: 1585\n",
      "  Type: docs, Count: 10000\n",
      "  Type: feat, Count: 10000\n",
      "  Type: fix, Count: 10000\n",
      "  Type: perf, Count: 519\n",
      "  Type: refactor, Count: 7283\n",
      "  Type: style, Count: 1112\n",
      "  Type: test, Count: 4480\n",
      "  Type: chore, Count: 10000\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to hold counts for each type\n",
    "type_counts = {lang: {atype: 0 for atype in angular_types} for lang in languages}\n",
    "\n",
    "# Iterate over the collected data to count occurrences of each type\n",
    "for lang, types in dfs.items():\n",
    "    for atype, items in types.items():\n",
    "        type_counts[lang][atype] = len(items)\n",
    "\n",
    "# Print the counts\n",
    "for lang, counts in type_counts.items():\n",
    "    print(f\"Language: {lang}\")\n",
    "    for atype, count in counts.items():\n",
    "        print(f\"  Type: {atype}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:30:34.598028Z",
     "start_time": "2024-06-20T17:30:32.509821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splitting and saving completed.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import json\n",
    "# Split the data and save to files\n",
    "output_dir = '../data/angular_filtered/subsets'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for lang, types in dfs.items():\n",
    "    for atype, items in types.items():\n",
    "        if items:  # Ensure there are items to split\n",
    "            # Split the data into 1:9 ratio for test and db\n",
    "            db_items, test_items = train_test_split(items, test_size=0.1, random_state=42)\n",
    "            # Save to files\n",
    "            # Save to JSON files\n",
    "            with open(os.path.join(output_dir, f'type_test/{lang}_{atype}_test.json'), 'w') as test_file:\n",
    "                json.dump(test_items, test_file, indent=4)\n",
    "            \n",
    "            with open(os.path.join(output_dir, f'type_db/{lang}_{atype}_db.json'), 'w') as db_file:\n",
    "                json.dump(db_items, db_file, indent=4)\n",
    "\n",
    "print(\"Data splitting and saving completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:33:10.613Z",
     "start_time": "2024-06-20T17:33:10.460784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data re-splitting and saving completed.\n",
      "Counts of each type in test_dev.json:\n",
      "Type: build, Count: 13\n",
      "Type: chore, Count: 100\n",
      "Type: ci, Count: 15\n",
      "Type: docs, Count: 100\n",
      "Type: feat, Count: 100\n",
      "Type: fix, Count: 100\n",
      "Type: perf, Count: 5\n",
      "Type: refactor, Count: 72\n",
      "Type: style, Count: 11\n",
      "Type: test, Count: 44\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the input and output directories\n",
    "input_dir = '../data/angular_filtered/subsets/type_test'\n",
    "output_dir = '../data/angular_filtered/subsets'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize dictionaries to hold the data by type and count occurrences\n",
    "data_by_type = {}\n",
    "type_counts = defaultdict(int)\n",
    "\n",
    "# Read all the test.json files\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith('_test.json'):\n",
    "        _, atype, _ = file_name.split('_')\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        \n",
    "        # Load the JSON data\n",
    "        with open(file_path, 'r') as file:\n",
    "            items = json.load(file)\n",
    "        \n",
    "        for item in items:\n",
    "            item['type'] = atype\n",
    "        \n",
    "        # Organize data by type and count occurrences\n",
    "        if atype not in data_by_type:\n",
    "            data_by_type[atype] = []\n",
    "        data_by_type[atype].extend(items)\n",
    "        type_counts[atype] += len(items)\n",
    "\n",
    "# Initialize lists to hold combined test_dev and test_all items\n",
    "test_dev_items = []\n",
    "test_all_items = []\n",
    "\n",
    "# Split the data by type into test_dev and test_all\n",
    "for atype, items in data_by_type.items():\n",
    "    if items:\n",
    "        # Split the data into 1:9 ratio for test_dev and test_all\n",
    "        dev_items, all_items = train_test_split(items, test_size=0.9, random_state=42)\n",
    "        \n",
    "        # Add the split data to the combined lists\n",
    "        test_dev_items.extend(dev_items)\n",
    "        test_all_items.extend(all_items)\n",
    "\n",
    "# Save the combined data to JSON files\n",
    "with open(os.path.join(output_dir, 'test_dev.json'), 'w') as test_dev_file:\n",
    "    json.dump(test_dev_items, test_dev_file, indent=4)\n",
    "\n",
    "test_all_items.extend(test_dev_items)\n",
    "with open(os.path.join(output_dir, 'test_data.json'), 'w') as test_all_file:\n",
    "    json.dump(test_all_items, test_all_file, indent=4)\n",
    "\n",
    "print(\"Data re-splitting and saving completed.\")\n",
    "\n",
    "# Print the counts of each type in test_dev.json\n",
    "print(\"Counts of each type in test_dev.json:\")\n",
    "for atype, count in type_counts.items():\n",
    "    # Calculate the expected count in test_dev.json\n",
    "    dev_count = count // 10\n",
    "    print(f\"Type: {atype}, Count: {dev_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:40:32.350284Z",
     "start_time": "2024-06-20T17:40:31.076917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging completed.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import json\n",
    "output_dir = '../data/angular_filtered/subsets/type_db'\n",
    "# Find all test JSON files in the output directory\n",
    "test_files = glob.glob(os.path.join(output_dir, '*_db.json'))\n",
    "\n",
    "all_test_items = []\n",
    "\n",
    "# Read and merge all test JSON files\n",
    "for file in test_files:\n",
    "    with open(file, 'r') as f:\n",
    "        items = json.load(f)\n",
    "        all_test_items.extend(items)\n",
    "\n",
    "# Save all merged test items to a single JSON file\n",
    "with open(os.path.join('../data/angular_filtered/subsets', 'db_data.json'), 'w') as test_file:\n",
    "    json.dump(all_test_items, test_file, indent=4)\n",
    "\n",
    "print(\"Merging completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:39:14.747337Z",
     "start_time": "2024-06-20T17:39:14.631760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging and conversion to JSONL completed.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Define the directory containing the JSON files\n",
    "output_dir = '../data/angular_filtered/subsets/classification'\n",
    "\n",
    "# List of types and their corresponding labels\n",
    "types = [\n",
    "    'build', 'ci', 'docs', 'feat', 'fix', \n",
    "    'perf', 'refactor', 'style', 'test', 'chore'\n",
    "]\n",
    "type_label_mapping = {type_name: idx for idx, type_name in enumerate(types)}\n",
    "\n",
    "# Save the type-label mapping to a txt file\n",
    "with open(os.path.join(output_dir, 'type_label_mapping.txt'), 'w') as mapping_file:\n",
    "    for type_name, label in type_label_mapping.items():\n",
    "        mapping_file.write(f'{type_name}: {label}\\n')\n",
    "\n",
    "all_test_items = []\n",
    "\n",
    "# Read and process all test JSON files\n",
    "for type_name in types:\n",
    "    test_file = os.path.join('../data/angular_filtered/subsets/type_test', f'JavaScript_{type_name}_test.json')\n",
    "    if os.path.exists(test_file):\n",
    "        with open(test_file, 'r') as f:\n",
    "            items = json.load(f)\n",
    "            for item in items:\n",
    "                all_test_items.append({\n",
    "                    \"code\": item['diff'],\n",
    "                    \"label\": type_label_mapping[type_name]\n",
    "                })\n",
    "\n",
    "# Shuffle the items to randomize their order\n",
    "random.shuffle(all_test_items)\n",
    "\n",
    "# Save all items to a single JSONL file\n",
    "with open(os.path.join(output_dir, 'test.jsonl'), 'w') as jsonl_file:\n",
    "    for item in all_test_items:\n",
    "        jsonl_file.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(\"Merging and conversion to JSONL completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:39:51.989660Z",
     "start_time": "2024-06-20T17:39:51.008705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splitting and saving to JSONL files completed.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Define the directory containing the JSON files\n",
    "output_dir = '../data/angular_filtered/subsets/classification'\n",
    "\n",
    "# List of types and their corresponding labels\n",
    "types = [\n",
    "    'build', 'ci', 'docs', 'feat', 'fix', \n",
    "    'perf', 'refactor', 'style', 'test', 'chore'\n",
    "]\n",
    "type_label_mapping = {type_name: idx for idx, type_name in enumerate(types)}\n",
    "\n",
    "train_items = []\n",
    "valid_items = []\n",
    "\n",
    "# Read and process all test JSON files\n",
    "for type_name in types:\n",
    "    test_file = os.path.join('../data/angular_filtered/subsets/type_db', f'JavaScript_{type_name}_db.json')\n",
    "    if os.path.exists(test_file):\n",
    "        with open(test_file, 'r') as f:\n",
    "            items = json.load(f)\n",
    "            data = [\n",
    "                {\n",
    "                    \"code\": item['diff'],\n",
    "                    \"label\": type_label_mapping[type_name]\n",
    "                }\n",
    "                for item in items\n",
    "            ]\n",
    "            # Split data into 1:8 ratio\n",
    "            train_split, valid_split = train_test_split(data, test_size=1/9, random_state=42)\n",
    "            valid_items.extend(valid_split)\n",
    "            train_items.extend(train_split)\n",
    "\n",
    "# Shuffle the items to randomize their order\n",
    "random.shuffle(valid_items)\n",
    "random.shuffle(train_items)\n",
    "\n",
    "# Save validation items to valid.jsonl\n",
    "with open(os.path.join(output_dir, 'valid.jsonl'), 'w') as valid_file:\n",
    "    for item in valid_items:\n",
    "        valid_file.write(json.dumps(item) + '\\n')\n",
    "\n",
    "# Save training items to train.jsonl\n",
    "with open(os.path.join(output_dir, 'train.jsonl'), 'w') as train_file:\n",
    "    for item in train_items:\n",
    "        train_file.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(\"Data splitting and saving to JSONL files completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# Initialize a dictionary to count the number of items for each type\n",
    "type_counts = defaultdict(int)\n",
    "\n",
    "# Count the number of items for each type\n",
    "for item in valid_items:\n",
    "    label = item[\"label\"]\n",
    "    type_name = types[label]\n",
    "    type_counts[type_name] += 1\n",
    "\n",
    "# Output the counts to a file\n",
    "count_output_file = os.path.join(output_dir, 'valid_type_counts.txt')\n",
    "with open(count_output_file, 'w') as f:\n",
    "    for type_name, count in type_counts.items():\n",
    "        f.write(f'{type_name}: {count}\\n')\n",
    "print(\"Item counts for each type in the training data have been computed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to hold counts for each type\n",
    "type_counts = {lang: {atype: 0 for atype in angular_types} for lang in languages}\n",
    "\n",
    "# Iterate over the collected data to count occurrences of each type\n",
    "for lang, types in dfs.items():\n",
    "    for atype, items in types.items():\n",
    "        type_counts[lang][atype] = len(items)\n",
    "\n",
    "# Print the counts\n",
    "for lang, counts in type_counts.items():\n",
    "    print(f\"Language: {lang}\")\n",
    "    for atype, count in counts.items():\n",
    "        print(f\"  Type: {atype}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to hold max diff length for each language\n",
    "max_diff_lengths = {lang: {atype: 0 for atype in angular_types} for lang in dfs.keys()}\n",
    "\n",
    "# Iterate over the collected data to find the max diff length for each type\n",
    "for lang, types in dfs.items():\n",
    "    for atype, items in types.items():\n",
    "        if items:  # Ensure there are items to check\n",
    "            max_diff_lengths[lang][atype] = max(len(item['diff']) for item in items)\n",
    "\n",
    "# Print the max diff lengths\n",
    "for lang, lengths in max_diff_lengths.items():\n",
    "    print(f\"Language: {lang}\")\n",
    "    for atype, length in lengths.items():\n",
    "        print(f\"  Type: {atype}, Max Diff Length: {length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T17:45:00.899431Z",
     "start_time": "2024-06-20T17:45:00.847861Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging and conversion to JSONL completed.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "# List of types and their corresponding labels\n",
    "types = [\n",
    "    'build', 'ci', 'docs', 'feat', 'fix', \n",
    "    'perf', 'refactor', 'style', 'test', 'chore'\n",
    "]\n",
    "type_label_mapping = {type_name: idx for idx, type_name in enumerate(types)}\n",
    "all_test_items = []\n",
    "\n",
    "# Read and process all test JSON files\n",
    "test_file = os.path.join('../data/angular_filtered/subsets', 'dev_test.json')\n",
    "if os.path.exists(test_file):\n",
    "    with open(test_file, 'r') as f:\n",
    "        items = json.load(f)\n",
    "        for item in items:\n",
    "            all_test_items.append({\n",
    "                \"code\": item['diff'],\n",
    "                \"label\": type_label_mapping[item['type']]\n",
    "            })\n",
    "\n",
    "# Shuffle the items to randomize their order\n",
    "random.shuffle(all_test_items)\n",
    "\n",
    "# Save all items to a single JSONL file\n",
    "with open(os.path.join('../data/angular_filtered/subsets/classification', 'angular_dev_test.jsonl'), 'w') as jsonl_file:\n",
    "    for item in all_test_items:\n",
    "        jsonl_file.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(\"Merging and conversion to JSONL completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm4commit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
