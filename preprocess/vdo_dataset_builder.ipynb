{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T08:53:58.472310Z",
     "start_time": "2024-06-25T08:53:56.746271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import json\n",
    "from utils import is_vdo_pattern, is_merge_rollback, tokenize_summary, remove_brackets, replace_issue_id"
   ],
   "id": "aee1d4707d51bcc5",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T08:53:59.499425Z",
     "start_time": "2024-06-25T08:53:58.474575Z"
    }
   },
   "cell_type": "code",
   "source": "nlp = StanfordCoreNLP('http://localhost', port=9000)",
   "id": "7c5b228e274d340b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T08:54:00.804204Z",
     "start_time": "2024-06-25T08:54:00.790665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def commit_processer(msg, nlp):\n",
    "    ## get the first sentence\n",
    "    ## remove issue id\n",
    "    ## remove merge, rollback commits and commits with a diff larger than 1 mb\n",
    "    ## broke reference messages into tokens\n",
    "    ## Max length for summary. Default is 30.\n",
    "    msg = sent_tokenize(msg.strip().replace('\\n', '. '))\n",
    "    if msg is None or msg == []:\n",
    "        return '', 0\n",
    "    first_sent = msg[0]\n",
    "    if is_merge_rollback(first_sent):\n",
    "        return '', 0\n",
    "    else:\n",
    "        first_sent = replace_issue_id(first_sent)\n",
    "        # first_sent = remove_brackets(first_sent)\n",
    "        if first_sent is None or first_sent == '':\n",
    "            return '', 0\n",
    "        first_sent = tokenize_summary(first_sent)\n",
    "        if len(first_sent.split()) > 30 or not is_vdo_pattern(first_sent, nlp):\n",
    "            return '', 0\n",
    "        else:\n",
    "            return first_sent, 1"
   ],
   "id": "6f4f30a31355f071",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T08:54:21.659483Z",
     "start_time": "2024-06-25T08:54:21.646340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def to_lemma(msg, nlp):\n",
    "    props={'annotators': 'lemma', 'outputFormat': 'json', 'timeout': 1000}\n",
    "    msg_list = msg.split()\n",
    "    annot_doc = nlp.annotate(msg,properties=props)\n",
    "    parsed_dict = json.loads(annot_doc)\n",
    "    lemma_list = [v for d in parsed_dict['sentences'][0]['tokens'] for k, v in d.items() if k == 'lemma']\n",
    "    msg_list[0] = lemma_list[0]\n",
    "    msg = ' '.join(msg_list)\n",
    "    return msg"
   ],
   "id": "6319a2db7390983f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-25T10:13:35.971349Z",
     "start_time": "2024-06-25T08:56:03.852524Z"
    }
   },
   "source": [
    "import pyarrow.parquet as pq\n",
    "import glob\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "# Create empty dictionaries for each language and type\n",
    "languages = ['JavaScript']\n",
    "verb_groups = ['add', 'fix', 'remove', 'update', 'use', 'move', 'prepare', 'improve', 'ignore', 'handle', 'rename', 'allow', 'set', 'revert', 'replace']\n",
    "\n",
    "dfs = {lang: {vtype: [] for vtype in verb_groups} for lang in languages}\n",
    "\n",
    "# Create a tqdm progress bar for each language\n",
    "bars = {lang: tqdm(total=60000, desc=lang) for lang in languages}  # Total: 10 types * 10000 each\n",
    "\n",
    "folder_path = '../rag/datasets--JetBrains-Research--commit-chronicle/snapshots/5fd076e67b812a9f3d1999e5e40f71715f84bb51/data'  # 文件夹的路径\n",
    "\n",
    "\n",
    "files = glob.glob(os.path.join(folder_path, 'train*')) + glob.glob(os.path.join(folder_path, 'validation*')) + glob.glob(os.path.join(folder_path, 'test*')) \n",
    "\n",
    "for file in files:\n",
    "    df = pq.read_table(file).to_pandas()\n",
    "\n",
    "    # Iterate over each language\n",
    "    for lang in languages[:]:\n",
    "        # Filter rows where language column matches the current language and additional conditions\n",
    "        lang_df = df[(df['language'] == lang) & \n",
    "                     (df['mods'].apply(len) == 1) & \n",
    "                    #  (df['message'].apply(len) <= 150) & \n",
    "                     (df['mods'].apply(lambda x: x[0]['change_type']) == 'MODIFY') &\n",
    "                     (df['mods'].apply(lambda x: len(f\"diff --git a/{x[0]['old_path']} b/{x[0]['new_path']} {x[0]['diff']}\") <= 5000))] \n",
    "                    #  (df['mods'].apply(lambda x: ( (x[0]['old_path'].count(lang_suffix) if x[0]['old_path'] else 0) == 1 and (x[0]['new_path'].count(lang_suffix) if x[0]['new_path'] else 0) == 1 and (x[0]['old_path'].count('.json') if x[0]['old_path'] else 0) == 0 and (x[0]['new_path'].count('.json') if x[0]['new_path'] else 0) == 0 )))]\n",
    "\n",
    "        # Iterate over each row in the filtered DataFrame\n",
    "        for index, row in lang_df.iterrows():\n",
    "            msg = row['message']\n",
    "            msg = to_lemma(msg, nlp)\n",
    "            if msg.split()[0] in verb_groups:\n",
    "                if commit_processer(msg, nlp)[1] == 1:\n",
    "                    diff = row['mods'][0]\n",
    "                    old_path = 'a/' + diff['old_path']\n",
    "                    new_path = 'b/' + diff['new_path']\n",
    "                    diff_content = diff['diff']  # assume diff_content is an empty string\n",
    "                    item = {\n",
    "                        'msg': msg,\n",
    "                        'diff': f\"diff --git {old_path} {new_path} {diff_content}\",\n",
    "                        'date': row['date'],\n",
    "                        'repo': row['repo']\n",
    "                    }\n",
    "                    \n",
    "                    # Find the type in the message\n",
    "                    for vtype in verb_groups:\n",
    "                        if msg.startswith(vtype):\n",
    "                            if len(dfs[lang][vtype]) < 4000:\n",
    "                                dfs[lang][vtype].append(item)\n",
    "                                bars[lang].update(1)\n",
    "                            break\n",
    "        \n",
    "            if all(len(dfs[lang][vtype]) >= 4000 for vtype in verb_groups):\n",
    "                print(f\"Reached 4000 rows for all types in {lang}\")\n",
    "                languages.remove(lang)  # Remove language from list to avoid further processing\n",
    "                break\n",
    "\n",
    "    # Break out of the loop if all languages have reached the required number of rows\n",
    "    if not languages:\n",
    "        break"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JavaScript:   0%|          | 0/60000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "719586d63fe540a5946b138d6d8b3189"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T10:29:37.596735Z",
     "start_time": "2024-06-25T10:29:37.583402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for lang in dfs:\n",
    "    print(f\"Language: {lang}\")\n",
    "    for vtype in dfs[lang]:\n",
    "        print(f\"  {vtype}: {len(dfs[lang][vtype])}\")\n"
   ],
   "id": "636a03db225aa00a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: JavaScript\n",
      "  add: 4000\n",
      "  fix: 4000\n",
      "  remove: 4000\n",
      "  update: 4000\n",
      "  use: 2025\n",
      "  move: 591\n",
      "  prepare: 299\n",
      "  improve: 3899\n",
      "  ignore: 727\n",
      "  handle: 1526\n",
      "  rename: 1597\n",
      "  allow: 2117\n",
      "  set: 2330\n",
      "  revert: 824\n",
      "  replace: 1786\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T10:13:36.017392Z",
     "start_time": "2024-06-25T10:13:35.980878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = []\n",
    "\n",
    "for lang, types in dfs.items():\n",
    "    for vtype, items in types.items():\n",
    "        for item in items:\n",
    "            # item['msg'] = to_lemma(item['msg'], nlp)\n",
    "            data.append(item)"
   ],
   "id": "1cb1b00ea059323a",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T10:13:36.496519Z",
     "start_time": "2024-06-25T10:13:36.020394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "# Dump the data to a JSON file\n",
    "with open('../data/vdo_filtered/lemma_data_js_new.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(data, f, indent=4)"
   ],
   "id": "ee21121f6d6e4648",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T21:58:45.242067Z",
     "start_time": "2024-06-18T21:58:45.226063Z"
    }
   },
   "cell_type": "code",
   "source": "dfs[lang]['add'][0]",
   "id": "6ad31736416c3c3a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'msg': 'Adds an config sample',\n",
       " 'diff': 'diff --git a/test/stubs/.eleventyignore b/test/stubs/.eleventyignore ignoredFolder\\n-ignoredFolder/ignored.md\\n\\\\ No newline at end of file\\n+./ignoredFolder/ignored.md\\n\\\\ No newline at end of file\\n',\n",
       " 'date': '10.12.2017 12:41:51',\n",
       " 'repo': '11ty/eleventy'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T15:27:42.585747Z",
     "start_time": "2024-06-18T15:27:42.508045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# 读取JSON文件\n",
    "with open('../data/vdo_filtered/test_data_js.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "first_words = [item['msg'].split()[0].lower() for item in data if 'msg' in item]\n",
    "\n",
    "word_counts = Counter(first_words)\n",
    "\n",
    "top_10_words = word_counts.most_common(10)\n",
    "for word, count in top_10_words:\n",
    "    print(f'{word} {count}')"
   ],
   "id": "fbec3895fce07299",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add 3227\n",
      "remove 952\n",
      "added 592\n",
      "update 394\n",
      "fix 215\n",
      "adding 182\n",
      "updated 172\n",
      "removed 170\n",
      "improve 156\n",
      "replace 117\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T19:33:33.445343Z",
     "start_time": "2024-06-18T19:33:03.843916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm.auto import tqdm\n",
    "props={'annotators': 'lemma', 'outputFormat': 'json', 'timeout': 1000 }\n",
    "with open('../data/vdo_filtered/test_data_js.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "for item in tqdm(data, total=len(data)):\n",
    "    msg = item['msg']\n",
    "    msg_list = msg.split()\n",
    "    annot_doc = nlp.annotate(msg,properties=props)\n",
    "    parsed_dict = json.loads(annot_doc)\n",
    "    lemma_list = [v for d in parsed_dict['sentences'][0]['tokens'] for k, v in d.items() if k == 'lemma']\n",
    "    msg_list[0] = lemma_list[0]\n",
    "    msg = ' '.join(msg_list)\n",
    "    item['msg'] = msg"
   ],
   "id": "fb952b25659d4aea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7444573c45b644d0b3fab4742f6f8e1d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T12:20:43.550389Z",
     "start_time": "2024-06-22T12:20:43.437068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def extract_random_items(input_file, output_file, num_items_per_type=10):\n",
    "    # 读取 JSON 文件\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # 分类 items\n",
    "    item_dict = {}\n",
    "    for item in data:\n",
    "        msg = item['msg']\n",
    "        first_word = msg.split()[0] if msg else ''\n",
    "        if first_word not in item_dict:\n",
    "            item_dict[first_word] = []\n",
    "        item_dict[first_word].append(item)\n",
    "    \n",
    "    # 随机选择每种类型的 items\n",
    "    selected_items = []\n",
    "    for key in item_dict:\n",
    "        if len(item_dict[key]) > num_items_per_type:\n",
    "            selected_items.extend(random.sample(item_dict[key], num_items_per_type))\n",
    "        else:\n",
    "            selected_items.extend(item_dict[key])\n",
    "    \n",
    "    # 写入新的 JSON 文件\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(selected_items, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 调用函数\n",
    "extract_random_items('../data/vdo_filtered/lemma_data_js.json', '../data/vdo_filtered/lemma_dev_test_150.json')"
   ],
   "id": "6cd5a4f17ec52280",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T15:17:43.183144Z",
     "start_time": "2024-06-26T15:17:41.345516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "# split data into train, dev, test according to the type of commit message(8:1:1)\n",
    "\n",
    "import json\n",
    "with open('../data/vdo_filtered/lemma_data_js_new.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "data_dict = defaultdict(list)\n",
    "for item in data:\n",
    "    msg = item['msg']\n",
    "    first_word = msg.split()[0] if msg else ''\n",
    "    data_dict[first_word].append(item)"
   ],
   "id": "b3f40ce0a370603e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T15:18:20.483660Z",
     "start_time": "2024-06-26T15:18:20.465659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for key, items in data_dict.items():\n",
    "    print(len(items))\n",
    "    break"
   ],
   "id": "9673660020190faa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T15:32:24.426034Z",
     "start_time": "2024-06-26T15:32:23.820466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split the data and save to files\n",
    "import os\n",
    "output_dir = '../data/vdo_filtered/classification'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "train_all = []\n",
    "dev_all = []\n",
    "test_all = []\n",
    "for key, items in data_dict.items():\n",
    "    train_data, dev_test_data = train_test_split(items, test_size=0.2, random_state=42)\n",
    "    dev_data, test_data = train_test_split(dev_test_data, test_size=0.5, random_state=42)\n",
    "    \n",
    "    train_all.extend(train_data)\n",
    "    dev_all.extend(dev_data)\n",
    "    test_all.extend(test_data)\n",
    "\n",
    "# Save the data to files\n",
    "with open(os.path.join(output_dir, 'train_data.json'), 'w', encoding='utf-8') as file:\n",
    "    json.dump(train_all, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(os.path.join(output_dir, 'dev_data.json'), 'w', encoding='utf-8') as file:\n",
    "    json.dump(dev_all, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(os.path.join(output_dir, 'test_data.json'), 'w', encoding='utf-8') as file:\n",
    "    json.dump(test_all, file, ensure_ascii=False, indent=4)"
   ],
   "id": "4e4cb3d000eb8bd4",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T15:32:33.344800Z",
     "start_time": "2024-06-26T15:32:33.324805Z"
    }
   },
   "cell_type": "code",
   "source": "len(train_all), len(dev_all), len(test_all)",
   "id": "16bf4c52ceefa437",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26972, 3373, 3376)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T15:37:21.877524Z",
     "start_time": "2024-06-26T15:37:21.382361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db_data = []\n",
    "db_data.extend(train_all)\n",
    "db_data.extend(dev_all)\n",
    "with open(os.path.join('../data/vdo_filtered', 'db_data.json'), 'w', encoding='utf-8') as file:\n",
    "    json.dump(db_data, file, ensure_ascii=False, indent=4)"
   ],
   "id": "873dee5079c23e46",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T15:38:08.892235Z",
     "start_time": "2024-06-26T15:38:08.863236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print the number of each type in the test data\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# 读取JSON文件\n",
    "with open('../data/vdo_filtered/classification/test_data.json', 'r', encoding='utf-8') as file:\n",
    "    test_data = json.load(file)\n"
   ],
   "id": "66f20c80b4e624e2",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T15:40:08.062564Z",
     "start_time": "2024-06-26T15:40:08.032566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dev_test = []\n",
    "data_dict = defaultdict(list)\n",
    "for item in test_data:\n",
    "    msg = item['msg']\n",
    "    first_word = msg.split()[0] if msg else ''\n",
    "    data_dict[first_word].append(item)\n",
    "\n",
    "for key, items in data_dict.items():\n",
    "    train_data, dev_test_data = train_test_split(items, test_size=1/6, random_state=42)\n",
    "    dev_test.extend(dev_test_data)\n",
    "\n",
    "with open('../data/vdo_filtered/dev_test_data.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(dev_test, file, ensure_ascii=False, indent=4)"
   ],
   "id": "5d927e398c89b326",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T15:48:15.311438Z",
     "start_time": "2024-06-26T15:48:15.297441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Define the directory containing the JSON files\n",
    "output_dir = '../data/vdo_filtered/classification'\n",
    "\n",
    "# List of types and their corresponding labels\n",
    "types = [\n",
    "    'add', 'fix', 'remove', 'update', 'use', \n",
    "    'move', 'prepare', 'improve', 'ignore', 'handle',\n",
    "    'rename', 'allow', 'set', 'revert', 'replace'\n",
    "]\n",
    "type_label_mapping = {type_name: idx for idx, type_name in enumerate(types)}\n",
    "\n",
    "# Save the type-label mapping to a txt file\n",
    "with open(os.path.join(output_dir, 'type_label_mapping.txt'), 'w') as mapping_file:\n",
    "    for type_name, label in type_label_mapping.items():\n",
    "        mapping_file.write(f'{type_name}: {label}\\n')"
   ],
   "id": "1805d6e3ee4afc91",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:03:47.232934Z",
     "start_time": "2024-06-26T16:03:47.199861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# assign type label to train, dev, test data\n",
    "import json\n",
    "import os\n",
    "\n",
    "with open('../data/vdo_filtered/dev_test_data.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for item in data:\n",
    "    item['type'] = item['msg'].split()[0]\n",
    "\n",
    "with open('../data/vdo_filtered/dev_test_data.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(data, file, ensure_ascii=False, indent=4)"
   ],
   "id": "836274ffad136126",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T09:44:20.177335Z",
     "start_time": "2024-06-27T09:44:19.573760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# 读取JSON文件\n",
    "input_file = '../data/vdo_filtered/db_data.json'\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 创建存储分类文件的文件夹\n",
    "output_folder = '../data/vdo_filtered/type_db'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 初始化一个字典来存储不同类型的数据\n",
    "type_data = {}\n",
    "\n",
    "# 遍历所有数据，根据type字段分类\n",
    "for item in data:\n",
    "    item_type = item['type']\n",
    "    if item_type not in type_data:\n",
    "        type_data[item_type] = []\n",
    "    type_data[item_type].append(item)\n",
    "\n",
    "# 将分类后的数据分别写入不同的JSON文件\n",
    "for item_type, items in type_data.items():\n",
    "    output_file = os.path.join(output_folder, f'{item_type}_db.json')\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(items, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"分类完成并已保存到type_db文件夹下。\")"
   ],
   "id": "65f14cb38bba6cb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类完成并已保存到type_db文件夹下。\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b83e1ea7f809bb6a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
