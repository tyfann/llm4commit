{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSV1X4nOoF9x"
   },
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install jq\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JmawYD7kGNLZ",
    "ExecuteTime": {
     "end_time": "2024-05-03T13:59:13.903886Z",
     "start_time": "2024-05-03T13:59:13.347701Z"
    }
   },
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import pickle\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6j2XMd6gCdQE",
    "ExecuteTime": {
     "end_time": "2024-05-03T13:59:17.665811Z",
     "start_time": "2024-05-03T13:59:17.525946Z"
    }
   },
   "source": [
    "with open(\"../data/chronicle/rag_msg.pkl\", \"rb\") as f:\n",
    "    msg_data = pickle.load(f)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T13:59:37.893817Z",
     "start_time": "2024-05-03T13:59:18.526657Z"
    }
   },
   "source": [
    "diff_loader = JSONLoader(\n",
    "    file_path='../data/chronicle/chronicle_rag_db.json',\n",
    "    jq_schema='.[].diff',\n",
    "    text_content=False)\n",
    "\n",
    "diff_data = diff_loader.load()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "language_loader = JSONLoader(\n",
    "    file_path='../data/chronicle/chronicle_rag_db.json',\n",
    "    jq_schema='.[].lang',\n",
    "    text_content=False)\n",
    "\n",
    "language_data = language_loader.load()"
   ],
   "metadata": {
    "id": "ltaWG7hinsp4",
    "ExecuteTime": {
     "end_time": "2024-05-03T13:59:54.755521Z",
     "start_time": "2024-05-03T13:59:37.911783Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R4qftc5k1bOh",
    "ExecuteTime": {
     "end_time": "2024-05-03T13:59:54.770674Z",
     "start_time": "2024-05-03T13:59:54.756466Z"
    }
   },
   "source": [
    "# 1-1000 data use java splitter, 1000-2000 use cpp splitter, 2000-3000 use csharp splitter, 3000-4000 use python splitter, 4000-5000 use javascript splitter\n",
    "\n",
    "\n",
    "# languages = [Language.JAVA, Language.CPP, Language.CSHARP, Language.PYTHON, Language.JS]\n",
    "languages = [Language.PYTHON, Language.JS]\n",
    "splitters = [RecursiveCharacterTextSplitter.from_language(language, chunk_size=500, chunk_overlap=50) for language in languages]\n",
    "\n",
    "# language_dict = {'java': 0, 'c++': 1, 'c#': 2, 'python': 3, 'javascript': 4}\n",
    "language_dict = {'Python': 0, 'JavaScript': 1}"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T14:06:14.065771Z",
     "start_time": "2024-05-03T14:06:14.051606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def similarity_search(documents, db):\n",
    "    # Initialize an empty dictionary to store aggregate scores for each candidate ID\n",
    "    aggregate_scores = {}\n",
    "\n",
    "    # Iterate through each document in the documents list\n",
    "    for document in documents:\n",
    "        # Apply similarity search function to the document\n",
    "        results = db.similarity_search_with_relevance_scores(document.page_content, score_threshold=0.0)\n",
    "        # Iterate through the results for each document\n",
    "        for candidate_doc in results:\n",
    "            id = candidate_doc[0].metadata['seq_num']\n",
    "            score = candidate_doc[1]\n",
    "            # Update the aggregate score for the candidate ID\n",
    "            aggregate_scores[id] = aggregate_scores.get(id, 0) + score\n",
    "\n",
    "    # Find the candidate ID with the highest aggregate score\n",
    "    if aggregate_scores:\n",
    "        max_candidate_id = max(aggregate_scores, key=aggregate_scores.get)\n",
    "    else:\n",
    "        max_candidate_id = -1\n",
    "\n",
    "    return max_candidate_id"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LJ1DFYLKnsp6",
    "outputId": "fe0afd44-16c4-4be2-bde4-b99cb0cfb9f8",
    "ExecuteTime": {
     "end_time": "2024-05-02T13:18:08.174244Z",
     "start_time": "2024-05-02T13:18:02.585227Z"
    }
   },
   "source": [
    "diff_split = []\n",
    "for i, doc in tqdm(enumerate(diff_data), total=len(diff_data), desc=\"Processing documents\"):\n",
    "    if language_data[i].page_content=='Python':\n",
    "        # diff_split.append(doc)\n",
    "        diff_split += splitters[language_dict[language_data[i].page_content]].split_documents([doc])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Processing documents:   0%|          | 0/500000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1867bf912f794b04ac60e88287bb0179"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the path to the pre-trained model you want to use\n",
    "modelPath = \"mixedbread-ai/mxbai-embed-large-v1\"\n",
    "\n",
    "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
    "model_kwargs = {'device':'cuda', 'trust_remote_code': True}\n",
    "\n",
    "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    cache_folder = '../models',\n",
    "    \n",
    "    model_name=modelPath,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T14:00:06.965480Z",
     "start_time": "2024-05-03T13:59:57.948720Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_diff_loader = JSONLoader(\n",
    "    file_path='../data/chronicle/rag_dev/chronicle_test_data_dev.json',\n",
    "    jq_schema='.[].diff',\n",
    "    text_content=False)\n",
    "\n",
    "test_diff_data = test_diff_loader.load()\n",
    "\n",
    "with open('../data/chronicle/rag_dev/chronicle_test_data_dev.json', 'r') as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Python part"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "db = Chroma.from_documents(diff_split, embeddings, persist_directory=\"./chroma_chronicle_db_mxbai_500chunk_normalized_python\")"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T14:08:11.640263Z",
     "start_time": "2024-05-03T14:08:11.626748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "python_test_data = []\n",
    "python_indices = []\n",
    "for i, item in enumerate(test_data):\n",
    "    if item['lang'] == 'Python':\n",
    "        python_test_data.append(item)\n",
    "        python_indices.append(i)"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T14:09:34.959631Z",
     "start_time": "2024-05-03T14:08:26.570272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db = Chroma(persist_directory=\"./chroma_chronicle_db_mxbai_500chunk_normalized_python\", embedding_function=embeddings)\n",
    "similar_diff = []\n",
    "for index in tqdm(python_indices, total=len(python_indices), desc=\"Processing documents\"):\n",
    "    documents = splitters[0].split_documents([test_diff_data[index]])\n",
    "    similar_diff.append(similarity_search(documents, db))\n",
    "\n",
    "data = []\n",
    "\n",
    "for sim_diff, test_diff in zip(similar_diff, python_test_data):\n",
    "    item = {'org_diff': test_diff['diff'],\n",
    "            'org_msg': test_diff['msg'],\n",
    "            'sim_msg': msg_data[sim_diff],\n",
    "            'sim_diff': diff_data[sim_diff-1].page_content}\n",
    "    data.append(item)\n",
    "    \n",
    "# Write the data to a JSON file\n",
    "with open('../data/chronicle/rag_baseline/rag_baseline_python.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Processing documents:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6d34f0a4c61e433a96ad566157fdda6b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# JavaScript part"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T14:12:28.878729Z",
     "start_time": "2024-05-03T14:12:28.871560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "js_test_data = []\n",
    "js_indices = []\n",
    "for i, item in enumerate(test_data):\n",
    "    if item['lang'] == 'JavaScript':\n",
    "        js_test_data.append(item)\n",
    "        js_indices.append(i)"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T07:49:01.303681Z",
     "start_time": "2024-05-06T07:49:00.632183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db = Chroma(persist_directory=\"./chroma_chronicle_db_mxbai_500chunk_normalized_js\", embedding_function=embeddings)\n",
    "similar_diff = []\n",
    "for index in tqdm(js_indices, total=len(js_indices), desc=\"Processing documents\"):\n",
    "    documents = splitters[1].split_documents([test_diff_data[index]])\n",
    "    similar_diff.append(similarity_search(documents, db))\n",
    "\n",
    "data = []\n",
    "\n",
    "for sim_diff, test_diff in zip(similar_diff, js_test_data):\n",
    "    item = {'org_diff': test_diff['diff'],\n",
    "            'org_msg': test_diff['msg'],\n",
    "            'sim_msg': msg_data[sim_diff],\n",
    "            'sim_diff': diff_data[sim_diff-1].page_content}\n",
    "    data.append(item)\n",
    "    \n",
    "# Write the data to a JSON file\n",
    "with open('../data/chronicle/rag_baseline/rag_baseline_js.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Chroma' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m db \u001B[38;5;241m=\u001B[39m \u001B[43mChroma\u001B[49m(persist_directory\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./chroma_chronicle_db_mxbai_500chunk_normalized_js\u001B[39m\u001B[38;5;124m\"\u001B[39m, embedding_function\u001B[38;5;241m=\u001B[39membeddings)\n\u001B[0;32m      2\u001B[0m similar_diff \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index \u001B[38;5;129;01min\u001B[39;00m tqdm(js_indices, total\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(js_indices), desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mProcessing documents\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "\u001B[1;31mNameError\u001B[0m: name 'Chroma' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T15:42:10.713727Z",
     "start_time": "2024-05-02T14:34:01.942274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "diff_split = []\n",
    "for i, doc in tqdm(enumerate(diff_data), total=len(diff_data), desc=\"Processing documents\"):\n",
    "    if language_data[i].page_content=='JavaScript':\n",
    "        # diff_split.append(doc)\n",
    "        diff_split += splitters[language_dict[language_data[i].page_content]].split_documents([doc])\n",
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "db = Chroma.from_documents(diff_split, embeddings, persist_directory=\"./chroma_chronicle_db_mxbai_500chunk_normalized_js\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Processing documents:   0%|          | 0/500000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b7361d3093ae4b7d8ae46dcd7eb3b8dc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Processing documents:   0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a86e9cbd29cd4f0e8b8f1e188fcb53b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 56,
   "source": [
    "similar_diff = []\n",
    "for i, test_data in tqdm(enumerate(test_diff_data[:100]), total=len(test_diff_data[:100]), desc=\"Processing documents\"):\n",
    "    query = test_data.page_content\n",
    "    similar_diff.append(diff_data[retriever.get_relevant_documents(query)[0].metadata['seq_num']-1])\n",
    "\n",
    "data = []\n",
    "\n",
    "for sim_diff, test_diff in zip(similar_diff, test_diff_data[:100]):\n",
    "    item = {\n",
    "        'sim_msg': msg_data[sim_diff.metadata['seq_num']],\n",
    "        'sim_diff': sim_diff.page_content,\n",
    "        'org_diff': test_diff.page_content\n",
    "    }\n",
    "    data.append(item)\n",
    "\n",
    "# Write the data to a JSON file\n",
    "with open('../data/chronicle/data_with_similar_diff_codebert.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Processing documents:   0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ead493dee85e49d4a9ffff8f99466ac1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "similar_diff = []\n",
    "for i, test_data in tqdm(enumerate(test_diff_data[:100]), total=len(test_diff_data[:100]), desc=\"Processing documents\"):\n",
    "    query = test_data.page_content\n",
    "    similar_diff.append(diff_data[retriever_2.get_relevant_documents(query)[0].metadata['seq_num']-1])\n",
    "\n",
    "data = []\n",
    "\n",
    "for sim_diff, test_diff in zip(similar_diff, test_diff_data[:100]):\n",
    "    item = {\n",
    "        'sim_msg': msg_data[sim_diff.metadata['seq_num']],\n",
    "        'sim_diff': sim_diff.page_content,\n",
    "        'org_diff': test_diff.page_content\n",
    "    }\n",
    "    data.append(item)\n",
    "\n",
    "# Write the data to a JSON file\n",
    "with open('../data/chronicle/data_with_similar_diff_mxbai.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T18:00:44.062169Z",
     "start_time": "2024-04-25T18:00:39.793930Z"
    }
   },
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "BTU8eiRk1bOm",
    "ExecuteTime": {
     "end_time": "2024-04-25T17:49:40.232903Z",
     "start_time": "2024-04-25T17:49:40.205900Z"
    }
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for sim_diff, test_diff in zip(similar_diff, test_diff_data):\n",
    "    item = {\n",
    "        'sim_msg': msg_data[sim_diff.metadata['seq_num']],\n",
    "        'sim_diff': sim_diff.page_content,\n",
    "        'org_diff': test_diff.page_content\n",
    "    }\n",
    "    data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "aVxF1tkQ1bOn",
    "ExecuteTime": {
     "end_time": "2024-04-25T17:49:43.781387Z",
     "start_time": "2024-04-25T17:49:43.722376Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write the data to a JSON file\n",
    "with open('../data/chronicle/data_with_similar_diff_codebert.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
