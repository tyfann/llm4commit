{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "mSV1X4nOoF9x",
    "ExecuteTime": {
     "end_time": "2024-05-14T19:31:25.483515Z",
     "start_time": "2024-05-14T19:31:16.252578Z"
    }
   },
   "source": [
    "!pip install langchain\n",
    "!pip install jq\n",
    "!pip install sentence-transformers"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JmawYD7kGNLZ",
    "ExecuteTime": {
     "end_time": "2024-05-27T21:27:46.412238Z",
     "start_time": "2024-05-27T21:27:45.678899Z"
    }
   },
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import pickle\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6j2XMd6gCdQE",
    "ExecuteTime": {
     "end_time": "2024-05-14T19:31:26.199475Z",
     "start_time": "2024-05-14T19:31:26.075287Z"
    }
   },
   "source": [
    "with open(\"../data/chronicle/rag_msg.pkl\", \"rb\") as f:\n",
    "    msg_data = pickle.load(f)"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T13:56:01.983818Z",
     "start_time": "2024-05-27T13:56:01.658563Z"
    }
   },
   "source": [
    "diff_loader = JSONLoader(\n",
    "    file_path='../data/final_preprocessed_data/js_rag_db_data_300.json',\n",
    "    jq_schema='.[].diff',\n",
    "    text_content=False)\n",
    "\n",
    "diff_data = diff_loader.load()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T13:56:02.247802Z",
     "start_time": "2024-05-27T13:56:01.985934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "msg_loader = JSONLoader(\n",
    "    file_path='../data/final_preprocessed_data/js_rag_db_data_300.json',\n",
    "    jq_schema='.[].msg',\n",
    "    text_content=False)\n",
    "\n",
    "msg_data = msg_loader.load()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "language_loader = JSONLoader(\n",
    "    file_path='../data/chronicle/chronicle_multi_diff_db_data.json',\n",
    "    jq_schema='.[].lang',\n",
    "    text_content=False)\n",
    "\n",
    "language_data = language_loader.load()"
   ],
   "metadata": {
    "id": "ltaWG7hinsp4",
    "ExecuteTime": {
     "end_time": "2024-05-16T20:07:40.808769Z",
     "start_time": "2024-05-16T20:07:27.066112Z"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R4qftc5k1bOh",
    "ExecuteTime": {
     "end_time": "2024-05-26T18:57:02.254122Z",
     "start_time": "2024-05-26T18:57:02.240120Z"
    }
   },
   "source": [
    "# 1-1000 data use java splitter, 1000-2000 use cpp splitter, 2000-3000 use csharp splitter, 3000-4000 use python splitter, 4000-5000 use javascript splitter\n",
    "\n",
    "\n",
    "# languages = [Language.JAVA, Language.CPP, Language.CSHARP, Language.PYTHON, Language.JS]\n",
    "languages = [Language.PYTHON, Language.JS]\n",
    "splitters = [RecursiveCharacterTextSplitter.from_language(language, chunk_size=500, chunk_overlap=50) for language in languages]\n",
    "\n",
    "# language_dict = {'java': 0, 'c++': 1, 'c#': 2, 'python': 3, 'javascript': 4}\n",
    "language_dict = {'Python': 0, 'JavaScript': 1}"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T18:50:23.242156Z",
     "start_time": "2024-05-26T18:50:23.223972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def similarity_search(documents, db):\n",
    "    # Initialize an empty dictionary to store aggregate scores for each candidate ID\n",
    "    aggregate_scores = {}\n",
    "\n",
    "    # Iterate through each document in the documents list\n",
    "    for document in documents:\n",
    "        # Apply similarity search function to the document\n",
    "        results = db.similarity_search_with_relevance_scores(document.page_content, score_threshold=0.0)\n",
    "        # Iterate through the results for each document\n",
    "        for candidate_doc in results:\n",
    "            id = candidate_doc[0].metadata['seq_num']\n",
    "            score = candidate_doc[1]\n",
    "            # Update the aggregate score for the candidate ID\n",
    "            aggregate_scores[id] = aggregate_scores.get(id, 0) + score\n",
    "\n",
    "    # Find the candidate ID with the highest aggregate score\n",
    "    if aggregate_scores:\n",
    "        max_candidate_id = max(aggregate_scores, key=aggregate_scores.get)\n",
    "    else:\n",
    "        max_candidate_id = -1\n",
    "\n",
    "    return max_candidate_id"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LJ1DFYLKnsp6",
    "outputId": "fe0afd44-16c4-4be2-bde4-b99cb0cfb9f8",
    "ExecuteTime": {
     "end_time": "2024-05-10T11:52:06.983579Z",
     "start_time": "2024-05-10T11:51:49.530283Z"
    }
   },
   "source": [
    "diff_split = []\n",
    "for i, doc in tqdm(enumerate(diff_data), total=len(diff_data), desc=\"Processing documents\"):\n",
    "    if language_data[i].page_content=='Python':\n",
    "        # diff_split.append(doc)\n",
    "        diff_split += splitters[language_dict[language_data[i].page_content]].split_documents([doc])"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the path to the pre-trained model you want to use\n",
    "modelPath = \"mixedbread-ai/mxbai-embed-large-v1\"\n",
    "\n",
    "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
    "model_kwargs = {'device':'cuda', 'trust_remote_code': True}\n",
    "\n",
    "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    cache_folder = '../models',\n",
    "    \n",
    "    model_name=modelPath,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T21:28:03.805737Z",
     "start_time": "2024-05-27T21:27:54.813892Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T14:45:31.309829Z",
     "start_time": "2024-05-27T14:45:31.080006Z"
    }
   },
   "cell_type": "code",
   "source": "res = embeddings.embed_query(diff_data[0].page_content)",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T15:27:28.637962Z",
     "start_time": "2024-05-10T11:52:56.339613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "db = Chroma.from_documents(diff_split, embeddings, persist_directory=\"./chroma_chronicle_db_mxbai_500chunk_normalized_python_multi_diff\")"
   ],
   "execution_count": 24,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T19:11:13.761723Z",
     "start_time": "2024-05-10T15:27:28.654431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "diff_split = []\n",
    "for i, doc in tqdm(enumerate(diff_data), total=len(diff_data), desc=\"Processing documents\"):\n",
    "    if language_data[i].page_content=='JavaScript':\n",
    "        # diff_split.append(doc)\n",
    "        diff_split += splitters[language_dict[language_data[i].page_content]].split_documents([doc])\n",
    "\n",
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "db = Chroma.from_documents(diff_split, embeddings, persist_directory=\"./chroma_chronicle_db_mxbai_500chunk_normalized_js_multi_diff\")"
   ],
   "execution_count": 25,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T20:08:03.281478Z",
     "start_time": "2024-05-16T20:08:03.259215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('../data/scenario/filtered_data/project_based_django_multi_diff_filtered.json', 'r', encoding='UTF-8') as f:\n",
    "    test_data = json.load(f)"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T20:16:14.151002Z",
     "start_time": "2024-05-16T20:16:14.137073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('../data/scenario/filtered_data/project_based_django_multi_diff_filtered.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(test_data, f, ensure_ascii=False, indent=4)"
   ],
   "execution_count": 24,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T21:28:03.883783Z",
     "start_time": "2024-05-27T21:28:03.807863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_diff_loader = JSONLoader(\n",
    "    file_path='../data/final_preprocessed_data/js_baseline_test_data_300.json',\n",
    "    jq_schema='.[].diff',\n",
    "    text_content=False)\n",
    "\n",
    "test_diff_data = test_diff_loader.load()[:1000]\n",
    "\n",
    "with open('../data/final_preprocessed_data/js_baseline_test_data_300.json', 'r', encoding='UTF-8') as f:\n",
    "    test_data = json.load(f)[:1000]"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Python part"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T21:28:08.135638Z",
     "start_time": "2024-05-27T21:28:07.084291Z"
    }
   },
   "cell_type": "code",
   "source": "db = Chroma(persist_directory=\"./final_js_rag_single_diff_db_300\", embedding_function=embeddings)",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T13:58:06.491237Z",
     "start_time": "2024-05-27T13:58:06.478212Z"
    }
   },
   "cell_type": "code",
   "source": "test_diff_data[0].page_content",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"diff --git a/test/Ownable.js b/test/Ownable.js @@ -38,6 +38,7 @@ contract('Ownable', function(accounts) {\\nlet originalOwner = await ownable.owner();\\ntry {\\nawait ownable.transferOwnership(null, {from: originalOwner});\\n+ assert.fail()\\n} catch(error) {\\nassertJump(error);\\n}\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T21:32:23.999362Z",
     "start_time": "2024-05-27T21:32:23.679363Z"
    }
   },
   "cell_type": "code",
   "source": "result = db.similarity_search_with_relevance_scores(test_diff_data[2].page_content)",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T21:32:24.451710Z",
     "start_time": "2024-05-27T21:32:24.432669Z"
    }
   },
   "cell_type": "code",
   "source": "result",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='diff --git a/scripts/release.js b/scripts/release.js @@ -45,6 +45,7 @@ program\\nwriteChangelog(program.changelog, repos)\\n.then(() => repos);\\n}\\n+ return repos;\\n})\\n.then((repos) => {\\nif (program.release && repos.length) {\\n', metadata={'seq_num': 995, 'source': 'D:\\\\TU Delft\\\\thesis\\\\LLM_CMG\\\\llm4commit\\\\data\\\\final_preprocessed_data\\\\js_rag_db_data_300.json'}),\n",
       "  0.8419218804857668),\n",
       " (Document(page_content='diff --git a/build/build.js b/build/build.js @@ -5,7 +5,7 @@ const fs = require(\"fs\");\\nconst path = require(\"path\");\\nconst pkg = require(\"../package.json\");\\nconst cwd = process.cwd();\\n-const ENV_RE = /__ENV__/g;\\n+const ENV_RE = /process\\\\.env\\\\.MOON_ENV/g;\\nconst comment = `/**\\n* Moon v${pkg.version}\\n', metadata={'seq_num': 10483, 'source': 'D:\\\\TU Delft\\\\thesis\\\\LLM_CMG\\\\llm4commit\\\\data\\\\final_preprocessed_data\\\\js_rag_db_data_300.json'}),\n",
       "  0.8402498728584911),\n",
       " (Document(page_content='diff --git a/scripts/release/release.js b/scripts/release/release.js @@ -5,5 +5,4 @@ const git = require(\"simple-git\")();\\nlet message = \"Release v\" + pkg.version;\\n// Commit and push to current branch\\n-git.commit(message, [\"dist\"]).push();\\n-\\n+git.commit(message, [\"dist\", \"package.json\"]).push();\\n', metadata={'seq_num': 10366, 'source': 'D:\\\\TU Delft\\\\thesis\\\\LLM_CMG\\\\llm4commit\\\\data\\\\final_preprocessed_data\\\\js_rag_db_data_300.json'}),\n",
       "  0.8396915008727066),\n",
       " (Document(page_content=\"diff --git a/src/index.js b/src/index.js @@ -3,8 +3,9 @@ const pkg = require('../package.json')\\ntry {\\nupdateNotifier({ pkg }).notify()\\n-} catch (_) {\\n- // noop\\n+} catch (e) {\\n+ console.log('Error checking for updates:')\\n+ console.log(e)\\n}\\nmodule.exports = require('@oclif/command')\\n\", metadata={'seq_num': 1069, 'source': 'D:\\\\TU Delft\\\\thesis\\\\LLM_CMG\\\\llm4commit\\\\data\\\\final_preprocessed_data\\\\js_rag_db_data_300.json'}),\n",
       "  0.8344021029947258)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T08:33:23.286634Z",
     "start_time": "2024-05-27T08:32:57.931399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# db = Chroma(persist_directory=\"./final_js_rag_single_diff_db_300\", embedding_function=embeddings)\n",
    "retriever = db.as_retriever()\n",
    "similar_diff = []\n",
    "for diff_doc in tqdm(test_diff_data, total=len(test_diff_data), desc=\"Processing documents\"):\n",
    "    # documents = splitters[1].split_documents([diff_doc])\n",
    "    similar_diff.append(retriever.invoke(diff_doc.page_content)[0])\n",
    "    # similar_diff.append(similarity_search(documents, db))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Processing documents:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d5104cb30d04bddb6ed084d0e75b612"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T08:33:54.749132Z",
     "start_time": "2024-05-27T08:33:54.731612Z"
    }
   },
   "cell_type": "code",
   "source": "similar_diff[0]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='diff --git a/protocols/delegate/test/Delegate-unit.js b/protocols/delegate/test/Delegate-unit.js @@ -181,7 +181,8 @@ contract(\\'Delegate Unit Tests\\', async accounts => {\\nPRICE_COEF,\\nEXP,\\n{ from: notOwner }\\n- )\\n+ ),\\n+ \"Ownable: caller is not the owner\"\\n)\\n})\\n', metadata={'seq_num': 1960, 'source': 'D:\\\\TU Delft\\\\thesis\\\\LLM_CMG\\\\llm4commit\\\\data\\\\final_preprocessed_data\\\\js_rag_db_data_300.json'})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T08:34:14.641841Z",
     "start_time": "2024-05-27T08:34:14.611898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "data = []\n",
    "\n",
    "for sim_diff, test_diff in zip(similar_diff, test_data):\n",
    "    item = {'org_diff': test_diff['diff'],\n",
    "            'org_msg': test_diff['msg'],\n",
    "            'sim_msg': msg_data[sim_diff.metadata['seq_num']-1].page_content,\n",
    "            # 'sim_msg': msg_data[sim_diff-1].page_content,\n",
    "            # 'sim_diff': diff_data[sim_diff-1].page_content}\n",
    "            'sim_diff': diff_data[sim_diff.metadata['seq_num']-1].page_content}\n",
    "    data.append(item)\n",
    "    \n",
    "# Write the data to a JSON file\n",
    "with open('../data/final_preprocessed_data/js_baseline/js_baseline_rag_300_prompt.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T15:10:36.691258Z",
     "start_time": "2024-05-11T15:06:56.822106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db = Chroma(persist_directory=\"./chroma_chronicle_db_mxbai_500chunk_normalized_js_multi_diff\", embedding_function=embeddings)\n",
    "similar_diff = []\n",
    "for diff_doc in tqdm(test_diff_data[:100], total=len(test_diff_data[:100]), desc=\"Processing documents\"):\n",
    "    documents = splitters[1].split_documents([diff_doc])\n",
    "    similar_diff.append(similarity_search(documents, db))\n",
    "\n",
    "data = []\n",
    "\n",
    "for sim_diff, test_diff in zip(similar_diff, test_data[1000:2000]):\n",
    "    item = {'org_diff': test_diff['diffs'],\n",
    "            'org_msg': test_diff['msg'],\n",
    "            'sim_msg': msg_data[sim_diff].page_content,\n",
    "            'sim_diff': diff_data[sim_diff-1].page_content}\n",
    "    data.append(item)\n",
    "    \n",
    "# Write the data to a JSON file\n",
    "with open('../data/chronicle/multi-diff/multi_diff_rag_js_prompt.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ],
   "execution_count": 25,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "db = Chroma.from_documents(diff_split, embeddings, persist_directory=\"./chroma_chronicle_db_mxbai_500chunk_normalized_python\")"
   ],
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T14:08:11.640263Z",
     "start_time": "2024-05-03T14:08:11.626748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "python_test_data = []\n",
    "python_indices = []\n",
    "for i, item in enumerate(test_data):\n",
    "    if item['lang'] == 'Python':\n",
    "        python_test_data.append(item)\n",
    "        python_indices.append(i)"
   ],
   "execution_count": 23,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T14:09:34.959631Z",
     "start_time": "2024-05-03T14:08:26.570272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db = Chroma(persist_directory=\"./chroma_chronicle_db_mxbai_500chunk_normalized_python\", embedding_function=embeddings)\n",
    "similar_diff = []\n",
    "for index in tqdm(python_indices, total=len(python_indices), desc=\"Processing documents\"):\n",
    "    documents = splitters[0].split_documents([test_diff_data[index]])\n",
    "    similar_diff.append(similarity_search(documents, db))\n",
    "\n",
    "data = []\n",
    "\n",
    "for sim_diff, test_diff in zip(similar_diff, python_test_data):\n",
    "    item = {'org_diff': test_diff['diff'],\n",
    "            'org_msg': test_diff['msg'],\n",
    "            'sim_msg': msg_data[sim_diff],\n",
    "            'sim_diff': diff_data[sim_diff-1].page_content}\n",
    "    data.append(item)\n",
    "    \n",
    "# Write the data to a JSON file\n",
    "with open('../data/chronicle/rag_baseline/rag_baseline_python.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ],
   "execution_count": 24,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# JavaScript part"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T14:12:28.878729Z",
     "start_time": "2024-05-03T14:12:28.871560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "js_test_data = []\n",
    "js_indices = []\n",
    "for i, item in enumerate(test_data):\n",
    "    if item['lang'] == 'JavaScript':\n",
    "        js_test_data.append(item)\n",
    "        js_indices.append(i)"
   ],
   "execution_count": 25,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T07:49:01.303681Z",
     "start_time": "2024-05-06T07:49:00.632183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db = Chroma(persist_directory=\"./chroma_chronicle_db_mxbai_500chunk_normalized_js\", embedding_function=embeddings)\n",
    "similar_diff = []\n",
    "for index in tqdm(js_indices, total=len(js_indices), desc=\"Processing documents\"):\n",
    "    documents = splitters[1].split_documents([test_diff_data[index]])\n",
    "    similar_diff.append(similarity_search(documents, db))\n",
    "\n",
    "data = []\n",
    "\n",
    "for sim_diff, test_diff in zip(similar_diff, js_test_data):\n",
    "    item = {'org_diff': test_diff['diff'],\n",
    "            'org_msg': test_diff['msg'],\n",
    "            'sim_msg': msg_data[sim_diff],\n",
    "            'sim_diff': diff_data[sim_diff-1].page_content}\n",
    "    data.append(item)\n",
    "    \n",
    "# Write the data to a JSON file\n",
    "with open('../data/chronicle/rag_baseline/rag_baseline_js.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T15:42:10.713727Z",
     "start_time": "2024-05-02T14:34:01.942274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "diff_split = []\n",
    "for i, doc in tqdm(enumerate(diff_data), total=len(diff_data), desc=\"Processing documents\"):\n",
    "    if language_data[i].page_content=='JavaScript':\n",
    "        # diff_split.append(doc)\n",
    "        diff_split += splitters[language_dict[language_data[i].page_content]].split_documents([doc])\n",
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "db = Chroma.from_documents(diff_split, embeddings, persist_directory=\"./chroma_chronicle_db_mxbai_500chunk_normalized_js\")"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "similar_diff = []\n",
    "for i, test_data in tqdm(enumerate(test_diff_data[:100]), total=len(test_diff_data[:100]), desc=\"Processing documents\"):\n",
    "    query = test_data.page_content\n",
    "    similar_diff.append(diff_data[retriever.get_relevant_documents(query)[0].metadata['seq_num']-1])\n",
    "\n",
    "data = []\n",
    "\n",
    "for sim_diff, test_diff in zip(similar_diff, test_diff_data[:100]):\n",
    "    item = {\n",
    "        'sim_msg': msg_data[sim_diff.metadata['seq_num']],\n",
    "        'sim_diff': sim_diff.page_content,\n",
    "        'org_diff': test_diff.page_content\n",
    "    }\n",
    "    data.append(item)\n",
    "\n",
    "# Write the data to a JSON file\n",
    "with open('../data/chronicle/data_with_similar_diff_codebert.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "similar_diff = []\n",
    "for i, test_data in tqdm(enumerate(test_diff_data[:100]), total=len(test_diff_data[:100]), desc=\"Processing documents\"):\n",
    "    query = test_data.page_content\n",
    "    similar_diff.append(diff_data[retriever_2.get_relevant_documents(query)[0].metadata['seq_num']-1])\n",
    "\n",
    "data = []\n",
    "\n",
    "for sim_diff, test_diff in zip(similar_diff, test_diff_data[:100]):\n",
    "    item = {\n",
    "        'sim_msg': msg_data[sim_diff.metadata['seq_num']],\n",
    "        'sim_diff': sim_diff.page_content,\n",
    "        'org_diff': test_diff.page_content\n",
    "    }\n",
    "    data.append(item)\n",
    "\n",
    "# Write the data to a JSON file\n",
    "with open('../data/chronicle/data_with_similar_diff_mxbai.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T18:00:44.062169Z",
     "start_time": "2024-04-25T18:00:39.793930Z"
    }
   },
   "execution_count": 57,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "BTU8eiRk1bOm",
    "ExecuteTime": {
     "end_time": "2024-04-25T17:49:40.232903Z",
     "start_time": "2024-04-25T17:49:40.205900Z"
    }
   },
   "source": [
    "data = []\n",
    "\n",
    "for sim_diff, test_diff in zip(similar_diff, test_diff_data):\n",
    "    item = {\n",
    "        'sim_msg': msg_data[sim_diff.metadata['seq_num']],\n",
    "        'sim_diff': sim_diff.page_content,\n",
    "        'org_diff': test_diff.page_content\n",
    "    }\n",
    "    data.append(item)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "aVxF1tkQ1bOn",
    "ExecuteTime": {
     "end_time": "2024-04-25T17:49:43.781387Z",
     "start_time": "2024-04-25T17:49:43.722376Z"
    }
   },
   "source": [
    "# Write the data to a JSON file\n",
    "with open('../data/chronicle/data_with_similar_diff_codebert.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
