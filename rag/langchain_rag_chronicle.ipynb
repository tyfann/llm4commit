{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSV1X4nOoF9x"
   },
   "source": [
    "!pip install langchain\n",
    "!pip install jq\n",
    "!pip install sentence-transformers"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JmawYD7kGNLZ",
    "ExecuteTime": {
     "end_time": "2024-05-08T07:42:45.025598Z",
     "start_time": "2024-05-08T07:42:44.492482Z"
    }
   },
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import pickle\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6j2XMd6gCdQE",
    "ExecuteTime": {
     "end_time": "2024-05-08T07:42:45.246375Z",
     "start_time": "2024-05-08T07:42:45.123697Z"
    }
   },
   "source": [
    "with open(\"../data/chronicle/rag_msg.pkl\", \"rb\") as f:\n",
    "    msg_data = pickle.load(f)"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T07:43:04.506024Z",
     "start_time": "2024-05-08T07:42:47.323923Z"
    }
   },
   "source": [
    "diff_loader = JSONLoader(\n",
    "    file_path='../data/chronicle/chronicle_rag_db.json',\n",
    "    jq_schema='.[].diff',\n",
    "    text_content=False)\n",
    "\n",
    "diff_data = diff_loader.load()"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "language_loader = JSONLoader(\n",
    "    file_path='../data/chronicle/chronicle_rag_db.json',\n",
    "    jq_schema='.[].lang',\n",
    "    text_content=False)\n",
    "\n",
    "language_data = language_loader.load()"
   ],
   "metadata": {
    "id": "ltaWG7hinsp4",
    "ExecuteTime": {
     "end_time": "2024-05-03T13:59:54.755521Z",
     "start_time": "2024-05-03T13:59:37.911783Z"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R4qftc5k1bOh",
    "ExecuteTime": {
     "end_time": "2024-05-08T07:43:04.521483Z",
     "start_time": "2024-05-08T07:43:04.507967Z"
    }
   },
   "source": [
    "# 1-1000 data use java splitter, 1000-2000 use cpp splitter, 2000-3000 use csharp splitter, 3000-4000 use python splitter, 4000-5000 use javascript splitter\n",
    "\n",
    "\n",
    "# languages = [Language.JAVA, Language.CPP, Language.CSHARP, Language.PYTHON, Language.JS]\n",
    "languages = [Language.PYTHON, Language.JS]\n",
    "splitters = [RecursiveCharacterTextSplitter.from_language(language, chunk_size=500, chunk_overlap=50) for language in languages]\n",
    "\n",
    "# language_dict = {'java': 0, 'c++': 1, 'c#': 2, 'python': 3, 'javascript': 4}\n",
    "language_dict = {'Python': 0, 'JavaScript': 1}"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T07:43:08.353152Z",
     "start_time": "2024-05-08T07:43:08.339636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def similarity_search(documents, db):\n",
    "    # Initialize an empty dictionary to store aggregate scores for each candidate ID\n",
    "    aggregate_scores = {}\n",
    "\n",
    "    # Iterate through each document in the documents list\n",
    "    for document in documents:\n",
    "        # Apply similarity search function to the document\n",
    "        results = db.similarity_search_with_relevance_scores(document.page_content, score_threshold=0.0)\n",
    "        # Iterate through the results for each document\n",
    "        for candidate_doc in results:\n",
    "            id = candidate_doc[0].metadata['seq_num']\n",
    "            score = candidate_doc[1]\n",
    "            # Update the aggregate score for the candidate ID\n",
    "            aggregate_scores[id] = aggregate_scores.get(id, 0) + score\n",
    "\n",
    "    # Find the candidate ID with the highest aggregate score\n",
    "    if aggregate_scores:\n",
    "        max_candidate_id = max(aggregate_scores, key=aggregate_scores.get)\n",
    "    else:\n",
    "        max_candidate_id = -1\n",
    "\n",
    "    return max_candidate_id"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LJ1DFYLKnsp6",
    "outputId": "fe0afd44-16c4-4be2-bde4-b99cb0cfb9f8",
    "ExecuteTime": {
     "end_time": "2024-05-02T13:18:08.174244Z",
     "start_time": "2024-05-02T13:18:02.585227Z"
    }
   },
   "source": [
    "diff_split = []\n",
    "for i, doc in tqdm(enumerate(diff_data), total=len(diff_data), desc=\"Processing documents\"):\n",
    "    if language_data[i].page_content=='Python':\n",
    "        # diff_split.append(doc)\n",
    "        diff_split += splitters[language_dict[language_data[i].page_content]].split_documents([doc])"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the path to the pre-trained model you want to use\n",
    "modelPath = \"mixedbread-ai/mxbai-embed-large-v1\"\n",
    "\n",
    "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
    "model_kwargs = {'device':'cuda', 'trust_remote_code': True}\n",
    "\n",
    "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    cache_folder = '../models',\n",
    "    \n",
    "    model_name=modelPath,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T07:43:27.347452Z",
     "start_time": "2024-05-08T07:43:18.379526Z"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T07:53:15.355021Z",
     "start_time": "2024-05-08T07:53:15.299648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_diff_loader = JSONLoader(\n",
    "    file_path='../data/scenario/scenario_react_chatgpt_zeroshot.json',\n",
    "    jq_schema='.[].diff',\n",
    "    text_content=False)\n",
    "\n",
    "test_diff_data = test_diff_loader.load()\n",
    "\n",
    "with open('../data/scenario/scenario_react_chatgpt_zeroshot.json', 'r', encoding='UTF-8') as f:\n",
    "    test_data = json.load(f)"
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Python part"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T07:53:44.490240Z",
     "start_time": "2024-05-08T07:53:34.409435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db = Chroma(persist_directory=\"./chroma_chronicle_db_mxbai_500chunk_normalized_js\", embedding_function=embeddings)\n",
    "similar_diff = []\n",
    "for diff_doc in tqdm(test_diff_data[:100], total=len(test_diff_data[:100]), desc=\"Processing documents\"):\n",
    "    documents = splitters[1].split_documents([diff_doc])\n",
    "    similar_diff.append(similarity_search(documents, db))\n",
    "\n",
    "data = []\n",
    "\n",
    "for sim_diff, test_diff in zip(similar_diff, test_data[:100]):\n",
    "    item = {'org_diff': test_diff['diff'],\n",
    "            'org_msg': test_diff['msg'],\n",
    "            'sim_msg': msg_data[sim_diff],\n",
    "            'sim_diff': diff_data[sim_diff-1].page_content}\n",
    "    data.append(item)\n",
    "    \n",
    "# Write the data to a JSON file\n",
    "with open('../data/scenario/scenario_react_chatgpt_rag_prompt.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "db = Chroma.from_documents(diff_split, embeddings, persist_directory=\"./chroma_chronicle_db_mxbai_500chunk_normalized_python\")"
   ],
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T14:08:11.640263Z",
     "start_time": "2024-05-03T14:08:11.626748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "python_test_data = []\n",
    "python_indices = []\n",
    "for i, item in enumerate(test_data):\n",
    "    if item['lang'] == 'Python':\n",
    "        python_test_data.append(item)\n",
    "        python_indices.append(i)"
   ],
   "execution_count": 23,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T14:09:34.959631Z",
     "start_time": "2024-05-03T14:08:26.570272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db = Chroma(persist_directory=\"./chroma_chronicle_db_mxbai_500chunk_normalized_python\", embedding_function=embeddings)\n",
    "similar_diff = []\n",
    "for index in tqdm(python_indices, total=len(python_indices), desc=\"Processing documents\"):\n",
    "    documents = splitters[0].split_documents([test_diff_data[index]])\n",
    "    similar_diff.append(similarity_search(documents, db))\n",
    "\n",
    "data = []\n",
    "\n",
    "for sim_diff, test_diff in zip(similar_diff, python_test_data):\n",
    "    item = {'org_diff': test_diff['diff'],\n",
    "            'org_msg': test_diff['msg'],\n",
    "            'sim_msg': msg_data[sim_diff],\n",
    "            'sim_diff': diff_data[sim_diff-1].page_content}\n",
    "    data.append(item)\n",
    "    \n",
    "# Write the data to a JSON file\n",
    "with open('../data/chronicle/rag_baseline/rag_baseline_python.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ],
   "execution_count": 24,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# JavaScript part"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T14:12:28.878729Z",
     "start_time": "2024-05-03T14:12:28.871560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "js_test_data = []\n",
    "js_indices = []\n",
    "for i, item in enumerate(test_data):\n",
    "    if item['lang'] == 'JavaScript':\n",
    "        js_test_data.append(item)\n",
    "        js_indices.append(i)"
   ],
   "execution_count": 25,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T07:49:01.303681Z",
     "start_time": "2024-05-06T07:49:00.632183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db = Chroma(persist_directory=\"./chroma_chronicle_db_mxbai_500chunk_normalized_js\", embedding_function=embeddings)\n",
    "similar_diff = []\n",
    "for index in tqdm(js_indices, total=len(js_indices), desc=\"Processing documents\"):\n",
    "    documents = splitters[1].split_documents([test_diff_data[index]])\n",
    "    similar_diff.append(similarity_search(documents, db))\n",
    "\n",
    "data = []\n",
    "\n",
    "for sim_diff, test_diff in zip(similar_diff, js_test_data):\n",
    "    item = {'org_diff': test_diff['diff'],\n",
    "            'org_msg': test_diff['msg'],\n",
    "            'sim_msg': msg_data[sim_diff],\n",
    "            'sim_diff': diff_data[sim_diff-1].page_content}\n",
    "    data.append(item)\n",
    "    \n",
    "# Write the data to a JSON file\n",
    "with open('../data/chronicle/rag_baseline/rag_baseline_js.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T15:42:10.713727Z",
     "start_time": "2024-05-02T14:34:01.942274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "diff_split = []\n",
    "for i, doc in tqdm(enumerate(diff_data), total=len(diff_data), desc=\"Processing documents\"):\n",
    "    if language_data[i].page_content=='JavaScript':\n",
    "        # diff_split.append(doc)\n",
    "        diff_split += splitters[language_dict[language_data[i].page_content]].split_documents([doc])\n",
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "db = Chroma.from_documents(diff_split, embeddings, persist_directory=\"./chroma_chronicle_db_mxbai_500chunk_normalized_js\")"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "similar_diff = []\n",
    "for i, test_data in tqdm(enumerate(test_diff_data[:100]), total=len(test_diff_data[:100]), desc=\"Processing documents\"):\n",
    "    query = test_data.page_content\n",
    "    similar_diff.append(diff_data[retriever.get_relevant_documents(query)[0].metadata['seq_num']-1])\n",
    "\n",
    "data = []\n",
    "\n",
    "for sim_diff, test_diff in zip(similar_diff, test_diff_data[:100]):\n",
    "    item = {\n",
    "        'sim_msg': msg_data[sim_diff.metadata['seq_num']],\n",
    "        'sim_diff': sim_diff.page_content,\n",
    "        'org_diff': test_diff.page_content\n",
    "    }\n",
    "    data.append(item)\n",
    "\n",
    "# Write the data to a JSON file\n",
    "with open('../data/chronicle/data_with_similar_diff_codebert.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "similar_diff = []\n",
    "for i, test_data in tqdm(enumerate(test_diff_data[:100]), total=len(test_diff_data[:100]), desc=\"Processing documents\"):\n",
    "    query = test_data.page_content\n",
    "    similar_diff.append(diff_data[retriever_2.get_relevant_documents(query)[0].metadata['seq_num']-1])\n",
    "\n",
    "data = []\n",
    "\n",
    "for sim_diff, test_diff in zip(similar_diff, test_diff_data[:100]):\n",
    "    item = {\n",
    "        'sim_msg': msg_data[sim_diff.metadata['seq_num']],\n",
    "        'sim_diff': sim_diff.page_content,\n",
    "        'org_diff': test_diff.page_content\n",
    "    }\n",
    "    data.append(item)\n",
    "\n",
    "# Write the data to a JSON file\n",
    "with open('../data/chronicle/data_with_similar_diff_mxbai.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T18:00:44.062169Z",
     "start_time": "2024-04-25T18:00:39.793930Z"
    }
   },
   "execution_count": 57,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "BTU8eiRk1bOm",
    "ExecuteTime": {
     "end_time": "2024-04-25T17:49:40.232903Z",
     "start_time": "2024-04-25T17:49:40.205900Z"
    }
   },
   "source": [
    "data = []\n",
    "\n",
    "for sim_diff, test_diff in zip(similar_diff, test_diff_data):\n",
    "    item = {\n",
    "        'sim_msg': msg_data[sim_diff.metadata['seq_num']],\n",
    "        'sim_diff': sim_diff.page_content,\n",
    "        'org_diff': test_diff.page_content\n",
    "    }\n",
    "    data.append(item)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "aVxF1tkQ1bOn",
    "ExecuteTime": {
     "end_time": "2024-04-25T17:49:43.781387Z",
     "start_time": "2024-04-25T17:49:43.722376Z"
    }
   },
   "source": [
    "# Write the data to a JSON file\n",
    "with open('../data/chronicle/data_with_similar_diff_codebert.json', 'w', encoding='UTF-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
